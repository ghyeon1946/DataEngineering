{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f3d9af1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T12:14:41.970407Z",
     "iopub.status.busy": "2022-06-13T12:14:41.969869Z",
     "iopub.status.idle": "2022-06-13T12:15:02.971234Z",
     "shell.execute_reply": "2022-06-13T12:15:02.970252Z"
    },
    "papermill": {
     "duration": 21.015516,
     "end_time": "2022-06-13T12:15:02.973598",
     "exception": false,
     "start_time": "2022-06-13T12:14:41.958082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm\r\n",
      "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.5/431.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.12.0)\r\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from timm) (1.11.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm) (4.2.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (9.1.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (2.27.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (1.21.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm) (2022.5.18.1)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm) (2.0.12)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm) (1.26.9)\r\n",
      "Installing collected packages: timm\r\n",
      "Successfully installed timm-0.5.4\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mRequirement already satisfied: torchmetrics in /opt/conda/lib/python3.7/site-packages (0.8.2)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (21.3)\r\n",
      "Requirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (1.21.6)\r\n",
      "Requirement already satisfied: pyDeprecate==0.3.* in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (0.3.2)\r\n",
      "Requirement already satisfied: torch>=1.3.1 in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (1.11.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.3.1->torchmetrics) (4.2.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->torchmetrics) (3.0.9)\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install timm # install pytorch image models\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6552cc44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T12:15:02.994935Z",
     "iopub.status.busy": "2022-06-13T12:15:02.994616Z",
     "iopub.status.idle": "2022-06-13T12:15:12.636194Z",
     "shell.execute_reply": "2022-06-13T12:15:12.635335Z"
    },
    "papermill": {
     "duration": 9.654785,
     "end_time": "2022-06-13T12:15:12.638509",
     "exception": false,
     "start_time": "2022-06-13T12:15:02.983724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "import timm\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from torch import nn\n",
    "from  torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0a607b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T12:15:12.659881Z",
     "iopub.status.busy": "2022-06-13T12:15:12.659148Z",
     "iopub.status.idle": "2022-06-13T12:15:12.665925Z",
     "shell.execute_reply": "2022-06-13T12:15:12.665164Z"
    },
    "papermill": {
     "duration": 0.019271,
     "end_time": "2022-06-13T12:15:12.667596",
     "exception": false,
     "start_time": "2022-06-13T12:15:12.648325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomModel(torch.nn.Module): \n",
    "    def __init__(self, model_backbone):\n",
    "        super(CustomModel,self).__init__()\n",
    "        self.model = model_backbone\n",
    "        self.num_in_features = self.model.get_classifier().in_features\n",
    "        print(self.num_in_features)\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.num_in_features),\n",
    "            nn.Linear(self.num_in_features, 512),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 100),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a094f15",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-13T12:15:12.687922Z",
     "iopub.status.busy": "2022-06-13T12:15:12.687253Z",
     "iopub.status.idle": "2022-06-13T12:15:12.695982Z",
     "shell.execute_reply": "2022-06-13T12:15:12.695120Z"
    },
    "papermill": {
     "duration": 0.021155,
     "end_time": "2022-06-13T12:15:12.698122",
     "exception": false,
     "start_time": "2022-06-13T12:15:12.676967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SorghumDataset(Dataset):\n",
    "    def __init__(self, dirs, labels, transformation=None):\n",
    "        super(SorghumDataset,self).__init__()\n",
    "        self.dirs = dirs\n",
    "        self.labels = labels\n",
    "        self.transformation = transformation\n",
    "    def __len__(self):\n",
    "        return len(self.dirs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(self.dirs[index])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = self.labels[index] # need to one hot encoding here\n",
    "        \n",
    "        image = np.array(image)\n",
    "\n",
    "        if self.transformation:\n",
    "            aug_image = self.transformation(image=image)\n",
    "            image = aug_image['image']\n",
    "            \n",
    "        image = image / 255.\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        \n",
    "        image = torch.from_numpy(image).type(torch.float32)\n",
    "        image = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))(image)\n",
    "        \n",
    "        labels = torch.from_numpy(np.array(self.labels[index])).type(torch.float32)\n",
    "\n",
    "\n",
    "        return image, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93660332",
   "metadata": {
    "papermill": {
     "duration": 0.009303,
     "end_time": "2022-06-13T12:15:12.716725",
     "exception": false,
     "start_time": "2022-06-13T12:15:12.707422",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa4d1e60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T12:15:12.737850Z",
     "iopub.status.busy": "2022-06-13T12:15:12.737208Z",
     "iopub.status.idle": "2022-06-13T12:15:23.992934Z",
     "shell.execute_reply": "2022-06-13T12:15:23.992123Z"
    },
    "papermill": {
     "duration": 11.267837,
     "end_time": "2022-06-13T12:15:23.994762",
     "exception": false,
     "start_time": "2022-06-13T12:15:12.726925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnext50d_32x4d-103e99f8.pth\" to /root/.cache/torch/hub/checkpoints/resnext50d_32x4d-103e99f8.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (model): ResNet(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Identity()\n",
       "          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "    (classifier): Sequential(\n",
       "      (0): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Linear(in_features=512, out_features=100, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model_name = 'resnext50d_32x4d'\n",
    "\n",
    "backbone = timm.create_model(model_name,pretrained=True)\n",
    "model = CustomModel(backbone)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c52d50a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T12:15:24.017432Z",
     "iopub.status.busy": "2022-06-13T12:15:24.016627Z",
     "iopub.status.idle": "2022-06-13T12:15:28.311018Z",
     "shell.execute_reply": "2022-06-13T12:15:28.310192Z"
    },
    "papermill": {
     "duration": 4.307797,
     "end_time": "2022-06-13T12:15:28.312901",
     "exception": false,
     "start_time": "2022-06-13T12:15:24.005104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('../input/sorghum-efficientnetv2-0-846-private-lb/resnext50d_32x4d_best.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75c50adb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T12:15:28.334723Z",
     "iopub.status.busy": "2022-06-13T12:15:28.334175Z",
     "iopub.status.idle": "2022-06-13T12:15:28.375149Z",
     "shell.execute_reply": "2022-06-13T12:15:28.374335Z"
    },
    "papermill": {
     "duration": 0.053868,
     "end_time": "2022-06-13T12:15:28.376980",
     "exception": false,
     "start_time": "2022-06-13T12:15:28.323112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>cultivar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000005362.png</td>\n",
       "      <td>PI_152923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000099707.png</td>\n",
       "      <td>PI_152923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000135300.png</td>\n",
       "      <td>PI_152923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000136796.png</td>\n",
       "      <td>PI_152923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000292439.png</td>\n",
       "      <td>PI_152923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename   cultivar\n",
       "0  1000005362.png  PI_152923\n",
       "1  1000099707.png  PI_152923\n",
       "2  1000135300.png  PI_152923\n",
       "3  1000136796.png  PI_152923\n",
       "4  1000292439.png  PI_152923"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.read_csv('../input/sorghum-id-fgvc-9/sample_submission.csv')\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "499bbf3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T12:15:28.398876Z",
     "iopub.status.busy": "2022-06-13T12:15:28.398584Z",
     "iopub.status.idle": "2022-06-13T12:15:28.422948Z",
     "shell.execute_reply": "2022-06-13T12:15:28.422213Z"
    },
    "papermill": {
     "duration": 0.037218,
     "end_time": "2022-06-13T12:15:28.424642",
     "exception": false,
     "start_time": "2022-06-13T12:15:28.387424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>cultivar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../input/sorghum-id-fgvc-9/test/1000005362.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../input/sorghum-id-fgvc-9/test/1000099707.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../input/sorghum-id-fgvc-9/test/1000135300.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../input/sorghum-id-fgvc-9/test/1000136796.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../input/sorghum-id-fgvc-9/test/1000292439.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         filename  cultivar\n",
       "0  ../input/sorghum-id-fgvc-9/test/1000005362.png         0\n",
       "1  ../input/sorghum-id-fgvc-9/test/1000099707.png         0\n",
       "2  ../input/sorghum-id-fgvc-9/test/1000135300.png         0\n",
       "3  ../input/sorghum-id-fgvc-9/test/1000136796.png         0\n",
       "4  ../input/sorghum-id-fgvc-9/test/1000292439.png         0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub[\"filename\"] = sub[\"filename\"].apply(lambda image: '../input/sorghum-id-fgvc-9/test/' + image)\n",
    "sub[\"cultivar\"] = 0\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "382db9c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T12:15:28.447156Z",
     "iopub.status.busy": "2022-06-13T12:15:28.446801Z",
     "iopub.status.idle": "2022-06-13T12:15:28.452479Z",
     "shell.execute_reply": "2022-06-13T12:15:28.451618Z"
    },
    "papermill": {
     "duration": 0.019225,
     "end_time": "2022-06-13T12:15:28.454426",
     "exception": false,
     "start_time": "2022-06-13T12:15:28.435201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_transformation = A.Compose([\n",
    "    A.Resize(width=512, height=512, p=1.0)\n",
    "])\n",
    "\n",
    "testing_dataset = SorghumDataset(sub['filename'], sub['cultivar'], validation_transformation)\n",
    "testing_dataloader = DataLoader(testing_dataset, \n",
    "                                batch_size=32, \n",
    "                                shuffle=False, \n",
    "                                num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0999e9e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T12:15:28.477989Z",
     "iopub.status.busy": "2022-06-13T12:15:28.477696Z",
     "iopub.status.idle": "2022-06-13T12:41:50.733721Z",
     "shell.execute_reply": "2022-06-13T12:41:50.732529Z"
    },
    "papermill": {
     "duration": 1582.271807,
     "end_time": "2022-06-13T12:41:50.736967",
     "exception": false,
     "start_time": "2022-06-13T12:15:28.465160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 739/739 [26:22<00:00,  2.14s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "cnt = 0\n",
    "\n",
    "resnet_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image, label in tqdm(testing_dataloader):\n",
    "        image = image.to(device)\n",
    "        outputs = model(image)\n",
    "        for i in range(len(outputs)):\n",
    "            resnet_preds.append(outputs[i][:100])\n",
    "#         resnet_preds.append(outputs[0][:100])\n",
    "        preds = outputs.detach().cpu()\n",
    "        predictions.append(preds.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d75c123",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T12:41:50.844261Z",
     "iopub.status.busy": "2022-06-13T12:41:50.843929Z",
     "iopub.status.idle": "2022-06-13T12:41:51.441771Z",
     "shell.execute_reply": "2022-06-13T12:41:51.440823Z"
    },
    "papermill": {
     "duration": 0.653803,
     "end_time": "2022-06-13T12:41:51.444704",
     "exception": false,
     "start_time": "2022-06-13T12:41:50.790901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "resnet_preds_ = []\n",
    "\n",
    "for i in range(len(resnet_preds)):\n",
    "    resnet_preds_.append(resnet_preds[i].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76a314d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T12:41:51.638836Z",
     "iopub.status.busy": "2022-06-13T12:41:51.638232Z",
     "iopub.status.idle": "2022-06-13T12:45:38.445111Z",
     "shell.execute_reply": "2022-06-13T12:45:38.444327Z"
    },
    "papermill": {
     "duration": 226.903186,
     "end_time": "2022-06-13T12:45:38.447305",
     "exception": false,
     "start_time": "2022-06-13T12:41:51.544119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "resnet_preds_list = pd.DataFrame({'0_class':[]})\n",
    "\n",
    "for i in range(99, 0, -1): \n",
    "    resnet_preds_list.insert(1, \"{}_class\".format(i), [])\n",
    "\n",
    "for i in range(len(resnet_preds_)):\n",
    "    resnet_preds_list.loc[i] = resnet_preds_[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc3f3f7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T12:45:38.553865Z",
     "iopub.status.busy": "2022-06-13T12:45:38.553561Z",
     "iopub.status.idle": "2022-06-13T12:45:42.748565Z",
     "shell.execute_reply": "2022-06-13T12:45:42.746643Z"
    },
    "papermill": {
     "duration": 4.25014,
     "end_time": "2022-06-13T12:45:42.750991",
     "exception": false,
     "start_time": "2022-06-13T12:45:38.500851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_class</th>\n",
       "      <th>1_class</th>\n",
       "      <th>2_class</th>\n",
       "      <th>3_class</th>\n",
       "      <th>4_class</th>\n",
       "      <th>5_class</th>\n",
       "      <th>6_class</th>\n",
       "      <th>7_class</th>\n",
       "      <th>8_class</th>\n",
       "      <th>9_class</th>\n",
       "      <th>...</th>\n",
       "      <th>90_class</th>\n",
       "      <th>91_class</th>\n",
       "      <th>92_class</th>\n",
       "      <th>93_class</th>\n",
       "      <th>94_class</th>\n",
       "      <th>95_class</th>\n",
       "      <th>96_class</th>\n",
       "      <th>97_class</th>\n",
       "      <th>98_class</th>\n",
       "      <th>99_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.801618</td>\n",
       "      <td>-2.975569</td>\n",
       "      <td>-1.010900</td>\n",
       "      <td>0.004538</td>\n",
       "      <td>0.350488</td>\n",
       "      <td>-4.283353</td>\n",
       "      <td>-4.044071</td>\n",
       "      <td>-1.817170</td>\n",
       "      <td>-3.906837</td>\n",
       "      <td>-5.185993</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.440704</td>\n",
       "      <td>-5.510637</td>\n",
       "      <td>-3.565371</td>\n",
       "      <td>-4.367570</td>\n",
       "      <td>-3.586168</td>\n",
       "      <td>-4.021678</td>\n",
       "      <td>-5.994962</td>\n",
       "      <td>-4.712767</td>\n",
       "      <td>-2.815238</td>\n",
       "      <td>-1.952368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.122581</td>\n",
       "      <td>-1.158474</td>\n",
       "      <td>-8.431793</td>\n",
       "      <td>-7.456934</td>\n",
       "      <td>-7.085815</td>\n",
       "      <td>-5.844799</td>\n",
       "      <td>-4.357646</td>\n",
       "      <td>-6.926689</td>\n",
       "      <td>-6.120798</td>\n",
       "      <td>0.864344</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.683628</td>\n",
       "      <td>-4.942460</td>\n",
       "      <td>-5.104192</td>\n",
       "      <td>-3.478405</td>\n",
       "      <td>-3.778158</td>\n",
       "      <td>-4.185591</td>\n",
       "      <td>-8.658447</td>\n",
       "      <td>-5.647788</td>\n",
       "      <td>-5.539834</td>\n",
       "      <td>-7.571363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-5.502335</td>\n",
       "      <td>-1.471846</td>\n",
       "      <td>8.994921</td>\n",
       "      <td>-1.644590</td>\n",
       "      <td>2.780464</td>\n",
       "      <td>-4.725876</td>\n",
       "      <td>-4.252627</td>\n",
       "      <td>-5.074745</td>\n",
       "      <td>0.530009</td>\n",
       "      <td>-0.460110</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.465601</td>\n",
       "      <td>-5.762954</td>\n",
       "      <td>-5.205477</td>\n",
       "      <td>-4.648644</td>\n",
       "      <td>-1.996166</td>\n",
       "      <td>-7.006385</td>\n",
       "      <td>-6.289888</td>\n",
       "      <td>-5.483791</td>\n",
       "      <td>-3.666237</td>\n",
       "      <td>-3.386745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.438631</td>\n",
       "      <td>-2.900598</td>\n",
       "      <td>-2.842964</td>\n",
       "      <td>-2.837878</td>\n",
       "      <td>-2.651607</td>\n",
       "      <td>-3.997262</td>\n",
       "      <td>-2.419579</td>\n",
       "      <td>-2.409816</td>\n",
       "      <td>-1.671640</td>\n",
       "      <td>-2.249619</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150546</td>\n",
       "      <td>-3.092925</td>\n",
       "      <td>-1.705112</td>\n",
       "      <td>-3.123335</td>\n",
       "      <td>-2.864271</td>\n",
       "      <td>-2.119378</td>\n",
       "      <td>-2.402761</td>\n",
       "      <td>-2.806396</td>\n",
       "      <td>-3.288818</td>\n",
       "      <td>-2.080209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.856150</td>\n",
       "      <td>-0.279349</td>\n",
       "      <td>-4.226741</td>\n",
       "      <td>-2.668807</td>\n",
       "      <td>-2.526486</td>\n",
       "      <td>-1.116520</td>\n",
       "      <td>-0.516609</td>\n",
       "      <td>-5.410573</td>\n",
       "      <td>-4.072620</td>\n",
       "      <td>-1.866208</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.197257</td>\n",
       "      <td>-7.641562</td>\n",
       "      <td>-1.674403</td>\n",
       "      <td>-4.825066</td>\n",
       "      <td>-5.543388</td>\n",
       "      <td>-5.794561</td>\n",
       "      <td>-5.393475</td>\n",
       "      <td>-6.552042</td>\n",
       "      <td>0.802193</td>\n",
       "      <td>-5.098389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0_class   1_class   2_class   3_class   4_class   5_class   6_class  \\\n",
       "0 -4.801618 -2.975569 -1.010900  0.004538  0.350488 -4.283353 -4.044071   \n",
       "1 -3.122581 -1.158474 -8.431793 -7.456934 -7.085815 -5.844799 -4.357646   \n",
       "2 -5.502335 -1.471846  8.994921 -1.644590  2.780464 -4.725876 -4.252627   \n",
       "3 -1.438631 -2.900598 -2.842964 -2.837878 -2.651607 -3.997262 -2.419579   \n",
       "4 -5.856150 -0.279349 -4.226741 -2.668807 -2.526486 -1.116520 -0.516609   \n",
       "\n",
       "    7_class   8_class   9_class  ...  90_class  91_class  92_class  93_class  \\\n",
       "0 -1.817170 -3.906837 -5.185993  ... -4.440704 -5.510637 -3.565371 -4.367570   \n",
       "1 -6.926689 -6.120798  0.864344  ... -1.683628 -4.942460 -5.104192 -3.478405   \n",
       "2 -5.074745  0.530009 -0.460110  ... -3.465601 -5.762954 -5.205477 -4.648644   \n",
       "3 -2.409816 -1.671640 -2.249619  ... -2.150546 -3.092925 -1.705112 -3.123335   \n",
       "4 -5.410573 -4.072620 -1.866208  ... -2.197257 -7.641562 -1.674403 -4.825066   \n",
       "\n",
       "   94_class  95_class  96_class  97_class  98_class  99_class  \n",
       "0 -3.586168 -4.021678 -5.994962 -4.712767 -2.815238 -1.952368  \n",
       "1 -3.778158 -4.185591 -8.658447 -5.647788 -5.539834 -7.571363  \n",
       "2 -1.996166 -7.006385 -6.289888 -5.483791 -3.666237 -3.386745  \n",
       "3 -2.864271 -2.119378 -2.402761 -2.806396 -3.288818 -2.080209  \n",
       "4 -5.543388 -5.794561 -5.393475 -6.552042  0.802193 -5.098389  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_preds_list.to_csv('resnet_submission.csv', index=False)\n",
    "\n",
    "resnet_preds_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03fb71b",
   "metadata": {
    "papermill": {
     "duration": 0.055214,
     "end_time": "2022-06-13T12:45:42.931418",
     "exception": false,
     "start_time": "2022-06-13T12:45:42.876204",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# EfficientNetB5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f9eb57f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T12:45:43.048905Z",
     "iopub.status.busy": "2022-06-13T12:45:43.048562Z",
     "iopub.status.idle": "2022-06-13T12:46:14.499411Z",
     "shell.execute_reply": "2022-06-13T12:46:14.498604Z"
    },
    "papermill": {
     "duration": 31.568836,
     "end_time": "2022-06-13T12:46:14.558186",
     "exception": false,
     "start_time": "2022-06-13T12:45:42.989350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_m_21k-361418a2.pth\" to /root/.cache/torch/hub/checkpoints/tf_efficientnetv2_m_21k-361418a2.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (model): EfficientNet(\n",
       "    (conv_stem): Conv2dSame(3, 24, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "    (bn1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): SiLU(inplace=True)\n",
       "    (blocks): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): ConvBnAct(\n",
       "          (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvBnAct(\n",
       "          (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): ConvBnAct(\n",
       "          (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): EdgeResidual(\n",
       "          (conv_exp): Conv2dSame(24, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (bn1): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): EdgeResidual(\n",
       "          (conv_exp): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): EdgeResidual(\n",
       "          (conv_exp): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): EdgeResidual(\n",
       "          (conv_exp): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): EdgeResidual(\n",
       "          (conv_exp): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): EdgeResidual(\n",
       "          (conv_exp): Conv2dSame(48, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (bn1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(192, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): EdgeResidual(\n",
       "          (conv_exp): Conv2d(80, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): EdgeResidual(\n",
       "          (conv_exp): Conv2d(80, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): EdgeResidual(\n",
       "          (conv_exp): Conv2d(80, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): EdgeResidual(\n",
       "          (conv_exp): Conv2d(80, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2dSame(320, 320, kernel_size=(3, 3), stride=(2, 2), groups=320, bias=False)\n",
       "          (bn2): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(320, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(20, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(320, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640, bias=False)\n",
       "          (bn2): BatchNorm2d(640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(640, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(40, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640, bias=False)\n",
       "          (bn2): BatchNorm2d(640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(640, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(40, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640, bias=False)\n",
       "          (bn2): BatchNorm2d(640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(640, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(40, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (conv_pw): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640, bias=False)\n",
       "          (bn2): BatchNorm2d(640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(640, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(40, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): InvertedResidual(\n",
       "          (conv_pw): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640, bias=False)\n",
       "          (bn2): BatchNorm2d(640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(640, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(40, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (6): InvertedResidual(\n",
       "          (conv_pw): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640, bias=False)\n",
       "          (bn2): BatchNorm2d(640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(640, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(40, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (bn2): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(960, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
       "          (bn2): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
       "          (bn2): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
       "          (bn2): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
       "          (bn2): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): InvertedResidual(\n",
       "          (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
       "          (bn2): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (6): InvertedResidual(\n",
       "          (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
       "          (bn2): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (7): InvertedResidual(\n",
       "          (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
       "          (bn2): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (8): InvertedResidual(\n",
       "          (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
       "          (bn2): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (9): InvertedResidual(\n",
       "          (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
       "          (bn2): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (10): InvertedResidual(\n",
       "          (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
       "          (bn2): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (11): InvertedResidual(\n",
       "          (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
       "          (bn2): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (12): InvertedResidual(\n",
       "          (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
       "          (bn2): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (13): InvertedResidual(\n",
       "          (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1056, 1056, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1056, bias=False)\n",
       "          (bn2): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2dSame(1056, 1056, kernel_size=(3, 3), stride=(2, 2), groups=1056, bias=False)\n",
       "          (bn2): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1056, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (6): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (7): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (8): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (9): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (10): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (11): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (12): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (13): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (14): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (15): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (16): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (17): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
       "          (bn2): BatchNorm2d(3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(3072, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(128, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
       "          (bn2): BatchNorm2d(3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(3072, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(128, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
       "          (bn2): BatchNorm2d(3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(3072, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(128, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (conv_pw): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
       "          (bn2): BatchNorm2d(3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(3072, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(128, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_head): Conv2d(512, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act2): SiLU(inplace=True)\n",
       "    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "    (classifier): Sequential(\n",
       "      (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Linear(in_features=1280, out_features=512, bias=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Linear(in_features=512, out_features=100, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model_name = 'tf_efficientnetv2_m_in21k'\n",
    "\n",
    "backbone = timm.create_model(model_name,pretrained=True)\n",
    "B5_model = CustomModel(backbone)\n",
    "\n",
    "B5_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "052d4207",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T12:46:14.667960Z",
     "iopub.status.busy": "2022-06-13T12:46:14.667639Z",
     "iopub.status.idle": "2022-06-13T12:46:21.439877Z",
     "shell.execute_reply": "2022-06-13T12:46:21.439109Z"
    },
    "papermill": {
     "duration": 6.829386,
     "end_time": "2022-06-13T12:46:21.441651",
     "exception": false,
     "start_time": "2022-06-13T12:46:14.612265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('../input/sorghum-identification-12345/tf_efficientnetv2_m_in21k_best.pt')\n",
    "B5_model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90d75bae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T12:46:21.551067Z",
     "iopub.status.busy": "2022-06-13T12:46:21.550720Z",
     "iopub.status.idle": "2022-06-13T13:11:46.073548Z",
     "shell.execute_reply": "2022-06-13T13:11:46.072493Z"
    },
    "papermill": {
     "duration": 1524.580132,
     "end_time": "2022-06-13T13:11:46.075900",
     "exception": false,
     "start_time": "2022-06-13T12:46:21.495768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 739/739 [25:24<00:00,  2.06s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "cnt = 0\n",
    "\n",
    "B5_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image, label in tqdm(testing_dataloader):\n",
    "        image = image.to(device)\n",
    "        outputs = model(image)\n",
    "        for i in range(len(outputs)):\n",
    "            B5_preds.append(outputs[i][:100])\n",
    "#         resnet_preds.append(outputs[0][:100])\n",
    "        preds = outputs.detach().cpu()\n",
    "        predictions.append(preds.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a52cfa80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T13:11:46.271046Z",
     "iopub.status.busy": "2022-06-13T13:11:46.270695Z",
     "iopub.status.idle": "2022-06-13T13:11:46.797711Z",
     "shell.execute_reply": "2022-06-13T13:11:46.796762Z"
    },
    "papermill": {
     "duration": 0.626667,
     "end_time": "2022-06-13T13:11:46.800059",
     "exception": false,
     "start_time": "2022-06-13T13:11:46.173392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "B5_preds_ = []\n",
    "\n",
    "for i in range(len(B5_preds)):\n",
    "    B5_preds_.append(B5_preds[i].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32361837",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T13:11:46.997520Z",
     "iopub.status.busy": "2022-06-13T13:11:46.997094Z",
     "iopub.status.idle": "2022-06-13T13:15:28.971040Z",
     "shell.execute_reply": "2022-06-13T13:15:28.970073Z"
    },
    "papermill": {
     "duration": 222.075173,
     "end_time": "2022-06-13T13:15:28.973386",
     "exception": false,
     "start_time": "2022-06-13T13:11:46.898213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "B5_preds_list = pd.DataFrame({'0_class':[]})\n",
    "\n",
    "for i in range(99, 0, -1): \n",
    "    B5_preds_list.insert(1, \"{}_class\".format(i), [])\n",
    "\n",
    "for i in range(len(B5_preds_)):\n",
    "    B5_preds_list.loc[i] = B5_preds_[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d11e739d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T13:15:29.167893Z",
     "iopub.status.busy": "2022-06-13T13:15:29.167250Z",
     "iopub.status.idle": "2022-06-13T13:15:29.205749Z",
     "shell.execute_reply": "2022-06-13T13:15:29.204817Z"
    },
    "papermill": {
     "duration": 0.136645,
     "end_time": "2022-06-13T13:15:29.207983",
     "exception": false,
     "start_time": "2022-06-13T13:15:29.071338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_class</th>\n",
       "      <th>1_class</th>\n",
       "      <th>2_class</th>\n",
       "      <th>3_class</th>\n",
       "      <th>4_class</th>\n",
       "      <th>5_class</th>\n",
       "      <th>6_class</th>\n",
       "      <th>7_class</th>\n",
       "      <th>8_class</th>\n",
       "      <th>9_class</th>\n",
       "      <th>...</th>\n",
       "      <th>90_class</th>\n",
       "      <th>91_class</th>\n",
       "      <th>92_class</th>\n",
       "      <th>93_class</th>\n",
       "      <th>94_class</th>\n",
       "      <th>95_class</th>\n",
       "      <th>96_class</th>\n",
       "      <th>97_class</th>\n",
       "      <th>98_class</th>\n",
       "      <th>99_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.801618</td>\n",
       "      <td>-2.975569</td>\n",
       "      <td>-1.010900</td>\n",
       "      <td>0.004538</td>\n",
       "      <td>0.350488</td>\n",
       "      <td>-4.283353</td>\n",
       "      <td>-4.044071</td>\n",
       "      <td>-1.817170</td>\n",
       "      <td>-3.906837</td>\n",
       "      <td>-5.185993</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.440704</td>\n",
       "      <td>-5.510637</td>\n",
       "      <td>-3.565371</td>\n",
       "      <td>-4.367570</td>\n",
       "      <td>-3.586168</td>\n",
       "      <td>-4.021678</td>\n",
       "      <td>-5.994962</td>\n",
       "      <td>-4.712767</td>\n",
       "      <td>-2.815238</td>\n",
       "      <td>-1.952368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.122581</td>\n",
       "      <td>-1.158474</td>\n",
       "      <td>-8.431793</td>\n",
       "      <td>-7.456934</td>\n",
       "      <td>-7.085815</td>\n",
       "      <td>-5.844799</td>\n",
       "      <td>-4.357646</td>\n",
       "      <td>-6.926689</td>\n",
       "      <td>-6.120798</td>\n",
       "      <td>0.864344</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.683628</td>\n",
       "      <td>-4.942460</td>\n",
       "      <td>-5.104192</td>\n",
       "      <td>-3.478405</td>\n",
       "      <td>-3.778158</td>\n",
       "      <td>-4.185591</td>\n",
       "      <td>-8.658447</td>\n",
       "      <td>-5.647788</td>\n",
       "      <td>-5.539834</td>\n",
       "      <td>-7.571363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-5.502335</td>\n",
       "      <td>-1.471846</td>\n",
       "      <td>8.994921</td>\n",
       "      <td>-1.644590</td>\n",
       "      <td>2.780464</td>\n",
       "      <td>-4.725876</td>\n",
       "      <td>-4.252627</td>\n",
       "      <td>-5.074745</td>\n",
       "      <td>0.530009</td>\n",
       "      <td>-0.460110</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.465601</td>\n",
       "      <td>-5.762954</td>\n",
       "      <td>-5.205477</td>\n",
       "      <td>-4.648644</td>\n",
       "      <td>-1.996166</td>\n",
       "      <td>-7.006385</td>\n",
       "      <td>-6.289888</td>\n",
       "      <td>-5.483791</td>\n",
       "      <td>-3.666237</td>\n",
       "      <td>-3.386745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.438631</td>\n",
       "      <td>-2.900598</td>\n",
       "      <td>-2.842964</td>\n",
       "      <td>-2.837878</td>\n",
       "      <td>-2.651607</td>\n",
       "      <td>-3.997262</td>\n",
       "      <td>-2.419579</td>\n",
       "      <td>-2.409816</td>\n",
       "      <td>-1.671640</td>\n",
       "      <td>-2.249619</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150546</td>\n",
       "      <td>-3.092925</td>\n",
       "      <td>-1.705112</td>\n",
       "      <td>-3.123335</td>\n",
       "      <td>-2.864271</td>\n",
       "      <td>-2.119378</td>\n",
       "      <td>-2.402761</td>\n",
       "      <td>-2.806396</td>\n",
       "      <td>-3.288818</td>\n",
       "      <td>-2.080209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.856150</td>\n",
       "      <td>-0.279349</td>\n",
       "      <td>-4.226741</td>\n",
       "      <td>-2.668807</td>\n",
       "      <td>-2.526486</td>\n",
       "      <td>-1.116520</td>\n",
       "      <td>-0.516609</td>\n",
       "      <td>-5.410573</td>\n",
       "      <td>-4.072620</td>\n",
       "      <td>-1.866208</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.197257</td>\n",
       "      <td>-7.641562</td>\n",
       "      <td>-1.674403</td>\n",
       "      <td>-4.825066</td>\n",
       "      <td>-5.543388</td>\n",
       "      <td>-5.794561</td>\n",
       "      <td>-5.393475</td>\n",
       "      <td>-6.552042</td>\n",
       "      <td>0.802193</td>\n",
       "      <td>-5.098389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23634</th>\n",
       "      <td>-7.223331</td>\n",
       "      <td>-5.498235</td>\n",
       "      <td>-7.183830</td>\n",
       "      <td>-6.044033</td>\n",
       "      <td>-3.454030</td>\n",
       "      <td>-2.869962</td>\n",
       "      <td>-5.262556</td>\n",
       "      <td>-7.025408</td>\n",
       "      <td>-7.600406</td>\n",
       "      <td>-8.049458</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.938259</td>\n",
       "      <td>-7.105239</td>\n",
       "      <td>-5.009425</td>\n",
       "      <td>-5.695383</td>\n",
       "      <td>-4.508875</td>\n",
       "      <td>-4.701503</td>\n",
       "      <td>-6.263385</td>\n",
       "      <td>-6.115704</td>\n",
       "      <td>-5.626700</td>\n",
       "      <td>-7.625852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23635</th>\n",
       "      <td>-4.395103</td>\n",
       "      <td>-5.223400</td>\n",
       "      <td>-2.850434</td>\n",
       "      <td>-0.915796</td>\n",
       "      <td>-1.466926</td>\n",
       "      <td>-4.560630</td>\n",
       "      <td>-1.761602</td>\n",
       "      <td>-4.885693</td>\n",
       "      <td>-3.868564</td>\n",
       "      <td>-7.511627</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.109028</td>\n",
       "      <td>-6.600894</td>\n",
       "      <td>-5.732879</td>\n",
       "      <td>-3.651545</td>\n",
       "      <td>-4.017533</td>\n",
       "      <td>-4.707999</td>\n",
       "      <td>-6.996043</td>\n",
       "      <td>-2.584617</td>\n",
       "      <td>-3.975872</td>\n",
       "      <td>-3.213772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23636</th>\n",
       "      <td>-7.714730</td>\n",
       "      <td>-1.864583</td>\n",
       "      <td>-7.216725</td>\n",
       "      <td>-7.704274</td>\n",
       "      <td>-9.781572</td>\n",
       "      <td>-10.407336</td>\n",
       "      <td>-3.443444</td>\n",
       "      <td>-6.472396</td>\n",
       "      <td>-7.049388</td>\n",
       "      <td>-6.616426</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.660957</td>\n",
       "      <td>-8.343438</td>\n",
       "      <td>-9.968068</td>\n",
       "      <td>-7.830461</td>\n",
       "      <td>3.343957</td>\n",
       "      <td>-7.972091</td>\n",
       "      <td>-7.800015</td>\n",
       "      <td>-5.601650</td>\n",
       "      <td>-6.300623</td>\n",
       "      <td>-10.347117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23637</th>\n",
       "      <td>-5.930204</td>\n",
       "      <td>-7.474098</td>\n",
       "      <td>-6.242562</td>\n",
       "      <td>-5.108381</td>\n",
       "      <td>-5.487892</td>\n",
       "      <td>0.793515</td>\n",
       "      <td>-6.607038</td>\n",
       "      <td>-3.528217</td>\n",
       "      <td>-6.645050</td>\n",
       "      <td>-8.464906</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.553557</td>\n",
       "      <td>-5.071094</td>\n",
       "      <td>-7.368112</td>\n",
       "      <td>-6.799894</td>\n",
       "      <td>-3.683982</td>\n",
       "      <td>-5.894517</td>\n",
       "      <td>-4.045214</td>\n",
       "      <td>-7.671366</td>\n",
       "      <td>-3.703960</td>\n",
       "      <td>-5.831462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23638</th>\n",
       "      <td>-0.383157</td>\n",
       "      <td>-1.644464</td>\n",
       "      <td>-1.487512</td>\n",
       "      <td>-0.678131</td>\n",
       "      <td>-2.647243</td>\n",
       "      <td>-3.429455</td>\n",
       "      <td>-1.926242</td>\n",
       "      <td>-2.053614</td>\n",
       "      <td>-1.152958</td>\n",
       "      <td>-1.459475</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.268825</td>\n",
       "      <td>-2.186186</td>\n",
       "      <td>-2.537500</td>\n",
       "      <td>-0.500601</td>\n",
       "      <td>-2.068858</td>\n",
       "      <td>-2.378392</td>\n",
       "      <td>-2.535790</td>\n",
       "      <td>-2.654131</td>\n",
       "      <td>-1.075358</td>\n",
       "      <td>-1.216355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23639 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0_class   1_class   2_class   3_class   4_class    5_class   6_class  \\\n",
       "0     -4.801618 -2.975569 -1.010900  0.004538  0.350488  -4.283353 -4.044071   \n",
       "1     -3.122581 -1.158474 -8.431793 -7.456934 -7.085815  -5.844799 -4.357646   \n",
       "2     -5.502335 -1.471846  8.994921 -1.644590  2.780464  -4.725876 -4.252627   \n",
       "3     -1.438631 -2.900598 -2.842964 -2.837878 -2.651607  -3.997262 -2.419579   \n",
       "4     -5.856150 -0.279349 -4.226741 -2.668807 -2.526486  -1.116520 -0.516609   \n",
       "...         ...       ...       ...       ...       ...        ...       ...   \n",
       "23634 -7.223331 -5.498235 -7.183830 -6.044033 -3.454030  -2.869962 -5.262556   \n",
       "23635 -4.395103 -5.223400 -2.850434 -0.915796 -1.466926  -4.560630 -1.761602   \n",
       "23636 -7.714730 -1.864583 -7.216725 -7.704274 -9.781572 -10.407336 -3.443444   \n",
       "23637 -5.930204 -7.474098 -6.242562 -5.108381 -5.487892   0.793515 -6.607038   \n",
       "23638 -0.383157 -1.644464 -1.487512 -0.678131 -2.647243  -3.429455 -1.926242   \n",
       "\n",
       "        7_class   8_class   9_class  ...  90_class  91_class  92_class  \\\n",
       "0     -1.817170 -3.906837 -5.185993  ... -4.440704 -5.510637 -3.565371   \n",
       "1     -6.926689 -6.120798  0.864344  ... -1.683628 -4.942460 -5.104192   \n",
       "2     -5.074745  0.530009 -0.460110  ... -3.465601 -5.762954 -5.205477   \n",
       "3     -2.409816 -1.671640 -2.249619  ... -2.150546 -3.092925 -1.705112   \n",
       "4     -5.410573 -4.072620 -1.866208  ... -2.197257 -7.641562 -1.674403   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "23634 -7.025408 -7.600406 -8.049458  ... -5.938259 -7.105239 -5.009425   \n",
       "23635 -4.885693 -3.868564 -7.511627  ... -5.109028 -6.600894 -5.732879   \n",
       "23636 -6.472396 -7.049388 -6.616426  ... -5.660957 -8.343438 -9.968068   \n",
       "23637 -3.528217 -6.645050 -8.464906  ... -3.553557 -5.071094 -7.368112   \n",
       "23638 -2.053614 -1.152958 -1.459475  ... -3.268825 -2.186186 -2.537500   \n",
       "\n",
       "       93_class  94_class  95_class  96_class  97_class  98_class   99_class  \n",
       "0     -4.367570 -3.586168 -4.021678 -5.994962 -4.712767 -2.815238  -1.952368  \n",
       "1     -3.478405 -3.778158 -4.185591 -8.658447 -5.647788 -5.539834  -7.571363  \n",
       "2     -4.648644 -1.996166 -7.006385 -6.289888 -5.483791 -3.666237  -3.386745  \n",
       "3     -3.123335 -2.864271 -2.119378 -2.402761 -2.806396 -3.288818  -2.080209  \n",
       "4     -4.825066 -5.543388 -5.794561 -5.393475 -6.552042  0.802193  -5.098389  \n",
       "...         ...       ...       ...       ...       ...       ...        ...  \n",
       "23634 -5.695383 -4.508875 -4.701503 -6.263385 -6.115704 -5.626700  -7.625852  \n",
       "23635 -3.651545 -4.017533 -4.707999 -6.996043 -2.584617 -3.975872  -3.213772  \n",
       "23636 -7.830461  3.343957 -7.972091 -7.800015 -5.601650 -6.300623 -10.347117  \n",
       "23637 -6.799894 -3.683982 -5.894517 -4.045214 -7.671366 -3.703960  -5.831462  \n",
       "23638 -0.500601 -2.068858 -2.378392 -2.535790 -2.654131 -1.075358  -1.216355  \n",
       "\n",
       "[23639 rows x 100 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B5_preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9c91c4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T13:15:29.399830Z",
     "iopub.status.busy": "2022-06-13T13:15:29.399240Z",
     "iopub.status.idle": "2022-06-13T13:15:33.416994Z",
     "shell.execute_reply": "2022-06-13T13:15:33.416221Z"
    },
    "papermill": {
     "duration": 4.115239,
     "end_time": "2022-06-13T13:15:33.418882",
     "exception": false,
     "start_time": "2022-06-13T13:15:29.303643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_class</th>\n",
       "      <th>1_class</th>\n",
       "      <th>2_class</th>\n",
       "      <th>3_class</th>\n",
       "      <th>4_class</th>\n",
       "      <th>5_class</th>\n",
       "      <th>6_class</th>\n",
       "      <th>7_class</th>\n",
       "      <th>8_class</th>\n",
       "      <th>9_class</th>\n",
       "      <th>...</th>\n",
       "      <th>90_class</th>\n",
       "      <th>91_class</th>\n",
       "      <th>92_class</th>\n",
       "      <th>93_class</th>\n",
       "      <th>94_class</th>\n",
       "      <th>95_class</th>\n",
       "      <th>96_class</th>\n",
       "      <th>97_class</th>\n",
       "      <th>98_class</th>\n",
       "      <th>99_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.801618</td>\n",
       "      <td>-2.975569</td>\n",
       "      <td>-1.010900</td>\n",
       "      <td>0.004538</td>\n",
       "      <td>0.350488</td>\n",
       "      <td>-4.283353</td>\n",
       "      <td>-4.044071</td>\n",
       "      <td>-1.817170</td>\n",
       "      <td>-3.906837</td>\n",
       "      <td>-5.185993</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.440704</td>\n",
       "      <td>-5.510637</td>\n",
       "      <td>-3.565371</td>\n",
       "      <td>-4.367570</td>\n",
       "      <td>-3.586168</td>\n",
       "      <td>-4.021678</td>\n",
       "      <td>-5.994962</td>\n",
       "      <td>-4.712767</td>\n",
       "      <td>-2.815238</td>\n",
       "      <td>-1.952368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.122581</td>\n",
       "      <td>-1.158474</td>\n",
       "      <td>-8.431793</td>\n",
       "      <td>-7.456934</td>\n",
       "      <td>-7.085815</td>\n",
       "      <td>-5.844799</td>\n",
       "      <td>-4.357646</td>\n",
       "      <td>-6.926689</td>\n",
       "      <td>-6.120798</td>\n",
       "      <td>0.864344</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.683628</td>\n",
       "      <td>-4.942460</td>\n",
       "      <td>-5.104192</td>\n",
       "      <td>-3.478405</td>\n",
       "      <td>-3.778158</td>\n",
       "      <td>-4.185591</td>\n",
       "      <td>-8.658447</td>\n",
       "      <td>-5.647788</td>\n",
       "      <td>-5.539834</td>\n",
       "      <td>-7.571363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-5.502335</td>\n",
       "      <td>-1.471846</td>\n",
       "      <td>8.994921</td>\n",
       "      <td>-1.644590</td>\n",
       "      <td>2.780464</td>\n",
       "      <td>-4.725876</td>\n",
       "      <td>-4.252627</td>\n",
       "      <td>-5.074745</td>\n",
       "      <td>0.530009</td>\n",
       "      <td>-0.460110</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.465601</td>\n",
       "      <td>-5.762954</td>\n",
       "      <td>-5.205477</td>\n",
       "      <td>-4.648644</td>\n",
       "      <td>-1.996166</td>\n",
       "      <td>-7.006385</td>\n",
       "      <td>-6.289888</td>\n",
       "      <td>-5.483791</td>\n",
       "      <td>-3.666237</td>\n",
       "      <td>-3.386745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.438631</td>\n",
       "      <td>-2.900598</td>\n",
       "      <td>-2.842964</td>\n",
       "      <td>-2.837878</td>\n",
       "      <td>-2.651607</td>\n",
       "      <td>-3.997262</td>\n",
       "      <td>-2.419579</td>\n",
       "      <td>-2.409816</td>\n",
       "      <td>-1.671640</td>\n",
       "      <td>-2.249619</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150546</td>\n",
       "      <td>-3.092925</td>\n",
       "      <td>-1.705112</td>\n",
       "      <td>-3.123335</td>\n",
       "      <td>-2.864271</td>\n",
       "      <td>-2.119378</td>\n",
       "      <td>-2.402761</td>\n",
       "      <td>-2.806396</td>\n",
       "      <td>-3.288818</td>\n",
       "      <td>-2.080209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.856150</td>\n",
       "      <td>-0.279349</td>\n",
       "      <td>-4.226741</td>\n",
       "      <td>-2.668807</td>\n",
       "      <td>-2.526486</td>\n",
       "      <td>-1.116520</td>\n",
       "      <td>-0.516609</td>\n",
       "      <td>-5.410573</td>\n",
       "      <td>-4.072620</td>\n",
       "      <td>-1.866208</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.197257</td>\n",
       "      <td>-7.641562</td>\n",
       "      <td>-1.674403</td>\n",
       "      <td>-4.825066</td>\n",
       "      <td>-5.543388</td>\n",
       "      <td>-5.794561</td>\n",
       "      <td>-5.393475</td>\n",
       "      <td>-6.552042</td>\n",
       "      <td>0.802193</td>\n",
       "      <td>-5.098389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0_class   1_class   2_class   3_class   4_class   5_class   6_class  \\\n",
       "0 -4.801618 -2.975569 -1.010900  0.004538  0.350488 -4.283353 -4.044071   \n",
       "1 -3.122581 -1.158474 -8.431793 -7.456934 -7.085815 -5.844799 -4.357646   \n",
       "2 -5.502335 -1.471846  8.994921 -1.644590  2.780464 -4.725876 -4.252627   \n",
       "3 -1.438631 -2.900598 -2.842964 -2.837878 -2.651607 -3.997262 -2.419579   \n",
       "4 -5.856150 -0.279349 -4.226741 -2.668807 -2.526486 -1.116520 -0.516609   \n",
       "\n",
       "    7_class   8_class   9_class  ...  90_class  91_class  92_class  93_class  \\\n",
       "0 -1.817170 -3.906837 -5.185993  ... -4.440704 -5.510637 -3.565371 -4.367570   \n",
       "1 -6.926689 -6.120798  0.864344  ... -1.683628 -4.942460 -5.104192 -3.478405   \n",
       "2 -5.074745  0.530009 -0.460110  ... -3.465601 -5.762954 -5.205477 -4.648644   \n",
       "3 -2.409816 -1.671640 -2.249619  ... -2.150546 -3.092925 -1.705112 -3.123335   \n",
       "4 -5.410573 -4.072620 -1.866208  ... -2.197257 -7.641562 -1.674403 -4.825066   \n",
       "\n",
       "   94_class  95_class  96_class  97_class  98_class  99_class  \n",
       "0 -3.586168 -4.021678 -5.994962 -4.712767 -2.815238 -1.952368  \n",
       "1 -3.778158 -4.185591 -8.658447 -5.647788 -5.539834 -7.571363  \n",
       "2 -1.996166 -7.006385 -6.289888 -5.483791 -3.666237 -3.386745  \n",
       "3 -2.864271 -2.119378 -2.402761 -2.806396 -3.288818 -2.080209  \n",
       "4 -5.543388 -5.794561 -5.393475 -6.552042  0.802193 -5.098389  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B5_preds_list.to_csv('B5_submission.csv', index=False)\n",
    "\n",
    "B5_preds_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26fa0d4",
   "metadata": {
    "papermill": {
     "duration": 0.097887,
     "end_time": "2022-06-13T13:15:33.615717",
     "exception": false,
     "start_time": "2022-06-13T13:15:33.517830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# EfficientNetB4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfdb0361",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T13:15:33.811377Z",
     "iopub.status.busy": "2022-06-13T13:15:33.811012Z",
     "iopub.status.idle": "2022-06-13T13:15:46.949096Z",
     "shell.execute_reply": "2022-06-13T13:15:46.947838Z"
    },
    "papermill": {
     "duration": 13.238675,
     "end_time": "2022-06-13T13:15:46.951214",
     "exception": false,
     "start_time": "2022-06-13T13:15:33.712539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet_pytorch\r\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet_pytorch) (1.11.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (4.2.0)\r\n",
      "Building wheels for collected packages: efficientnet_pytorch\r\n",
      "  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=1492bac251fbf034459a17f3ae8a1a794a82cf14714272f9fd1483ef463e06d4\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/0e/cc/b2/49e74588263573ff778da58cc99b9c6349b496636a7e165be6\r\n",
      "Successfully built efficientnet_pytorch\r\n",
      "Installing collected packages: efficientnet_pytorch\r\n",
      "Successfully installed efficientnet_pytorch-0.7.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1de9c4cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T13:15:47.150411Z",
     "iopub.status.busy": "2022-06-13T13:15:47.150024Z",
     "iopub.status.idle": "2022-06-13T13:15:48.935407Z",
     "shell.execute_reply": "2022-06-13T13:15:48.934222Z"
    },
    "papermill": {
     "duration": 1.887315,
     "end_time": "2022-06-13T13:15:48.938739",
     "exception": false,
     "start_time": "2022-06-13T13:15:47.051424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 48, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "  )\n",
       "  (_bn0): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        48, 48, kernel_size=(3, 3), stride=[1, 1], groups=48, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        48, 12, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        12, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        24, 24, kernel_size=(3, 3), stride=(1, 1), groups=24, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        24, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 24, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        192, 192, kernel_size=(5, 5), stride=[2, 2], groups=192, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        192, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        336, 336, kernel_size=(3, 3), stride=[2, 2], groups=336, bias=False\n",
       "        (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        336, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (16): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[1, 1], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (17): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (18): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (19): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (20): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (21): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (22): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=[2, 2], groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (23): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (24): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (25): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (26): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (27): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (28): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (29): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (30): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(3, 3), stride=[1, 1], groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 448, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (31): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        448, 2688, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        2688, 2688, kernel_size=(3, 3), stride=(1, 1), groups=2688, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        2688, 112, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        112, 2688, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        2688, 448, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_bn1): BatchNorm2d(1792, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.4, inplace=False)\n",
       "  (_fc): Linear(in_features=1792, out_features=1000, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "model_B4 = EfficientNet.from_name('efficientnet-b4')\n",
    "model_B4.load_state_dict(torch.load(\"../input/test-for-kaggle-0426/epoch25.pt\", map_location='cuda'))\n",
    "\n",
    "model_B4.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a3fe176",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T13:15:49.159704Z",
     "iopub.status.busy": "2022-06-13T13:15:49.159264Z",
     "iopub.status.idle": "2022-06-13T13:40:45.932423Z",
     "shell.execute_reply": "2022-06-13T13:40:45.930682Z"
    },
    "papermill": {
     "duration": 1496.885355,
     "end_time": "2022-06-13T13:40:45.934409",
     "exception": false,
     "start_time": "2022-06-13T13:15:49.049054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 739/739 [24:56<00:00,  2.03s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "cnt = 0\n",
    "\n",
    "B4_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image, label in tqdm(testing_dataloader):\n",
    "        image = image.to(device)\n",
    "        outputs = model_B4(image)\n",
    "        for i in range(len(outputs)):\n",
    "            B4_preds.append(outputs[i][:100])\n",
    "#         resnet_preds.append(outputs[0][:100])\n",
    "        preds = outputs.detach().cpu()\n",
    "        predictions.append(preds.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eda4fcf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T13:40:46.247012Z",
     "iopub.status.busy": "2022-06-13T13:40:46.245189Z",
     "iopub.status.idle": "2022-06-13T13:40:46.795989Z",
     "shell.execute_reply": "2022-06-13T13:40:46.795197Z"
    },
    "papermill": {
     "duration": 0.708778,
     "end_time": "2022-06-13T13:40:46.798135",
     "exception": false,
     "start_time": "2022-06-13T13:40:46.089357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "B4_preds_ = []\n",
    "\n",
    "for i in range(len(B4_preds)):\n",
    "    B4_preds_.append(B4_preds[i].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e29c4fb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T13:40:47.080087Z",
     "iopub.status.busy": "2022-06-13T13:40:47.079749Z",
     "iopub.status.idle": "2022-06-13T13:44:25.973154Z",
     "shell.execute_reply": "2022-06-13T13:44:25.972306Z"
    },
    "papermill": {
     "duration": 219.035363,
     "end_time": "2022-06-13T13:44:25.975307",
     "exception": false,
     "start_time": "2022-06-13T13:40:46.939944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "B4_preds_list = pd.DataFrame({'0_class':[]})\n",
    "\n",
    "for i in range(99, 0, -1): \n",
    "    B4_preds_list.insert(1, \"{}_class\".format(i), [])\n",
    "\n",
    "for i in range(len(B5_preds_)):\n",
    "    B4_preds_list.loc[i] = B4_preds_[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f21cde9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T13:44:26.260869Z",
     "iopub.status.busy": "2022-06-13T13:44:26.260538Z",
     "iopub.status.idle": "2022-06-13T13:44:30.267529Z",
     "shell.execute_reply": "2022-06-13T13:44:30.266498Z"
    },
    "papermill": {
     "duration": 4.149039,
     "end_time": "2022-06-13T13:44:30.269650",
     "exception": false,
     "start_time": "2022-06-13T13:44:26.120611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_class</th>\n",
       "      <th>1_class</th>\n",
       "      <th>2_class</th>\n",
       "      <th>3_class</th>\n",
       "      <th>4_class</th>\n",
       "      <th>5_class</th>\n",
       "      <th>6_class</th>\n",
       "      <th>7_class</th>\n",
       "      <th>8_class</th>\n",
       "      <th>9_class</th>\n",
       "      <th>...</th>\n",
       "      <th>90_class</th>\n",
       "      <th>91_class</th>\n",
       "      <th>92_class</th>\n",
       "      <th>93_class</th>\n",
       "      <th>94_class</th>\n",
       "      <th>95_class</th>\n",
       "      <th>96_class</th>\n",
       "      <th>97_class</th>\n",
       "      <th>98_class</th>\n",
       "      <th>99_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.571729</td>\n",
       "      <td>5.977733</td>\n",
       "      <td>9.317001</td>\n",
       "      <td>4.836597</td>\n",
       "      <td>10.742593</td>\n",
       "      <td>-1.167939</td>\n",
       "      <td>2.632571</td>\n",
       "      <td>2.478869</td>\n",
       "      <td>2.740776</td>\n",
       "      <td>3.915955</td>\n",
       "      <td>...</td>\n",
       "      <td>5.307129</td>\n",
       "      <td>0.584284</td>\n",
       "      <td>-6.052864</td>\n",
       "      <td>3.654924</td>\n",
       "      <td>4.087985</td>\n",
       "      <td>-2.358377</td>\n",
       "      <td>0.874906</td>\n",
       "      <td>3.654458</td>\n",
       "      <td>-3.120832</td>\n",
       "      <td>2.567825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.153302</td>\n",
       "      <td>5.257663</td>\n",
       "      <td>-1.771159</td>\n",
       "      <td>-2.009251</td>\n",
       "      <td>3.975573</td>\n",
       "      <td>0.781690</td>\n",
       "      <td>6.442517</td>\n",
       "      <td>-1.134265</td>\n",
       "      <td>2.609756</td>\n",
       "      <td>8.876288</td>\n",
       "      <td>...</td>\n",
       "      <td>6.548178</td>\n",
       "      <td>6.517748</td>\n",
       "      <td>1.615280</td>\n",
       "      <td>5.225770</td>\n",
       "      <td>1.504519</td>\n",
       "      <td>3.653682</td>\n",
       "      <td>0.868913</td>\n",
       "      <td>0.422606</td>\n",
       "      <td>2.367360</td>\n",
       "      <td>1.117713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.170490</td>\n",
       "      <td>5.333634</td>\n",
       "      <td>16.227261</td>\n",
       "      <td>6.155414</td>\n",
       "      <td>11.360859</td>\n",
       "      <td>-4.582110</td>\n",
       "      <td>0.473570</td>\n",
       "      <td>0.876260</td>\n",
       "      <td>6.308060</td>\n",
       "      <td>3.260907</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.221341</td>\n",
       "      <td>-0.275140</td>\n",
       "      <td>-4.477224</td>\n",
       "      <td>5.252504</td>\n",
       "      <td>2.675060</td>\n",
       "      <td>-4.745510</td>\n",
       "      <td>2.464423</td>\n",
       "      <td>-1.880267</td>\n",
       "      <td>-1.138426</td>\n",
       "      <td>3.315015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.998833</td>\n",
       "      <td>-0.157031</td>\n",
       "      <td>1.312434</td>\n",
       "      <td>0.691659</td>\n",
       "      <td>2.927119</td>\n",
       "      <td>2.488154</td>\n",
       "      <td>4.697551</td>\n",
       "      <td>4.778651</td>\n",
       "      <td>4.995743</td>\n",
       "      <td>3.199096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447240</td>\n",
       "      <td>-2.934232</td>\n",
       "      <td>2.130785</td>\n",
       "      <td>0.838407</td>\n",
       "      <td>-1.519703</td>\n",
       "      <td>-0.061749</td>\n",
       "      <td>-1.849497</td>\n",
       "      <td>1.312758</td>\n",
       "      <td>1.179950</td>\n",
       "      <td>2.895253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.754803</td>\n",
       "      <td>10.598209</td>\n",
       "      <td>3.455296</td>\n",
       "      <td>4.492938</td>\n",
       "      <td>4.692679</td>\n",
       "      <td>1.566526</td>\n",
       "      <td>6.785053</td>\n",
       "      <td>-1.870538</td>\n",
       "      <td>7.657426</td>\n",
       "      <td>12.105474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.519930</td>\n",
       "      <td>-2.021164</td>\n",
       "      <td>3.169623</td>\n",
       "      <td>9.916356</td>\n",
       "      <td>3.480946</td>\n",
       "      <td>-1.801913</td>\n",
       "      <td>-2.876096</td>\n",
       "      <td>0.658748</td>\n",
       "      <td>11.843788</td>\n",
       "      <td>0.946106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0_class    1_class    2_class   3_class    4_class   5_class   6_class  \\\n",
       "0 -2.571729   5.977733   9.317001  4.836597  10.742593 -1.167939  2.632571   \n",
       "1  7.153302   5.257663  -1.771159 -2.009251   3.975573  0.781690  6.442517   \n",
       "2 -3.170490   5.333634  16.227261  6.155414  11.360859 -4.582110  0.473570   \n",
       "3 -0.998833  -0.157031   1.312434  0.691659   2.927119  2.488154  4.697551   \n",
       "4 -2.754803  10.598209   3.455296  4.492938   4.692679  1.566526  6.785053   \n",
       "\n",
       "    7_class   8_class    9_class  ...  90_class  91_class  92_class  93_class  \\\n",
       "0  2.478869  2.740776   3.915955  ...  5.307129  0.584284 -6.052864  3.654924   \n",
       "1 -1.134265  2.609756   8.876288  ...  6.548178  6.517748  1.615280  5.225770   \n",
       "2  0.876260  6.308060   3.260907  ... -2.221341 -0.275140 -4.477224  5.252504   \n",
       "3  4.778651  4.995743   3.199096  ...  0.447240 -2.934232  2.130785  0.838407   \n",
       "4 -1.870538  7.657426  12.105474  ... -0.519930 -2.021164  3.169623  9.916356   \n",
       "\n",
       "   94_class  95_class  96_class  97_class   98_class  99_class  \n",
       "0  4.087985 -2.358377  0.874906  3.654458  -3.120832  2.567825  \n",
       "1  1.504519  3.653682  0.868913  0.422606   2.367360  1.117713  \n",
       "2  2.675060 -4.745510  2.464423 -1.880267  -1.138426  3.315015  \n",
       "3 -1.519703 -0.061749 -1.849497  1.312758   1.179950  2.895253  \n",
       "4  3.480946 -1.801913 -2.876096  0.658748  11.843788  0.946106  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B4_preds_list.to_csv('B4_submission.csv', index=False)\n",
    "\n",
    "B4_preds_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36072720",
   "metadata": {
    "papermill": {
     "duration": 0.14145,
     "end_time": "2022-06-13T13:44:30.554200",
     "exception": false,
     "start_time": "2022-06-13T13:44:30.412750",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ensemble Learning (b4+b5+resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3ec4562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T13:44:30.883161Z",
     "iopub.status.busy": "2022-06-13T13:44:30.882816Z",
     "iopub.status.idle": "2022-06-13T13:44:31.857584Z",
     "shell.execute_reply": "2022-06-13T13:44:31.856798Z"
    },
    "papermill": {
     "duration": 1.119598,
     "end_time": "2022-06-13T13:44:31.859590",
     "exception": false,
     "start_time": "2022-06-13T13:44:30.739992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_class</th>\n",
       "      <th>1_class</th>\n",
       "      <th>2_class</th>\n",
       "      <th>3_class</th>\n",
       "      <th>4_class</th>\n",
       "      <th>5_class</th>\n",
       "      <th>6_class</th>\n",
       "      <th>7_class</th>\n",
       "      <th>8_class</th>\n",
       "      <th>9_class</th>\n",
       "      <th>...</th>\n",
       "      <th>90_class</th>\n",
       "      <th>91_class</th>\n",
       "      <th>92_class</th>\n",
       "      <th>93_class</th>\n",
       "      <th>94_class</th>\n",
       "      <th>95_class</th>\n",
       "      <th>96_class</th>\n",
       "      <th>97_class</th>\n",
       "      <th>98_class</th>\n",
       "      <th>99_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.801618</td>\n",
       "      <td>-2.975569</td>\n",
       "      <td>-1.010900</td>\n",
       "      <td>0.004538</td>\n",
       "      <td>0.350488</td>\n",
       "      <td>-4.283353</td>\n",
       "      <td>-4.044071</td>\n",
       "      <td>-1.817170</td>\n",
       "      <td>-3.906837</td>\n",
       "      <td>-5.185993</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.440704</td>\n",
       "      <td>-5.510637</td>\n",
       "      <td>-3.565371</td>\n",
       "      <td>-4.367570</td>\n",
       "      <td>-3.586168</td>\n",
       "      <td>-4.021678</td>\n",
       "      <td>-5.994962</td>\n",
       "      <td>-4.712767</td>\n",
       "      <td>-2.815238</td>\n",
       "      <td>-1.952368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.122581</td>\n",
       "      <td>-1.158474</td>\n",
       "      <td>-8.431793</td>\n",
       "      <td>-7.456934</td>\n",
       "      <td>-7.085815</td>\n",
       "      <td>-5.844799</td>\n",
       "      <td>-4.357646</td>\n",
       "      <td>-6.926689</td>\n",
       "      <td>-6.120798</td>\n",
       "      <td>0.864344</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.683628</td>\n",
       "      <td>-4.942460</td>\n",
       "      <td>-5.104192</td>\n",
       "      <td>-3.478405</td>\n",
       "      <td>-3.778158</td>\n",
       "      <td>-4.185591</td>\n",
       "      <td>-8.658447</td>\n",
       "      <td>-5.647788</td>\n",
       "      <td>-5.539834</td>\n",
       "      <td>-7.571363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-5.502335</td>\n",
       "      <td>-1.471846</td>\n",
       "      <td>8.994921</td>\n",
       "      <td>-1.644590</td>\n",
       "      <td>2.780464</td>\n",
       "      <td>-4.725876</td>\n",
       "      <td>-4.252627</td>\n",
       "      <td>-5.074745</td>\n",
       "      <td>0.530009</td>\n",
       "      <td>-0.460110</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.465601</td>\n",
       "      <td>-5.762954</td>\n",
       "      <td>-5.205477</td>\n",
       "      <td>-4.648644</td>\n",
       "      <td>-1.996166</td>\n",
       "      <td>-7.006385</td>\n",
       "      <td>-6.289888</td>\n",
       "      <td>-5.483791</td>\n",
       "      <td>-3.666237</td>\n",
       "      <td>-3.386745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.438631</td>\n",
       "      <td>-2.900598</td>\n",
       "      <td>-2.842964</td>\n",
       "      <td>-2.837878</td>\n",
       "      <td>-2.651607</td>\n",
       "      <td>-3.997262</td>\n",
       "      <td>-2.419579</td>\n",
       "      <td>-2.409816</td>\n",
       "      <td>-1.671640</td>\n",
       "      <td>-2.249619</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.150546</td>\n",
       "      <td>-3.092925</td>\n",
       "      <td>-1.705112</td>\n",
       "      <td>-3.123335</td>\n",
       "      <td>-2.864271</td>\n",
       "      <td>-2.119378</td>\n",
       "      <td>-2.402761</td>\n",
       "      <td>-2.806396</td>\n",
       "      <td>-3.288818</td>\n",
       "      <td>-2.080209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.856150</td>\n",
       "      <td>-0.279349</td>\n",
       "      <td>-4.226741</td>\n",
       "      <td>-2.668807</td>\n",
       "      <td>-2.526486</td>\n",
       "      <td>-1.116520</td>\n",
       "      <td>-0.516609</td>\n",
       "      <td>-5.410573</td>\n",
       "      <td>-4.072620</td>\n",
       "      <td>-1.866208</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.197257</td>\n",
       "      <td>-7.641562</td>\n",
       "      <td>-1.674403</td>\n",
       "      <td>-4.825066</td>\n",
       "      <td>-5.543388</td>\n",
       "      <td>-5.794561</td>\n",
       "      <td>-5.393475</td>\n",
       "      <td>-6.552042</td>\n",
       "      <td>0.802193</td>\n",
       "      <td>-5.098389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0_class   1_class   2_class   3_class   4_class   5_class   6_class  \\\n",
       "0 -4.801618 -2.975569 -1.010900  0.004538  0.350488 -4.283353 -4.044071   \n",
       "1 -3.122581 -1.158474 -8.431793 -7.456934 -7.085815 -5.844799 -4.357646   \n",
       "2 -5.502335 -1.471846  8.994921 -1.644590  2.780464 -4.725876 -4.252627   \n",
       "3 -1.438631 -2.900598 -2.842964 -2.837878 -2.651607 -3.997262 -2.419579   \n",
       "4 -5.856150 -0.279349 -4.226741 -2.668807 -2.526486 -1.116520 -0.516609   \n",
       "\n",
       "    7_class   8_class   9_class  ...  90_class  91_class  92_class  93_class  \\\n",
       "0 -1.817170 -3.906837 -5.185993  ... -4.440704 -5.510637 -3.565371 -4.367570   \n",
       "1 -6.926689 -6.120798  0.864344  ... -1.683628 -4.942460 -5.104192 -3.478405   \n",
       "2 -5.074745  0.530009 -0.460110  ... -3.465601 -5.762954 -5.205477 -4.648644   \n",
       "3 -2.409816 -1.671640 -2.249619  ... -2.150546 -3.092925 -1.705112 -3.123335   \n",
       "4 -5.410573 -4.072620 -1.866208  ... -2.197257 -7.641562 -1.674403 -4.825066   \n",
       "\n",
       "   94_class  95_class  96_class  97_class  98_class  99_class  \n",
       "0 -3.586168 -4.021678 -5.994962 -4.712767 -2.815238 -1.952368  \n",
       "1 -3.778158 -4.185591 -8.658447 -5.647788 -5.539834 -7.571363  \n",
       "2 -1.996166 -7.006385 -6.289888 -5.483791 -3.666237 -3.386745  \n",
       "3 -2.864271 -2.119378 -2.402761 -2.806396 -3.288818 -2.080209  \n",
       "4 -5.543388 -5.794561 -5.393475 -6.552042  0.802193 -5.098389  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_result = pd.read_csv('../input/resnet-submission/resnet_submission.csv')\n",
    "\n",
    "resnet_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcdc270b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T13:44:32.148904Z",
     "iopub.status.busy": "2022-06-13T13:44:32.148569Z",
     "iopub.status.idle": "2022-06-13T13:44:33.078072Z",
     "shell.execute_reply": "2022-06-13T13:44:33.077310Z"
    },
    "papermill": {
     "duration": 1.077387,
     "end_time": "2022-06-13T13:44:33.079859",
     "exception": false,
     "start_time": "2022-06-13T13:44:32.002472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_class</th>\n",
       "      <th>1_class</th>\n",
       "      <th>2_class</th>\n",
       "      <th>3_class</th>\n",
       "      <th>4_class</th>\n",
       "      <th>5_class</th>\n",
       "      <th>6_class</th>\n",
       "      <th>7_class</th>\n",
       "      <th>8_class</th>\n",
       "      <th>9_class</th>\n",
       "      <th>...</th>\n",
       "      <th>90_class</th>\n",
       "      <th>91_class</th>\n",
       "      <th>92_class</th>\n",
       "      <th>93_class</th>\n",
       "      <th>94_class</th>\n",
       "      <th>95_class</th>\n",
       "      <th>96_class</th>\n",
       "      <th>97_class</th>\n",
       "      <th>98_class</th>\n",
       "      <th>99_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.892773</td>\n",
       "      <td>-3.966359</td>\n",
       "      <td>-2.954552</td>\n",
       "      <td>-1.383131</td>\n",
       "      <td>-1.230138</td>\n",
       "      <td>-1.605695</td>\n",
       "      <td>7.928654</td>\n",
       "      <td>4.240064</td>\n",
       "      <td>0.428775</td>\n",
       "      <td>0.965414</td>\n",
       "      <td>...</td>\n",
       "      <td>8.436068</td>\n",
       "      <td>1.253415</td>\n",
       "      <td>-5.170112</td>\n",
       "      <td>3.990971</td>\n",
       "      <td>3.111282</td>\n",
       "      <td>-1.808589</td>\n",
       "      <td>-2.022345</td>\n",
       "      <td>-7.563506</td>\n",
       "      <td>2.165346</td>\n",
       "      <td>-2.866596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.648182</td>\n",
       "      <td>-0.164810</td>\n",
       "      <td>-2.596343</td>\n",
       "      <td>0.256760</td>\n",
       "      <td>1.757425</td>\n",
       "      <td>-3.846923</td>\n",
       "      <td>1.825878</td>\n",
       "      <td>0.021879</td>\n",
       "      <td>1.588355</td>\n",
       "      <td>-0.191997</td>\n",
       "      <td>...</td>\n",
       "      <td>3.332699</td>\n",
       "      <td>-0.817180</td>\n",
       "      <td>-1.912100</td>\n",
       "      <td>-2.092699</td>\n",
       "      <td>0.625315</td>\n",
       "      <td>-5.305366</td>\n",
       "      <td>0.989622</td>\n",
       "      <td>-1.957383</td>\n",
       "      <td>-3.334180</td>\n",
       "      <td>-3.192983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.481743</td>\n",
       "      <td>0.391381</td>\n",
       "      <td>-0.247089</td>\n",
       "      <td>2.485051</td>\n",
       "      <td>-1.057789</td>\n",
       "      <td>0.696110</td>\n",
       "      <td>-4.091997</td>\n",
       "      <td>-1.179258</td>\n",
       "      <td>-0.500628</td>\n",
       "      <td>-3.465500</td>\n",
       "      <td>...</td>\n",
       "      <td>7.796316</td>\n",
       "      <td>-1.606795</td>\n",
       "      <td>2.412722</td>\n",
       "      <td>-0.056671</td>\n",
       "      <td>2.127215</td>\n",
       "      <td>1.371859</td>\n",
       "      <td>3.259758</td>\n",
       "      <td>-6.339055</td>\n",
       "      <td>0.069074</td>\n",
       "      <td>0.115981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.715939</td>\n",
       "      <td>1.363132</td>\n",
       "      <td>-2.073458</td>\n",
       "      <td>2.271285</td>\n",
       "      <td>-1.479022</td>\n",
       "      <td>-1.946633</td>\n",
       "      <td>1.016834</td>\n",
       "      <td>-2.028146</td>\n",
       "      <td>-2.438629</td>\n",
       "      <td>-0.708824</td>\n",
       "      <td>...</td>\n",
       "      <td>3.569920</td>\n",
       "      <td>3.384302</td>\n",
       "      <td>0.336098</td>\n",
       "      <td>-3.883107</td>\n",
       "      <td>-0.452094</td>\n",
       "      <td>-3.788221</td>\n",
       "      <td>6.431556</td>\n",
       "      <td>-1.021312</td>\n",
       "      <td>1.686525</td>\n",
       "      <td>1.532885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.381458</td>\n",
       "      <td>2.214819</td>\n",
       "      <td>4.023111</td>\n",
       "      <td>4.014326</td>\n",
       "      <td>1.586189</td>\n",
       "      <td>-1.834667</td>\n",
       "      <td>-0.580804</td>\n",
       "      <td>6.224906</td>\n",
       "      <td>-0.865272</td>\n",
       "      <td>-1.505553</td>\n",
       "      <td>...</td>\n",
       "      <td>4.452855</td>\n",
       "      <td>2.541638</td>\n",
       "      <td>-0.841258</td>\n",
       "      <td>1.238704</td>\n",
       "      <td>-3.511993</td>\n",
       "      <td>1.813707</td>\n",
       "      <td>1.622686</td>\n",
       "      <td>1.142952</td>\n",
       "      <td>0.504797</td>\n",
       "      <td>-0.824462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0_class   1_class   2_class   3_class   4_class   5_class   6_class  \\\n",
       "0  0.892773 -3.966359 -2.954552 -1.383131 -1.230138 -1.605695  7.928654   \n",
       "1  1.648182 -0.164810 -2.596343  0.256760  1.757425 -3.846923  1.825878   \n",
       "2 -0.481743  0.391381 -0.247089  2.485051 -1.057789  0.696110 -4.091997   \n",
       "3 -0.715939  1.363132 -2.073458  2.271285 -1.479022 -1.946633  1.016834   \n",
       "4 -1.381458  2.214819  4.023111  4.014326  1.586189 -1.834667 -0.580804   \n",
       "\n",
       "    7_class   8_class   9_class  ...  90_class  91_class  92_class  93_class  \\\n",
       "0  4.240064  0.428775  0.965414  ...  8.436068  1.253415 -5.170112  3.990971   \n",
       "1  0.021879  1.588355 -0.191997  ...  3.332699 -0.817180 -1.912100 -2.092699   \n",
       "2 -1.179258 -0.500628 -3.465500  ...  7.796316 -1.606795  2.412722 -0.056671   \n",
       "3 -2.028146 -2.438629 -0.708824  ...  3.569920  3.384302  0.336098 -3.883107   \n",
       "4  6.224906 -0.865272 -1.505553  ...  4.452855  2.541638 -0.841258  1.238704   \n",
       "\n",
       "   94_class  95_class  96_class  97_class  98_class  99_class  \n",
       "0  3.111282 -1.808589 -2.022345 -7.563506  2.165346 -2.866596  \n",
       "1  0.625315 -5.305366  0.989622 -1.957383 -3.334180 -3.192983  \n",
       "2  2.127215  1.371859  3.259758 -6.339055  0.069074  0.115981  \n",
       "3 -0.452094 -3.788221  6.431556 -1.021312  1.686525  1.532885  \n",
       "4 -3.511993  1.813707  1.622686  1.142952  0.504797 -0.824462  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b5_result = pd.read_csv('../input/b5-sub/B5_submission.csv')\n",
    "\n",
    "b5_result = b5_result * 10\n",
    "\n",
    "b5_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "539e6cec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T13:44:33.492177Z",
     "iopub.status.busy": "2022-06-13T13:44:33.491626Z",
     "iopub.status.idle": "2022-06-13T13:44:34.594692Z",
     "shell.execute_reply": "2022-06-13T13:44:34.592333Z"
    },
    "papermill": {
     "duration": 1.376145,
     "end_time": "2022-06-13T13:44:34.598272",
     "exception": false,
     "start_time": "2022-06-13T13:44:33.222127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_class</th>\n",
       "      <th>1_class</th>\n",
       "      <th>2_class</th>\n",
       "      <th>3_class</th>\n",
       "      <th>4_class</th>\n",
       "      <th>5_class</th>\n",
       "      <th>6_class</th>\n",
       "      <th>7_class</th>\n",
       "      <th>8_class</th>\n",
       "      <th>9_class</th>\n",
       "      <th>...</th>\n",
       "      <th>90_class</th>\n",
       "      <th>91_class</th>\n",
       "      <th>92_class</th>\n",
       "      <th>93_class</th>\n",
       "      <th>94_class</th>\n",
       "      <th>95_class</th>\n",
       "      <th>96_class</th>\n",
       "      <th>97_class</th>\n",
       "      <th>98_class</th>\n",
       "      <th>99_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.971418</td>\n",
       "      <td>7.601085</td>\n",
       "      <td>6.253662</td>\n",
       "      <td>4.200216</td>\n",
       "      <td>11.877116</td>\n",
       "      <td>-2.441239</td>\n",
       "      <td>5.244116</td>\n",
       "      <td>-0.190037</td>\n",
       "      <td>3.397834</td>\n",
       "      <td>4.547288</td>\n",
       "      <td>...</td>\n",
       "      <td>4.986811</td>\n",
       "      <td>1.825171</td>\n",
       "      <td>-7.296851</td>\n",
       "      <td>6.224649</td>\n",
       "      <td>3.114617</td>\n",
       "      <td>-0.308570</td>\n",
       "      <td>0.266424</td>\n",
       "      <td>0.494667</td>\n",
       "      <td>-5.222482</td>\n",
       "      <td>4.137463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.472752</td>\n",
       "      <td>5.776456</td>\n",
       "      <td>-2.737694</td>\n",
       "      <td>-1.720482</td>\n",
       "      <td>3.652450</td>\n",
       "      <td>0.276067</td>\n",
       "      <td>6.608448</td>\n",
       "      <td>0.864106</td>\n",
       "      <td>4.592688</td>\n",
       "      <td>8.591151</td>\n",
       "      <td>...</td>\n",
       "      <td>7.866305</td>\n",
       "      <td>6.736653</td>\n",
       "      <td>2.717580</td>\n",
       "      <td>6.264142</td>\n",
       "      <td>2.302805</td>\n",
       "      <td>2.655147</td>\n",
       "      <td>1.018003</td>\n",
       "      <td>-0.706559</td>\n",
       "      <td>2.704113</td>\n",
       "      <td>1.300578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.553531</td>\n",
       "      <td>6.922027</td>\n",
       "      <td>15.746008</td>\n",
       "      <td>5.205610</td>\n",
       "      <td>11.518566</td>\n",
       "      <td>-2.183513</td>\n",
       "      <td>0.309439</td>\n",
       "      <td>1.676199</td>\n",
       "      <td>8.241667</td>\n",
       "      <td>3.346487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024887</td>\n",
       "      <td>-0.938581</td>\n",
       "      <td>-4.564708</td>\n",
       "      <td>3.530740</td>\n",
       "      <td>1.694313</td>\n",
       "      <td>-5.831820</td>\n",
       "      <td>1.436764</td>\n",
       "      <td>-1.031949</td>\n",
       "      <td>-1.899167</td>\n",
       "      <td>3.243559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.805562</td>\n",
       "      <td>0.220727</td>\n",
       "      <td>0.537807</td>\n",
       "      <td>-0.447724</td>\n",
       "      <td>4.249485</td>\n",
       "      <td>1.422490</td>\n",
       "      <td>3.598542</td>\n",
       "      <td>4.122486</td>\n",
       "      <td>6.112689</td>\n",
       "      <td>3.032334</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.533039</td>\n",
       "      <td>-2.219996</td>\n",
       "      <td>1.833291</td>\n",
       "      <td>1.130475</td>\n",
       "      <td>-0.753881</td>\n",
       "      <td>0.365807</td>\n",
       "      <td>-2.043800</td>\n",
       "      <td>1.860660</td>\n",
       "      <td>0.673981</td>\n",
       "      <td>3.255714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.025319</td>\n",
       "      <td>9.523963</td>\n",
       "      <td>3.477371</td>\n",
       "      <td>4.063594</td>\n",
       "      <td>4.372114</td>\n",
       "      <td>2.653487</td>\n",
       "      <td>5.072325</td>\n",
       "      <td>-1.769767</td>\n",
       "      <td>4.885382</td>\n",
       "      <td>8.662114</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.438711</td>\n",
       "      <td>-0.779596</td>\n",
       "      <td>3.789347</td>\n",
       "      <td>6.610578</td>\n",
       "      <td>3.096414</td>\n",
       "      <td>0.834889</td>\n",
       "      <td>-1.003939</td>\n",
       "      <td>3.763621</td>\n",
       "      <td>12.943184</td>\n",
       "      <td>1.951309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0_class   1_class    2_class   3_class    4_class   5_class   6_class  \\\n",
       "0 -1.971418  7.601085   6.253662  4.200216  11.877116 -2.441239  5.244116   \n",
       "1  7.472752  5.776456  -2.737694 -1.720482   3.652450  0.276067  6.608448   \n",
       "2 -3.553531  6.922027  15.746008  5.205610  11.518566 -2.183513  0.309439   \n",
       "3 -1.805562  0.220727   0.537807 -0.447724   4.249485  1.422490  3.598542   \n",
       "4 -1.025319  9.523963   3.477371  4.063594   4.372114  2.653487  5.072325   \n",
       "\n",
       "    7_class   8_class   9_class  ...  90_class  91_class  92_class  93_class  \\\n",
       "0 -0.190037  3.397834  4.547288  ...  4.986811  1.825171 -7.296851  6.224649   \n",
       "1  0.864106  4.592688  8.591151  ...  7.866305  6.736653  2.717580  6.264142   \n",
       "2  1.676199  8.241667  3.346487  ... -0.024887 -0.938581 -4.564708  3.530740   \n",
       "3  4.122486  6.112689  3.032334  ... -0.533039 -2.219996  1.833291  1.130475   \n",
       "4 -1.769767  4.885382  8.662114  ... -0.438711 -0.779596  3.789347  6.610578   \n",
       "\n",
       "   94_class  95_class  96_class  97_class   98_class  99_class  \n",
       "0  3.114617 -0.308570  0.266424  0.494667  -5.222482  4.137463  \n",
       "1  2.302805  2.655147  1.018003 -0.706559   2.704113  1.300578  \n",
       "2  1.694313 -5.831820  1.436764 -1.031949  -1.899167  3.243559  \n",
       "3 -0.753881  0.365807 -2.043800  1.860660   0.673981  3.255714  \n",
       "4  3.096414  0.834889 -1.003939  3.763621  12.943184  1.951309  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b4_result = pd.read_csv('../input/b4-sub/B4_submission.csv')\n",
    "\n",
    "# b4_result = b4_result * 10\n",
    "\n",
    "b4_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b21654d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T13:44:35.095535Z",
     "iopub.status.busy": "2022-06-13T13:44:35.094974Z",
     "iopub.status.idle": "2022-06-13T13:44:35.334787Z",
     "shell.execute_reply": "2022-06-13T13:44:35.333977Z"
    },
    "papermill": {
     "duration": 0.489049,
     "end_time": "2022-06-13T13:44:35.337134",
     "exception": false,
     "start_time": "2022-06-13T13:44:34.848085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "b5_result['max_value'] = b5_result.max(axis=1)\n",
    "b5_result['class'] = b5_result.idxmax(axis=1)\n",
    "\n",
    "resnet_result['max_value'] = resnet_result.max(axis=1)\n",
    "resnet_result['class'] = resnet_result.idxmax(axis=1)\n",
    "\n",
    "b4_result['max_value'] = b4_result.max(axis=1)\n",
    "b4_result['class'] = b4_result.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07e8a8d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T13:44:35.624781Z",
     "iopub.status.busy": "2022-06-13T13:44:35.624440Z",
     "iopub.status.idle": "2022-06-13T13:44:35.646767Z",
     "shell.execute_reply": "2022-06-13T13:44:35.646045Z"
    },
    "papermill": {
     "duration": 0.16657,
     "end_time": "2022-06-13T13:44:35.648579",
     "exception": false,
     "start_time": "2022-06-13T13:44:35.482009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_class</th>\n",
       "      <th>1_class</th>\n",
       "      <th>2_class</th>\n",
       "      <th>3_class</th>\n",
       "      <th>4_class</th>\n",
       "      <th>5_class</th>\n",
       "      <th>6_class</th>\n",
       "      <th>7_class</th>\n",
       "      <th>8_class</th>\n",
       "      <th>9_class</th>\n",
       "      <th>...</th>\n",
       "      <th>92_class</th>\n",
       "      <th>93_class</th>\n",
       "      <th>94_class</th>\n",
       "      <th>95_class</th>\n",
       "      <th>96_class</th>\n",
       "      <th>97_class</th>\n",
       "      <th>98_class</th>\n",
       "      <th>99_class</th>\n",
       "      <th>max_value</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.892773</td>\n",
       "      <td>-3.966359</td>\n",
       "      <td>-2.954552</td>\n",
       "      <td>-1.383131</td>\n",
       "      <td>-1.230138</td>\n",
       "      <td>-1.605695</td>\n",
       "      <td>7.928654</td>\n",
       "      <td>4.240064</td>\n",
       "      <td>0.428775</td>\n",
       "      <td>0.965414</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.170112</td>\n",
       "      <td>3.990971</td>\n",
       "      <td>3.111282</td>\n",
       "      <td>-1.808589</td>\n",
       "      <td>-2.022345</td>\n",
       "      <td>-7.563506</td>\n",
       "      <td>2.165346</td>\n",
       "      <td>-2.866596</td>\n",
       "      <td>8.436068</td>\n",
       "      <td>90_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.648182</td>\n",
       "      <td>-0.164810</td>\n",
       "      <td>-2.596343</td>\n",
       "      <td>0.256760</td>\n",
       "      <td>1.757425</td>\n",
       "      <td>-3.846923</td>\n",
       "      <td>1.825878</td>\n",
       "      <td>0.021879</td>\n",
       "      <td>1.588355</td>\n",
       "      <td>-0.191997</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.912100</td>\n",
       "      <td>-2.092699</td>\n",
       "      <td>0.625315</td>\n",
       "      <td>-5.305366</td>\n",
       "      <td>0.989622</td>\n",
       "      <td>-1.957383</td>\n",
       "      <td>-3.334180</td>\n",
       "      <td>-3.192983</td>\n",
       "      <td>8.935465</td>\n",
       "      <td>29_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.481743</td>\n",
       "      <td>0.391381</td>\n",
       "      <td>-0.247089</td>\n",
       "      <td>2.485051</td>\n",
       "      <td>-1.057789</td>\n",
       "      <td>0.696110</td>\n",
       "      <td>-4.091997</td>\n",
       "      <td>-1.179258</td>\n",
       "      <td>-0.500628</td>\n",
       "      <td>-3.465500</td>\n",
       "      <td>...</td>\n",
       "      <td>2.412722</td>\n",
       "      <td>-0.056671</td>\n",
       "      <td>2.127215</td>\n",
       "      <td>1.371859</td>\n",
       "      <td>3.259758</td>\n",
       "      <td>-6.339055</td>\n",
       "      <td>0.069074</td>\n",
       "      <td>0.115981</td>\n",
       "      <td>7.796316</td>\n",
       "      <td>90_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.715939</td>\n",
       "      <td>1.363132</td>\n",
       "      <td>-2.073458</td>\n",
       "      <td>2.271285</td>\n",
       "      <td>-1.479022</td>\n",
       "      <td>-1.946633</td>\n",
       "      <td>1.016834</td>\n",
       "      <td>-2.028146</td>\n",
       "      <td>-2.438629</td>\n",
       "      <td>-0.708824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.336098</td>\n",
       "      <td>-3.883107</td>\n",
       "      <td>-0.452094</td>\n",
       "      <td>-3.788221</td>\n",
       "      <td>6.431556</td>\n",
       "      <td>-1.021312</td>\n",
       "      <td>1.686525</td>\n",
       "      <td>1.532885</td>\n",
       "      <td>7.316633</td>\n",
       "      <td>29_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.381458</td>\n",
       "      <td>2.214819</td>\n",
       "      <td>4.023111</td>\n",
       "      <td>4.014326</td>\n",
       "      <td>1.586189</td>\n",
       "      <td>-1.834667</td>\n",
       "      <td>-0.580804</td>\n",
       "      <td>6.224906</td>\n",
       "      <td>-0.865272</td>\n",
       "      <td>-1.505553</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.841258</td>\n",
       "      <td>1.238704</td>\n",
       "      <td>-3.511993</td>\n",
       "      <td>1.813707</td>\n",
       "      <td>1.622686</td>\n",
       "      <td>1.142952</td>\n",
       "      <td>0.504797</td>\n",
       "      <td>-0.824462</td>\n",
       "      <td>8.706151</td>\n",
       "      <td>60_class</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0_class   1_class   2_class   3_class   4_class   5_class   6_class  \\\n",
       "0  0.892773 -3.966359 -2.954552 -1.383131 -1.230138 -1.605695  7.928654   \n",
       "1  1.648182 -0.164810 -2.596343  0.256760  1.757425 -3.846923  1.825878   \n",
       "2 -0.481743  0.391381 -0.247089  2.485051 -1.057789  0.696110 -4.091997   \n",
       "3 -0.715939  1.363132 -2.073458  2.271285 -1.479022 -1.946633  1.016834   \n",
       "4 -1.381458  2.214819  4.023111  4.014326  1.586189 -1.834667 -0.580804   \n",
       "\n",
       "    7_class   8_class   9_class  ...  92_class  93_class  94_class  95_class  \\\n",
       "0  4.240064  0.428775  0.965414  ... -5.170112  3.990971  3.111282 -1.808589   \n",
       "1  0.021879  1.588355 -0.191997  ... -1.912100 -2.092699  0.625315 -5.305366   \n",
       "2 -1.179258 -0.500628 -3.465500  ...  2.412722 -0.056671  2.127215  1.371859   \n",
       "3 -2.028146 -2.438629 -0.708824  ...  0.336098 -3.883107 -0.452094 -3.788221   \n",
       "4  6.224906 -0.865272 -1.505553  ... -0.841258  1.238704 -3.511993  1.813707   \n",
       "\n",
       "   96_class  97_class  98_class  99_class  max_value     class  \n",
       "0 -2.022345 -7.563506  2.165346 -2.866596   8.436068  90_class  \n",
       "1  0.989622 -1.957383 -3.334180 -3.192983   8.935465  29_class  \n",
       "2  3.259758 -6.339055  0.069074  0.115981   7.796316  90_class  \n",
       "3  6.431556 -1.021312  1.686525  1.532885   7.316633  29_class  \n",
       "4  1.622686  1.142952  0.504797 -0.824462   8.706151  60_class  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b5_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d06c963d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T13:44:35.934995Z",
     "iopub.status.busy": "2022-06-13T13:44:35.934484Z",
     "iopub.status.idle": "2022-06-13T13:44:35.957676Z",
     "shell.execute_reply": "2022-06-13T13:44:35.956770Z"
    },
    "papermill": {
     "duration": 0.166702,
     "end_time": "2022-06-13T13:44:35.959769",
     "exception": false,
     "start_time": "2022-06-13T13:44:35.793067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_class</th>\n",
       "      <th>1_class</th>\n",
       "      <th>2_class</th>\n",
       "      <th>3_class</th>\n",
       "      <th>4_class</th>\n",
       "      <th>5_class</th>\n",
       "      <th>6_class</th>\n",
       "      <th>7_class</th>\n",
       "      <th>8_class</th>\n",
       "      <th>9_class</th>\n",
       "      <th>...</th>\n",
       "      <th>92_class</th>\n",
       "      <th>93_class</th>\n",
       "      <th>94_class</th>\n",
       "      <th>95_class</th>\n",
       "      <th>96_class</th>\n",
       "      <th>97_class</th>\n",
       "      <th>98_class</th>\n",
       "      <th>99_class</th>\n",
       "      <th>max_value</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.801618</td>\n",
       "      <td>-2.975569</td>\n",
       "      <td>-1.010900</td>\n",
       "      <td>0.004538</td>\n",
       "      <td>0.350488</td>\n",
       "      <td>-4.283353</td>\n",
       "      <td>-4.044071</td>\n",
       "      <td>-1.817170</td>\n",
       "      <td>-3.906837</td>\n",
       "      <td>-5.185993</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.565371</td>\n",
       "      <td>-4.367570</td>\n",
       "      <td>-3.586168</td>\n",
       "      <td>-4.021678</td>\n",
       "      <td>-5.994962</td>\n",
       "      <td>-4.712767</td>\n",
       "      <td>-2.815238</td>\n",
       "      <td>-1.952368</td>\n",
       "      <td>5.471979</td>\n",
       "      <td>86_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.122581</td>\n",
       "      <td>-1.158474</td>\n",
       "      <td>-8.431793</td>\n",
       "      <td>-7.456934</td>\n",
       "      <td>-7.085815</td>\n",
       "      <td>-5.844799</td>\n",
       "      <td>-4.357646</td>\n",
       "      <td>-6.926689</td>\n",
       "      <td>-6.120798</td>\n",
       "      <td>0.864344</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.104192</td>\n",
       "      <td>-3.478405</td>\n",
       "      <td>-3.778158</td>\n",
       "      <td>-4.185591</td>\n",
       "      <td>-8.658447</td>\n",
       "      <td>-5.647788</td>\n",
       "      <td>-5.539834</td>\n",
       "      <td>-7.571363</td>\n",
       "      <td>19.686892</td>\n",
       "      <td>55_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-5.502335</td>\n",
       "      <td>-1.471846</td>\n",
       "      <td>8.994921</td>\n",
       "      <td>-1.644590</td>\n",
       "      <td>2.780464</td>\n",
       "      <td>-4.725876</td>\n",
       "      <td>-4.252627</td>\n",
       "      <td>-5.074745</td>\n",
       "      <td>0.530009</td>\n",
       "      <td>-0.460110</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.205477</td>\n",
       "      <td>-4.648644</td>\n",
       "      <td>-1.996166</td>\n",
       "      <td>-7.006385</td>\n",
       "      <td>-6.289888</td>\n",
       "      <td>-5.483791</td>\n",
       "      <td>-3.666237</td>\n",
       "      <td>-3.386745</td>\n",
       "      <td>8.994921</td>\n",
       "      <td>2_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.438631</td>\n",
       "      <td>-2.900598</td>\n",
       "      <td>-2.842964</td>\n",
       "      <td>-2.837878</td>\n",
       "      <td>-2.651607</td>\n",
       "      <td>-3.997262</td>\n",
       "      <td>-2.419579</td>\n",
       "      <td>-2.409816</td>\n",
       "      <td>-1.671640</td>\n",
       "      <td>-2.249619</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.705112</td>\n",
       "      <td>-3.123335</td>\n",
       "      <td>-2.864271</td>\n",
       "      <td>-2.119378</td>\n",
       "      <td>-2.402761</td>\n",
       "      <td>-2.806396</td>\n",
       "      <td>-3.288818</td>\n",
       "      <td>-2.080209</td>\n",
       "      <td>13.621403</td>\n",
       "      <td>48_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.856150</td>\n",
       "      <td>-0.279349</td>\n",
       "      <td>-4.226741</td>\n",
       "      <td>-2.668807</td>\n",
       "      <td>-2.526486</td>\n",
       "      <td>-1.116520</td>\n",
       "      <td>-0.516609</td>\n",
       "      <td>-5.410573</td>\n",
       "      <td>-4.072620</td>\n",
       "      <td>-1.866208</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.674403</td>\n",
       "      <td>-4.825066</td>\n",
       "      <td>-5.543388</td>\n",
       "      <td>-5.794561</td>\n",
       "      <td>-5.393475</td>\n",
       "      <td>-6.552042</td>\n",
       "      <td>0.802193</td>\n",
       "      <td>-5.098389</td>\n",
       "      <td>4.296759</td>\n",
       "      <td>13_class</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0_class   1_class   2_class   3_class   4_class   5_class   6_class  \\\n",
       "0 -4.801618 -2.975569 -1.010900  0.004538  0.350488 -4.283353 -4.044071   \n",
       "1 -3.122581 -1.158474 -8.431793 -7.456934 -7.085815 -5.844799 -4.357646   \n",
       "2 -5.502335 -1.471846  8.994921 -1.644590  2.780464 -4.725876 -4.252627   \n",
       "3 -1.438631 -2.900598 -2.842964 -2.837878 -2.651607 -3.997262 -2.419579   \n",
       "4 -5.856150 -0.279349 -4.226741 -2.668807 -2.526486 -1.116520 -0.516609   \n",
       "\n",
       "    7_class   8_class   9_class  ...  92_class  93_class  94_class  95_class  \\\n",
       "0 -1.817170 -3.906837 -5.185993  ... -3.565371 -4.367570 -3.586168 -4.021678   \n",
       "1 -6.926689 -6.120798  0.864344  ... -5.104192 -3.478405 -3.778158 -4.185591   \n",
       "2 -5.074745  0.530009 -0.460110  ... -5.205477 -4.648644 -1.996166 -7.006385   \n",
       "3 -2.409816 -1.671640 -2.249619  ... -1.705112 -3.123335 -2.864271 -2.119378   \n",
       "4 -5.410573 -4.072620 -1.866208  ... -1.674403 -4.825066 -5.543388 -5.794561   \n",
       "\n",
       "   96_class  97_class  98_class  99_class  max_value     class  \n",
       "0 -5.994962 -4.712767 -2.815238 -1.952368   5.471979  86_class  \n",
       "1 -8.658447 -5.647788 -5.539834 -7.571363  19.686892  55_class  \n",
       "2 -6.289888 -5.483791 -3.666237 -3.386745   8.994921   2_class  \n",
       "3 -2.402761 -2.806396 -3.288818 -2.080209  13.621403  48_class  \n",
       "4 -5.393475 -6.552042  0.802193 -5.098389   4.296759  13_class  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d859c831",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T13:44:36.251801Z",
     "iopub.status.busy": "2022-06-13T13:44:36.251451Z",
     "iopub.status.idle": "2022-06-13T13:44:36.274719Z",
     "shell.execute_reply": "2022-06-13T13:44:36.273751Z"
    },
    "papermill": {
     "duration": 0.172705,
     "end_time": "2022-06-13T13:44:36.276957",
     "exception": false,
     "start_time": "2022-06-13T13:44:36.104252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_class</th>\n",
       "      <th>1_class</th>\n",
       "      <th>2_class</th>\n",
       "      <th>3_class</th>\n",
       "      <th>4_class</th>\n",
       "      <th>5_class</th>\n",
       "      <th>6_class</th>\n",
       "      <th>7_class</th>\n",
       "      <th>8_class</th>\n",
       "      <th>9_class</th>\n",
       "      <th>...</th>\n",
       "      <th>92_class</th>\n",
       "      <th>93_class</th>\n",
       "      <th>94_class</th>\n",
       "      <th>95_class</th>\n",
       "      <th>96_class</th>\n",
       "      <th>97_class</th>\n",
       "      <th>98_class</th>\n",
       "      <th>99_class</th>\n",
       "      <th>max_value</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.971418</td>\n",
       "      <td>7.601085</td>\n",
       "      <td>6.253662</td>\n",
       "      <td>4.200216</td>\n",
       "      <td>11.877116</td>\n",
       "      <td>-2.441239</td>\n",
       "      <td>5.244116</td>\n",
       "      <td>-0.190037</td>\n",
       "      <td>3.397834</td>\n",
       "      <td>4.547288</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.296851</td>\n",
       "      <td>6.224649</td>\n",
       "      <td>3.114617</td>\n",
       "      <td>-0.308570</td>\n",
       "      <td>0.266424</td>\n",
       "      <td>0.494667</td>\n",
       "      <td>-5.222482</td>\n",
       "      <td>4.137463</td>\n",
       "      <td>14.639492</td>\n",
       "      <td>39_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.472752</td>\n",
       "      <td>5.776456</td>\n",
       "      <td>-2.737694</td>\n",
       "      <td>-1.720482</td>\n",
       "      <td>3.652450</td>\n",
       "      <td>0.276067</td>\n",
       "      <td>6.608448</td>\n",
       "      <td>0.864106</td>\n",
       "      <td>4.592688</td>\n",
       "      <td>8.591151</td>\n",
       "      <td>...</td>\n",
       "      <td>2.717580</td>\n",
       "      <td>6.264142</td>\n",
       "      <td>2.302805</td>\n",
       "      <td>2.655147</td>\n",
       "      <td>1.018003</td>\n",
       "      <td>-0.706559</td>\n",
       "      <td>2.704113</td>\n",
       "      <td>1.300578</td>\n",
       "      <td>20.066410</td>\n",
       "      <td>55_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.553531</td>\n",
       "      <td>6.922027</td>\n",
       "      <td>15.746008</td>\n",
       "      <td>5.205610</td>\n",
       "      <td>11.518566</td>\n",
       "      <td>-2.183513</td>\n",
       "      <td>0.309439</td>\n",
       "      <td>1.676199</td>\n",
       "      <td>8.241667</td>\n",
       "      <td>3.346487</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.564708</td>\n",
       "      <td>3.530740</td>\n",
       "      <td>1.694313</td>\n",
       "      <td>-5.831820</td>\n",
       "      <td>1.436764</td>\n",
       "      <td>-1.031949</td>\n",
       "      <td>-1.899167</td>\n",
       "      <td>3.243559</td>\n",
       "      <td>15.746008</td>\n",
       "      <td>2_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.805562</td>\n",
       "      <td>0.220727</td>\n",
       "      <td>0.537807</td>\n",
       "      <td>-0.447724</td>\n",
       "      <td>4.249485</td>\n",
       "      <td>1.422490</td>\n",
       "      <td>3.598542</td>\n",
       "      <td>4.122486</td>\n",
       "      <td>6.112689</td>\n",
       "      <td>3.032334</td>\n",
       "      <td>...</td>\n",
       "      <td>1.833291</td>\n",
       "      <td>1.130475</td>\n",
       "      <td>-0.753881</td>\n",
       "      <td>0.365807</td>\n",
       "      <td>-2.043800</td>\n",
       "      <td>1.860660</td>\n",
       "      <td>0.673981</td>\n",
       "      <td>3.255714</td>\n",
       "      <td>19.067812</td>\n",
       "      <td>48_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.025319</td>\n",
       "      <td>9.523963</td>\n",
       "      <td>3.477371</td>\n",
       "      <td>4.063594</td>\n",
       "      <td>4.372114</td>\n",
       "      <td>2.653487</td>\n",
       "      <td>5.072325</td>\n",
       "      <td>-1.769767</td>\n",
       "      <td>4.885382</td>\n",
       "      <td>8.662114</td>\n",
       "      <td>...</td>\n",
       "      <td>3.789347</td>\n",
       "      <td>6.610578</td>\n",
       "      <td>3.096414</td>\n",
       "      <td>0.834889</td>\n",
       "      <td>-1.003939</td>\n",
       "      <td>3.763621</td>\n",
       "      <td>12.943184</td>\n",
       "      <td>1.951309</td>\n",
       "      <td>12.943184</td>\n",
       "      <td>98_class</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0_class   1_class    2_class   3_class    4_class   5_class   6_class  \\\n",
       "0 -1.971418  7.601085   6.253662  4.200216  11.877116 -2.441239  5.244116   \n",
       "1  7.472752  5.776456  -2.737694 -1.720482   3.652450  0.276067  6.608448   \n",
       "2 -3.553531  6.922027  15.746008  5.205610  11.518566 -2.183513  0.309439   \n",
       "3 -1.805562  0.220727   0.537807 -0.447724   4.249485  1.422490  3.598542   \n",
       "4 -1.025319  9.523963   3.477371  4.063594   4.372114  2.653487  5.072325   \n",
       "\n",
       "    7_class   8_class   9_class  ...  92_class  93_class  94_class  95_class  \\\n",
       "0 -0.190037  3.397834  4.547288  ... -7.296851  6.224649  3.114617 -0.308570   \n",
       "1  0.864106  4.592688  8.591151  ...  2.717580  6.264142  2.302805  2.655147   \n",
       "2  1.676199  8.241667  3.346487  ... -4.564708  3.530740  1.694313 -5.831820   \n",
       "3  4.122486  6.112689  3.032334  ...  1.833291  1.130475 -0.753881  0.365807   \n",
       "4 -1.769767  4.885382  8.662114  ...  3.789347  6.610578  3.096414  0.834889   \n",
       "\n",
       "   96_class  97_class   98_class  99_class  max_value     class  \n",
       "0  0.266424  0.494667  -5.222482  4.137463  14.639492  39_class  \n",
       "1  1.018003 -0.706559   2.704113  1.300578  20.066410  55_class  \n",
       "2  1.436764 -1.031949  -1.899167  3.243559  15.746008   2_class  \n",
       "3 -2.043800  1.860660   0.673981  3.255714  19.067812  48_class  \n",
       "4 -1.003939  3.763621  12.943184  1.951309  12.943184  98_class  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b4_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29efdad8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T13:44:36.568149Z",
     "iopub.status.busy": "2022-06-13T13:44:36.567807Z",
     "iopub.status.idle": "2022-06-13T13:44:37.543695Z",
     "shell.execute_reply": "2022-06-13T13:44:37.542902Z"
    },
    "papermill": {
     "duration": 1.119451,
     "end_time": "2022-06-13T13:44:37.545676",
     "exception": false,
     "start_time": "2022-06-13T13:44:36.426225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for i in range(len(b5_result)):\n",
    "    max_val = max((b5_result['max_value'][i] * 0.3),(resnet_result['max_value'][i] * 1.7),(b4_result['max_value'][i] * 0.5))\n",
    "    \n",
    "    if max_val == (b5_result['max_value'][i] * 0.3):\n",
    "        result.append((max_val, b5_result['class'][i]))\n",
    "        \n",
    "    elif max_val == (resnet_result['max_value'][i] * 1.7):\n",
    "        result.append((max_val, resnet_result['class'][i]))\n",
    "        \n",
    "    else:\n",
    "        result.append((max_val, b4_result['class'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f373ed2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T13:44:37.839036Z",
     "iopub.status.busy": "2022-06-13T13:44:37.838688Z",
     "iopub.status.idle": "2022-06-13T13:44:37.844060Z",
     "shell.execute_reply": "2022-06-13T13:44:37.843351Z"
    },
    "papermill": {
     "duration": 0.151855,
     "end_time": "2022-06-13T13:44:37.845984",
     "exception": false,
     "start_time": "2022-06-13T13:44:37.694129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9.302363729476928, '86_class'),\n",
       " (33.467715644836424, '55_class'),\n",
       " (15.291365242004394, '2_class'),\n",
       " (23.156384658813476, '48_class'),\n",
       " (7.3044905185699465, '13_class')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b95e1947",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T13:44:38.131954Z",
     "iopub.status.busy": "2022-06-13T13:44:38.131352Z",
     "iopub.status.idle": "2022-06-13T13:44:38.220813Z",
     "shell.execute_reply": "2022-06-13T13:44:38.219707Z"
    },
    "papermill": {
     "duration": 0.236616,
     "end_time": "2022-06-13T13:44:38.225980",
     "exception": false,
     "start_time": "2022-06-13T13:44:37.989364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[86, 55, 2, 48, 13, 63, 79, 57, 43, 77, 50, 10, 0, 40, 10, 96, 32, 64, 73, 54, 74, 63, 2, 98, 42, 18, 2, 43, 38, 6, 67, 88, 90, 88, 83, 60, 10, 0, 12, 74, 2, 65, 32, 72, 83, 33, 55, 87, 14, 7, 55, 12, 23, 49, 72, 70, 68, 71, 4, 66, 82, 26, 20, 47, 80, 93, 70, 51, 94, 84, 29, 10, 28, 57, 25, 36, 24, 59, 43, 79, 4, 44, 77, 86, 26, 77, 73, 8, 68, 30, 52, 39, 77, 17, 0, 79, 81, 57, 0, 35, 9, 34, 19, 57, 54, 32, 72, 80, 42, 90, 75, 57, 82, 2, 58, 30, 2, 29, 28, 13, 98, 15, 23, 53, 18, 62, 22, 11, 88, 18, 40, 6, 85, 37, 79, 44, 42, 81, 45, 23, 67, 25, 29, 37, 98, 66, 44, 68, 3, 79, 24, 40, 3, 91, 12, 93, 43, 22, 50, 55, 79, 53, 27, 64, 59, 91, 80, 28, 74, 42, 3, 10, 40, 17, 30, 88, 72, 16, 75, 47, 57, 78, 89, 22, 57, 10, 68, 84, 37, 98, 2, 49, 70, 67, 23, 39, 71, 74, 91, 89, 60, 97, 97, 26, 0, 58, 37, 13, 21, 28, 33, 55, 15, 61, 24, 37, 56, 65, 53, 57, 47, 81, 6, 18, 28, 25, 47, 23, 36, 16, 46, 8, 79, 58, 11, 70, 85, 63, 46, 91, 6, 70, 80, 41, 9, 49, 64, 23, 0, 12, 37, 71, 11, 19, 18, 67, 20, 53, 7, 57, 20, 96, 59, 36, 82, 80, 20, 50, 86, 0, 62, 7, 37, 97, 29, 20, 14, 24, 63, 38, 37, 91, 97, 87, 27, 2, 68, 70, 23, 60, 46, 37, 15, 32, 61, 23, 33, 90, 88, 73, 42, 77, 12, 82, 27, 93, 96, 1, 64, 6, 58, 44, 22, 88, 59, 96, 73, 21, 52, 56, 2, 25, 39, 88, 80, 38, 35, 1, 15, 15, 36, 69, 77, 44, 65, 50, 63, 97, 87, 47, 66, 78, 80, 95, 86, 6, 47, 37, 75, 80, 3, 36, 86, 46, 67, 6, 30, 7, 49, 57, 22, 54, 60, 58, 68, 18, 77, 43, 28, 55, 37, 54, 40, 84, 62, 52, 10, 5, 83, 32, 14, 87, 91, 12, 45, 28, 98, 54, 2, 30, 0, 72, 76, 58, 68, 8, 56, 48, 28, 36, 39, 73, 13, 6, 81, 65, 63, 74, 3, 23, 68, 4, 32, 12, 45, 57, 15, 37, 70, 78, 67, 30, 57, 4, 80, 37, 51, 40, 45, 33, 25, 37, 77, 26, 44, 6, 92, 31, 12, 56, 27, 9, 34, 51, 9, 91, 47, 40, 14, 28, 80, 95, 39, 10, 52, 90, 78, 37, 44, 84, 67, 2, 10, 54, 24, 11, 67, 67, 37, 83, 43, 67, 55, 26, 91, 57, 36, 5, 4, 16, 58, 11, 87, 76, 7, 54, 52, 14, 66, 11, 55, 94, 97, 67, 20, 98, 13, 46, 11, 47, 20, 71, 2, 47, 10, 22, 30, 25, 8, 24, 15, 72, 10, 98, 4, 10, 40, 5, 30, 94, 24, 92, 93, 4, 19, 23, 49, 38, 45, 25, 2, 83, 52, 95, 58, 30, 64, 10, 56, 23, 35, 15, 23, 63, 3, 7, 11, 8, 54, 28, 2, 42, 57, 2, 57, 83, 61, 85, 8, 71, 87, 19, 16, 49, 36, 16, 94, 6, 22, 73, 45, 28, 87, 10, 42, 55, 26, 37, 63, 2, 36, 45, 15, 16, 26, 75, 99, 31, 37, 17, 63, 20, 57, 79, 44, 96, 77, 42, 16, 48, 82, 68, 70, 17, 62, 2, 78, 10, 27, 37, 71, 44, 52, 92, 12, 68, 20, 78, 75, 56, 10, 18, 10, 34, 36, 8, 10, 94, 1, 13, 37, 29, 35, 52, 24, 40, 86, 74, 72, 93, 49, 69, 85, 56, 2, 66, 70, 57, 46, 34, 27, 2, 97, 82, 88, 35, 78, 38, 7, 32, 61, 28, 95, 5, 73, 0, 41, 71, 65, 63, 58, 59, 27, 59, 50, 82, 23, 51, 24, 29, 5, 51, 80, 76, 77, 83, 85, 98, 44, 39, 37, 66, 51, 19, 45, 11, 69, 55, 10, 86, 55, 48, 2, 43, 68, 19, 74, 41, 16, 33, 33, 79, 99, 57, 79, 0, 65, 2, 97, 7, 20, 83, 48, 57, 77, 70, 40, 53, 43, 25, 2, 40, 18, 92, 3, 98, 55, 21, 7, 27, 95, 9, 8, 61, 82, 35, 7, 11, 44, 38, 59, 68, 65, 85, 80, 3, 30, 71, 58, 75, 12, 50, 51, 57, 85, 56, 74, 1, 32, 19, 20, 73, 99, 57, 75, 25, 91, 88, 60, 10, 57, 49, 94, 73, 21, 79, 43, 71, 34, 2, 39, 16, 10, 69, 28, 70, 42, 43, 49, 78, 57, 81, 7, 84, 86, 53, 61, 42, 21, 24, 20, 12, 91, 2, 30, 37, 10, 51, 30, 5, 39, 71, 25, 23, 54, 9, 99, 59, 38, 62, 1, 39, 20, 14, 19, 6, 3, 58, 79, 96, 61, 62, 78, 22, 81, 57, 12, 18, 82, 49, 1, 48, 33, 84, 60, 23, 26, 94, 87, 49, 2, 36, 57, 40, 38, 28, 83, 5, 7, 0, 59, 57, 62, 49, 33, 14, 67, 36, 66, 31, 94, 72, 74, 77, 30, 44, 88, 84, 1, 10, 9, 97, 9, 42, 23, 71, 59, 2, 10, 20, 57, 2, 34, 36, 77, 84, 79, 93, 16, 27, 29, 19, 97, 5, 54, 65, 22, 57, 57, 80, 38, 85, 2, 54, 65, 8, 78, 77, 26, 44, 19, 22, 54, 9, 73, 18, 39, 10, 71, 29, 8, 2, 37, 65, 54, 97, 51, 64, 43, 66, 32, 0, 31, 23, 93, 49, 21, 93, 3, 78, 75, 9, 86, 78, 83, 19, 32, 74, 55, 99, 24, 35, 58, 17, 63, 47, 61, 55, 24, 50, 35, 19, 58, 9, 25, 95, 66, 23, 2, 57, 2, 31, 89, 53, 44, 12, 98, 68, 39, 11, 9, 60, 94, 33, 30, 65, 35, 11, 14, 83, 55, 93, 77, 83, 90, 82, 59, 47, 49, 24, 51, 96, 77, 57, 80, 12, 2, 31, 26, 30, 9, 18, 41, 22, 25, 57, 23, 61, 24, 57, 46, 13, 67, 10, 93, 22, 57, 77, 98, 15, 99, 32, 17, 40, 31, 51, 86, 78, 3, 55, 48, 60, 39, 69, 40, 4, 98, 34, 2, 66, 11, 8, 62, 75, 72, 77, 37, 74, 26, 71, 95, 91, 40, 44, 58, 7, 57, 26, 6, 95, 48, 36, 7, 73, 95, 76, 68, 55, 24, 95, 32, 35, 97, 68, 59, 30, 1, 18, 35, 78, 21, 6, 38, 30, 47, 99, 48, 63, 83, 2, 49, 69, 86, 16, 6, 46, 28, 21, 22, 74, 59, 74, 15, 64, 1, 79, 44, 96, 44, 22, 57, 23, 89, 50, 2, 47, 80, 10, 58, 80, 18, 56, 37, 57, 2, 33, 36, 0, 89, 16, 4, 85, 53, 88, 1, 46, 67, 87, 2, 94, 36, 81, 57, 66, 85, 20, 97, 72, 69, 51, 19, 59, 54, 78, 80, 10, 3, 37, 66, 89, 53, 23, 70, 6, 33, 63, 51, 78, 47, 92, 8, 43, 9, 12, 94, 44, 92, 64, 62, 34, 62, 24, 46, 22, 9, 89, 11, 16, 72, 78, 18, 70, 92, 14, 67, 20, 39, 24, 35, 10, 50, 64, 65, 5, 60, 96, 13, 50, 66, 13, 36, 33, 39, 23, 62, 58, 56, 63, 9, 74, 63, 99, 49, 86, 88, 83, 75, 68, 33, 55, 97, 53, 65, 92, 40, 98, 67, 35, 40, 57, 44, 58, 61, 19, 18, 26, 74, 84, 74, 59, 15, 69, 73, 81, 99, 82, 23, 44, 13, 0, 85, 23, 21, 85, 30, 11, 87, 63, 98, 83, 57, 97, 38, 33, 5, 68, 66, 76, 18, 71, 12, 32, 9, 67, 80, 35, 11, 66, 16, 78, 54, 91, 59, 26, 29, 27, 15, 24, 31, 28, 29, 28, 54, 4, 37, 48, 7, 47, 45, 53, 50, 93, 50, 39, 81, 1, 59, 61, 62, 56, 38, 8, 99, 75, 62, 59, 65, 19, 11, 87, 70, 63, 22, 2, 52, 67, 88, 34, 63, 24, 40, 12, 7, 15, 26, 19, 49, 10, 96, 99, 39, 36, 97, 67, 51, 8, 44, 12, 32, 78, 1, 32, 56, 9, 3, 41, 79, 72, 61, 3, 97, 99, 18, 0, 68, 55, 98, 43, 35, 71, 45, 13, 93, 43, 56, 70, 72, 40, 49, 91, 59, 72, 3, 15, 57, 61, 10, 21, 13, 36, 84, 67, 86, 5, 14, 40, 36, 38, 48, 60, 85, 8, 44, 74, 14, 49, 63, 14, 85, 64, 41, 56, 9, 71, 18, 44, 21, 28, 48, 93, 5, 75, 58, 83, 5, 22, 62, 75, 1, 28, 45, 24, 44, 95, 2, 5, 10, 6, 25, 96, 97, 40, 48, 34, 42, 65, 63, 74, 95, 70, 74, 66, 33, 55, 84, 9, 51, 6, 31, 70, 59, 61, 50, 57, 5, 98, 86, 32, 63, 68, 54, 63, 79, 62, 29, 45, 9, 0, 96, 15, 95, 74, 72, 19, 60, 30, 25, 68, 22, 49, 86, 58, 32, 79, 49, 28, 88, 61, 35, 44, 28, 50, 24, 68, 65, 13, 14, 70, 77, 31, 30, 15, 84, 11, 5, 74, 2, 73, 40, 28, 10, 10, 64, 74, 36, 0, 69, 19, 17, 17, 28, 33, 60, 80, 39, 4, 58, 22, 75, 44, 99, 66, 86, 58, 70, 51, 46, 81, 57, 10, 49, 6, 23, 85, 65, 17, 99, 26, 23, 57, 63, 10, 87, 79, 4, 36, 65, 99, 37, 10, 27, 79, 10, 4, 57, 48, 17, 30, 89, 24, 58, 58, 96, 43, 66, 3, 47, 1, 28, 41, 99, 96, 19, 61, 28, 98, 40, 57, 63, 94, 29, 36, 8, 33, 18, 79, 58, 17, 98, 35, 41, 85, 69, 66, 73, 0, 56, 68, 77, 21, 14, 92, 6, 63, 37, 90, 32, 69, 22, 83, 44, 59, 50, 72, 86, 88, 24, 12, 7, 53, 41, 6, 79, 55, 44, 8, 73, 22, 48, 94, 86, 64, 2, 53, 47, 18, 61, 33, 66, 45, 85, 39, 2, 30, 14, 54, 32, 2, 27, 28, 75, 91, 23, 73, 89, 94, 48, 60, 45, 86, 7, 8, 52, 2, 69, 98, 3, 23, 74, 35, 18, 83, 97, 50, 34, 60, 66, 73, 94, 71, 1, 96, 24, 55, 75, 1, 11, 57, 54, 50, 80, 29, 71, 66, 47, 11, 10, 88, 3, 60, 3, 7, 33, 57, 29, 36, 68, 17, 87, 25, 30, 86, 47, 64, 11, 73, 79, 2, 9, 54, 5, 28, 20, 10, 80, 62, 24, 49, 27, 52, 20, 60, 91, 53, 59, 80, 97, 63, 1, 64, 1, 21, 81, 76, 36, 48, 43, 24, 45, 79, 86, 0, 92, 24, 25, 23, 38, 23, 56, 40, 62, 50, 1, 94, 65, 49, 2, 26, 56, 31, 64, 36, 15, 88, 70, 21, 24, 10, 2, 79, 37, 45, 64, 87, 24, 53, 30, 37, 28, 33, 56, 3, 62, 3, 85, 57, 23, 53, 0, 76, 37, 45, 19, 39, 92, 88, 47, 22, 84, 37, 8, 7, 51, 42, 24, 66, 82, 76, 37, 59, 98, 2, 84, 81, 97, 94, 9, 37, 6, 28, 7, 56, 37, 37, 12, 95, 60, 85, 66, 55, 24, 67, 85, 73, 46, 86, 72, 24, 32, 88, 88, 27, 48, 37, 78, 37, 71, 46, 2, 5, 20, 6, 63, 42, 75, 63, 78, 55, 52, 33, 92, 68, 2, 36, 48, 76, 44, 53, 63, 94, 19, 6, 38, 31, 23, 87, 74, 3, 24, 74, 20, 36, 93, 69, 17, 48, 52, 5, 55, 84, 1, 26, 66, 24, 56, 44, 30, 38, 98, 29, 15, 78, 82, 83, 45, 63, 36, 72, 28, 85, 28, 79, 38, 85, 19, 67, 23, 85, 75, 58, 99, 45, 90, 29, 5, 44, 22, 40, 64, 36, 17, 47, 10, 4, 41, 21, 83, 34, 57, 43, 43, 53, 25, 24, 52, 63, 37, 25, 66, 72, 81, 58, 26, 98, 46, 78, 75, 53, 83, 0, 0, 63, 45, 71, 4, 44, 68, 44, 29, 10, 48, 61, 81, 51, 2, 71, 15, 62, 16, 48, 66, 21, 10, 85, 39, 55, 39, 8, 4, 66, 74, 94, 35, 33, 57, 58, 37, 18, 35, 22, 84, 54, 38, 86, 73, 32, 43, 37, 99, 81, 53, 58, 35, 97, 69, 33, 66, 52, 75, 68, 44, 76, 13, 43, 11, 12, 85, 64, 41, 50, 56, 29, 21, 27, 80, 69, 70, 10, 29, 59, 10, 85, 24, 27, 50, 96, 90, 78, 57, 75, 45, 81, 29, 68, 68, 56, 55, 71, 68, 54, 35, 57, 21, 11, 69, 62, 67, 93, 17, 84, 9, 66, 6, 65, 37, 55, 36, 51, 24, 46, 76, 62, 37, 65, 35, 69, 14, 81, 91, 11, 77, 12, 4, 55, 29, 3, 17, 14, 0, 42, 63, 64, 55, 80, 54, 18, 57, 37, 79, 62, 68, 53, 11, 7, 97, 66, 16, 70, 12, 46, 17, 13, 30, 12, 81, 11, 54, 6, 97, 59, 72, 10, 20, 32, 3, 85, 59, 73, 65, 32, 49, 7, 1, 35, 53, 30, 74, 28, 8, 36, 46, 88, 52, 16, 14, 57, 33, 82, 58, 88, 25, 39, 86, 20, 70, 53, 22, 87, 90, 63, 95, 38, 31, 10, 47, 69, 45, 7, 33, 31, 56, 30, 10, 86, 72, 58, 7, 0, 18, 78, 70, 75, 55, 93, 12, 7, 58, 67, 27, 36, 97, 35, 29, 28, 73, 34, 77, 69, 67, 37, 74, 59, 93, 57, 85, 60, 57, 84, 48, 24, 43, 98, 23, 90, 21, 98, 7, 93, 45, 0, 97, 3, 93, 62, 37, 37, 40, 84, 57, 40, 10, 66, 84, 73, 26, 87, 51, 74, 33, 32, 21, 87, 79, 21, 43, 98, 75, 25, 51, 57, 35, 2, 39, 53, 75, 26, 48, 13, 7, 58, 38, 35, 5, 19, 81, 0, 46, 12, 82, 12, 10, 24, 12, 84, 4, 7, 42, 29, 9, 14, 40, 75, 5, 59, 87, 84, 50, 56, 67, 61, 15, 5, 48, 55, 90, 8, 10, 47, 72, 23, 23, 8, 17, 20, 48, 44, 34, 78, 58, 81, 86, 16, 24, 86, 57, 48, 36, 4, 7, 2, 3, 65, 2, 95, 38, 70, 47, 66, 25, 68, 44, 3, 4, 34, 47, 67, 36, 86, 4, 26, 45, 27, 43, 92, 10, 22, 94, 9, 57, 65, 81, 43, 67, 3, 23, 12, 94, 2, 59, 86, 40, 58, 78, 42, 69, 21, 9, 33, 81, 10, 98, 42, 8, 65, 35, 63, 2, 46, 56, 97, 60, 12, 26, 16, 0, 90, 28, 39, 50, 92, 11, 76, 85, 66, 16, 35, 57, 81, 77, 68, 99, 37, 77, 78, 40, 62, 3, 25, 94, 77, 24, 66, 52, 10, 60, 82, 24, 30, 58, 12, 34, 15, 33, 1, 39, 13, 57, 84, 80, 3, 68, 57, 72, 24, 42, 81, 48, 79, 58, 95, 24, 68, 8, 55, 53, 43, 26, 47, 58, 36, 10, 72, 1, 11, 74, 35, 91, 91, 37, 83, 95, 29, 32, 49, 29, 6, 37, 75, 72, 90, 66, 37, 76, 11, 95, 66, 62, 29, 43, 90, 70, 24, 83, 50, 91, 96, 83, 54, 65, 79, 12, 85, 40, 55, 97, 13, 15, 57, 75, 64, 47, 46, 0, 66, 30, 23, 45, 62, 29, 88, 36, 33, 22, 37, 44, 79, 72, 33, 66, 28, 35, 57, 34, 0, 67, 79, 58, 87, 9, 35, 62, 55, 71, 78, 18, 62, 60, 69, 43, 93, 18, 30, 11, 6, 7, 7, 60, 14, 35, 10, 37, 86, 98, 62, 97, 92, 83, 26, 9, 88, 39, 40, 65, 34, 90, 18, 55, 17, 3, 40, 75, 2, 23, 72, 71, 10, 53, 44, 23, 69, 70, 10, 22, 10, 61, 16, 38, 7, 1, 11, 55, 92, 59, 78, 63, 87, 78, 45, 86, 8, 3, 33, 43, 51, 17, 57, 31, 35, 50, 88, 14, 1, 79, 78, 70, 71, 40, 21, 81, 14, 11, 54, 60, 87, 6, 37, 24, 83, 31, 25, 6, 74, 42, 84, 87, 16, 58, 39, 33, 9, 76, 50, 57, 28, 96, 64, 38, 66, 98, 42, 14, 24, 34, 42, 37, 63, 40, 7, 2, 17, 39, 81, 44, 82, 44, 4, 1, 77, 27, 62, 57, 78, 24, 31, 22, 2, 71, 20, 33, 50, 53, 67, 12, 59, 38, 77, 45, 55, 46, 36, 62, 30, 78, 9, 55, 77, 72, 59, 2, 8, 73, 62, 59, 66, 33, 39, 10, 34, 46, 10, 93, 12, 37, 1, 91, 55, 69, 13, 68, 6, 3, 12, 15, 9, 56, 72, 83, 20, 38, 36, 79, 80, 81, 10, 29, 98, 48, 37, 36, 81, 52, 30, 65, 72, 86, 32, 98, 59, 48, 55, 69, 55, 13, 69, 60, 81, 2, 50, 72, 51, 84, 33, 54, 54, 0, 13, 82, 92, 2, 96, 13, 90, 3, 16, 84, 22, 2, 80, 32, 70, 2, 68, 24, 78, 77, 7, 53, 89, 37, 63, 21, 75, 62, 37, 5, 73, 19, 42, 80, 81, 79, 63, 10, 77, 18, 82, 78, 83, 59, 84, 66, 79, 24, 8, 1, 96, 84, 9, 85, 34, 54, 0, 1, 10, 38, 57, 79, 0, 56, 37, 20, 70, 75, 24, 7, 74, 0, 61, 58, 60, 68, 42, 85, 32, 29, 10, 80, 81, 61, 65, 23, 4, 51, 6, 18, 29, 9, 87, 44, 34, 35, 67, 77, 26, 43, 9, 50, 54, 46, 28, 24, 22, 38, 13, 36, 31, 93, 28, 25, 23, 94, 24, 82, 42, 26, 29, 64, 39, 72, 38, 86, 30, 16, 2, 24, 2, 93, 52, 59, 75, 10, 40, 51, 51, 62, 25, 22, 25, 73, 72, 34, 37, 20, 43, 4, 11, 86, 80, 23, 14, 83, 28, 10, 23, 32, 21, 88, 78, 79, 48, 69, 64, 85, 23, 85, 84, 92, 70, 58, 66, 72, 56, 69, 10, 10, 16, 12, 82, 24, 87, 71, 37, 67, 19, 6, 22, 93, 0, 73, 2, 44, 11, 64, 63, 80, 97, 99, 54, 19, 84, 51, 2, 78, 66, 35, 24, 33, 29, 10, 59, 48, 99, 28, 73, 80, 30, 50, 36, 14, 67, 78, 66, 47, 53, 55, 8, 57, 57, 57, 65, 66, 97, 45, 39, 88, 31, 84, 48, 58, 21, 92, 5, 56, 3, 47, 8, 48, 32, 76, 39, 71, 98, 82, 32, 31, 88, 19, 69, 59, 48, 2, 10, 65, 22, 26, 49, 74, 78, 47, 59, 21, 31, 51, 91, 54, 98, 95, 17, 62, 12, 88, 73, 75, 32, 55, 58, 87, 35, 24, 38, 27, 24, 10, 8, 23, 39, 9, 71, 29, 36, 60, 41, 69, 85, 92, 1, 38, 7, 59, 24, 89, 52, 33, 2, 43, 49, 2, 58, 33, 80, 10, 50, 96, 2, 10, 72, 1, 73, 60, 10, 5, 42, 77, 88, 37, 63, 62, 17, 31, 19, 57, 32, 83, 11, 14, 80, 2, 53, 84, 45, 26, 85, 83, 39, 19, 68, 87, 77, 57, 7, 60, 28, 8, 51, 18, 36, 49, 71, 20, 83, 51, 33, 56, 6, 54, 64, 40, 32, 31, 37, 15, 91, 81, 37, 74, 23, 5, 54, 87, 46, 88, 7, 87, 23, 69, 58, 88, 85, 29, 36, 71, 30, 62, 24, 55, 5, 16, 11, 63, 5, 90, 61, 31, 1, 78, 39, 32, 82, 45, 39, 88, 10, 93, 84, 8, 72, 55, 35, 88, 13, 90, 13, 79, 73, 16, 25, 44, 24, 84, 96, 97, 23, 28, 61, 31, 37, 45, 92, 61, 56, 68, 97, 52, 89, 68, 24, 95, 86, 80, 97, 60, 64, 12, 64, 5, 53, 82, 44, 40, 84, 22, 11, 88, 11, 16, 11, 77, 13, 59, 92, 55, 36, 49, 47, 87, 60, 22, 48, 2, 63, 97, 73, 25, 24, 72, 80, 61, 6, 69, 2, 12, 60, 53, 4, 42, 63, 33, 77, 54, 21, 86, 35, 42, 70, 73, 56, 82, 85, 76, 11, 16, 50, 84, 0, 2, 84, 69, 16, 14, 88, 78, 66, 53, 52, 36, 49, 7, 28, 66, 80, 5, 62, 1, 83, 8, 98, 17, 75, 57, 67, 39, 85, 95, 13, 62, 83, 79, 9, 36, 95, 24, 70, 4, 55, 66, 27, 59, 48, 86, 19, 23, 30, 5, 38, 48, 86, 4, 1, 26, 2, 35, 22, 81, 37, 21, 37, 33, 30, 19, 75, 49, 11, 18, 3, 5, 48, 77, 8, 79, 49, 30, 20, 24, 67, 3, 33, 37, 74, 76, 47, 8, 58, 35, 11, 71, 15, 91, 16, 88, 17, 11, 60, 24, 28, 30, 58, 0, 62, 18, 77, 15, 74, 15, 49, 54, 10, 85, 39, 16, 11, 34, 38, 36, 57, 62, 10, 92, 35, 99, 38, 28, 80, 27, 53, 43, 10, 12, 52, 57, 82, 6, 86, 65, 9, 41, 38, 67, 10, 75, 15, 5, 57, 53, 15, 21, 29, 32, 52, 20, 18, 45, 22, 9, 73, 78, 31, 24, 2, 20, 65, 5, 59, 81, 14, 98, 51, 25, 49, 5, 2, 90, 12, 3, 69, 48, 54, 76, 42, 3, 84, 2, 64, 53, 23, 56, 33, 53, 22, 58, 63, 11, 2, 70, 70, 24, 4, 36, 14, 93, 22, 20, 64, 38, 76, 64, 54, 24, 16, 86, 98, 0, 95, 24, 67, 36, 69, 56, 14, 66, 13, 81, 11, 75, 86, 57, 29, 60, 19, 39, 5, 14, 39, 2, 27, 85, 47, 36, 67, 36, 14, 79, 71, 50, 31, 60, 59, 87, 41, 15, 26, 63, 29, 10, 62, 17, 66, 14, 56, 51, 29, 75, 27, 23, 36, 0, 18, 7, 60, 37, 24, 51, 51, 38, 42, 20, 76, 50, 93, 15, 46, 27, 38, 76, 59, 16, 59, 57, 34, 38, 89, 30, 74, 66, 96, 59, 87, 78, 9, 43, 62, 2, 69, 18, 43, 52, 49, 73, 77, 75, 26, 30, 77, 63, 62, 81, 33, 91, 27, 27, 24, 28, 49, 78, 73, 51, 10, 29, 54, 35, 57, 57, 80, 41, 21, 29, 96, 47, 4, 27, 55, 9, 37, 49, 4, 9, 99, 26, 69, 49, 85, 20, 5, 93, 64, 8, 69, 86, 2, 0, 80, 66, 47, 88, 49, 68, 7, 93, 57, 63, 92, 94, 98, 44, 57, 83, 14, 76, 37, 18, 57, 24, 69, 19, 19, 52, 58, 94, 55, 12, 58, 43, 49, 34, 77, 81, 18, 0, 81, 80, 79, 10, 87, 17, 67, 12, 86, 98, 66, 27, 21, 67, 3, 85, 7, 86, 11, 34, 30, 43, 14, 33, 12, 46, 93, 23, 4, 45, 57, 26, 12, 75, 97, 19, 37, 17, 72, 9, 73, 18, 7, 30, 12, 7, 50, 10, 40, 80, 3, 59, 42, 45, 28, 6, 87, 95, 62, 71, 38, 13, 94, 88, 10, 63, 27, 73, 53, 28, 57, 69, 38, 89, 81, 92, 5, 57, 48, 46, 56, 22, 11, 18, 92, 24, 72, 25, 64, 12, 66, 38, 86, 23, 21, 86, 77, 97, 45, 71, 29, 57, 2, 10, 60, 94, 21, 7, 95, 26, 93, 20, 62, 29, 16, 10, 85, 80, 10, 81, 58, 29, 46, 79, 87, 8, 71, 73, 17, 23, 10, 57, 91, 45, 93, 76, 65, 30, 88, 98, 52, 44, 4, 29, 67, 45, 10, 36, 75, 15, 74, 62, 93, 57, 76, 4, 30, 79, 3, 35, 29, 69, 38, 68, 14, 74, 87, 79, 50, 46, 97, 71, 57, 54, 35, 64, 66, 45, 20, 54, 17, 23, 51, 1, 10, 12, 9, 39, 39, 99, 86, 86, 24, 81, 98, 31, 62, 48, 30, 51, 57, 26, 67, 10, 8, 71, 78, 92, 67, 2, 14, 52, 60, 80, 9, 24, 85, 77, 54, 2, 11, 42, 37, 51, 52, 70, 81, 65, 24, 33, 74, 36, 57, 85, 44, 37, 43, 42, 63, 3, 14, 10, 30, 29, 93, 90, 29, 7, 10, 76, 33, 32, 20, 38, 57, 9, 5, 11, 82, 39, 61, 59, 72, 58, 54, 37, 67, 87, 18, 51, 77, 43, 43, 92, 71, 4, 80, 10, 3, 22, 36, 76, 9, 28, 33, 77, 22, 24, 32, 61, 61, 50, 39, 41, 2, 33, 62, 7, 43, 47, 69, 71, 67, 9, 85, 87, 6, 47, 40, 44, 28, 83, 96, 93, 42, 86, 76, 50, 21, 89, 38, 7, 36, 16, 4, 53, 36, 81, 97, 3, 6, 34, 20, 52, 83, 11, 80, 84, 70, 33, 11, 45, 86, 71, 38, 58, 99, 57, 23, 46, 55, 55, 16, 63, 90, 43, 37, 64, 6, 37, 11, 12, 74, 55, 2, 43, 0, 9, 40, 24, 60, 47, 76, 14, 97, 30, 0, 71, 58, 95, 97, 6, 28, 66, 74, 2, 27, 29, 93, 99, 30, 46, 88, 84, 30, 50, 27, 39, 51, 37, 72, 70, 14, 5, 45, 97, 19, 63, 48, 85, 84, 61, 9, 11, 54, 83, 12, 47, 42, 68, 64, 46, 25, 1, 88, 51, 66, 56, 42, 39, 78, 80, 59, 13, 77, 90, 82, 72, 81, 22, 8, 91, 52, 53, 18, 29, 92, 60, 57, 22, 32, 35, 5, 58, 27, 93, 81, 59, 85, 20, 24, 37, 94, 84, 92, 72, 66, 96, 58, 57, 7, 97, 48, 1, 62, 73, 76, 81, 39, 62, 76, 90, 85, 51, 79, 11, 51, 78, 43, 21, 26, 48, 48, 74, 70, 24, 71, 82, 4, 43, 47, 45, 39, 79, 79, 51, 79, 31, 20, 47, 24, 68, 78, 37, 54, 43, 87, 75, 33, 24, 50, 81, 30, 55, 32, 3, 7, 83, 54, 51, 37, 57, 40, 40, 69, 8, 19, 98, 84, 5, 88, 37, 57, 11, 75, 24, 52, 55, 22, 21, 6, 32, 43, 48, 26, 15, 68, 8, 84, 58, 35, 87, 2, 2, 10, 73, 1, 94, 96, 26, 84, 41, 70, 37, 53, 79, 89, 5, 67, 62, 82, 2, 24, 25, 22, 69, 11, 83, 32, 95, 78, 12, 62, 63, 23, 4, 10, 93, 57, 38, 66, 73, 77, 66, 19, 12, 49, 98, 52, 54, 94, 65, 63, 59, 30, 36, 87, 73, 49, 13, 69, 91, 93, 23, 22, 58, 66, 54, 89, 86, 28, 37, 69, 57, 78, 76, 88, 10, 93, 93, 1, 83, 67, 46, 30, 24, 37, 93, 21, 70, 97, 31, 61, 75, 44, 9, 49, 37, 14, 75, 2, 85, 55, 65, 66, 59, 44, 98, 13, 62, 38, 2, 65, 11, 36, 46, 83, 10, 60, 90, 85, 27, 96, 4, 34, 6, 43, 34, 96, 74, 84, 36, 65, 50, 3, 7, 81, 15, 11, 98, 66, 14, 64, 32, 78, 44, 31, 10, 54, 40, 74, 32, 81, 16, 38, 59, 69, 66, 45, 21, 97, 22, 51, 75, 71, 35, 76, 31, 65, 98, 56, 13, 11, 2, 28, 24, 57, 4, 31, 80, 0, 78, 2, 44, 32, 92, 86, 95, 93, 48, 74, 16, 61, 66, 85, 29, 84, 7, 76, 80, 62, 24, 96, 85, 72, 86, 38, 79, 2, 80, 33, 0, 30, 44, 6, 65, 81, 87, 36, 83, 9, 95, 47, 47, 37, 49, 4, 72, 23, 42, 59, 24, 76, 58, 53, 22, 24, 98, 5, 46, 37, 44, 58, 65, 33, 78, 39, 28, 38, 72, 36, 17, 24, 64, 82, 55, 94, 1, 17, 23, 63, 41, 22, 60, 9, 47, 3, 98, 50, 32, 10, 55, 15, 8, 13, 61, 2, 4, 10, 79, 86, 74, 76, 20, 5, 61, 10, 12, 79, 67, 65, 70, 20, 35, 82, 5, 10, 73, 27, 59, 57, 39, 47, 5, 84, 79, 85, 21, 77, 23, 0, 35, 97, 86, 30, 35, 9, 59, 39, 10, 18, 3, 41, 78, 17, 36, 54, 53, 41, 11, 75, 88, 39, 19, 70, 48, 16, 95, 14, 76, 62, 30, 8, 14, 10, 20, 12, 87, 49, 57, 66, 31, 89, 77, 68, 20, 5, 8, 58, 43, 84, 36, 81, 52, 88, 66, 73, 77, 37, 36, 19, 48, 18, 81, 8, 60, 94, 47, 44, 29, 37, 39, 17, 59, 90, 22, 24, 87, 86, 36, 16, 37, 26, 6, 32, 74, 33, 24, 33, 3, 37, 94, 46, 93, 92, 61, 16, 77, 10, 93, 92, 90, 87, 88, 24, 60, 5, 63, 1, 66, 36, 85, 9, 49, 72, 22, 68, 55, 11, 93, 14, 50, 22, 81, 5, 86, 69, 63, 21, 29, 7, 30, 20, 21, 88, 14, 49, 67, 11, 72, 88, 78, 10, 1, 16, 38, 44, 29, 74, 4, 88, 84, 29, 48, 68, 4, 93, 64, 9, 30, 10, 59, 37, 81, 73, 5, 44, 55, 2, 32, 17, 31, 75, 91, 8, 20, 77, 6, 13, 14, 66, 89, 57, 73, 57, 26, 85, 2, 60, 81, 17, 7, 68, 13, 24, 23, 10, 63, 48, 38, 54, 37, 30, 11, 9, 10, 67, 55, 17, 5, 93, 7, 47, 78, 79, 44, 60, 36, 47, 61, 15, 37, 3, 79, 93, 65, 23, 12, 24, 37, 88, 75, 92, 81, 76, 69, 57, 35, 64, 2, 40, 2, 64, 43, 59, 99, 28, 78, 69, 71, 4, 60, 92, 66, 0, 29, 37, 78, 81, 89, 80, 2, 47, 93, 23, 24, 6, 86, 32, 92, 50, 32, 67, 10, 39, 8, 53, 76, 67, 15, 53, 16, 70, 29, 97, 78, 50, 18, 11, 37, 63, 40, 81, 85, 33, 17, 60, 29, 2, 46, 84, 99, 45, 38, 56, 75, 72, 38, 70, 35, 19, 23, 63, 99, 66, 10, 3, 99, 50, 81, 67, 29, 46, 93, 49, 38, 5, 8, 19, 48, 95, 28, 71, 60, 96, 5, 1, 57, 22, 54, 80, 8, 88, 66, 9, 37, 20, 77, 9, 77, 54, 85, 12, 73, 53, 12, 2, 25, 88, 32, 48, 29, 57, 49, 61, 40, 75, 96, 57, 27, 10, 34, 4, 20, 51, 50, 39, 47, 8, 5, 34, 21, 20, 57, 78, 44, 67, 19, 33, 55, 14, 70, 76, 78, 68, 97, 75, 36, 64, 86, 29, 87, 26, 0, 7, 43, 34, 22, 73, 5, 42, 52, 6, 0, 86, 42, 10, 79, 4, 35, 83, 58, 78, 18, 19, 64, 24, 10, 29, 2, 7, 10, 13, 68, 10, 71, 54, 76, 92, 1, 39, 86, 33, 59, 30, 11, 89, 64, 35, 26, 0, 87, 37, 23, 23, 20, 37, 9, 48, 57, 88, 39, 24, 30, 41, 98, 15, 92, 2, 57, 5, 11, 28, 80, 0, 8, 81, 54, 98, 63, 23, 36, 11, 55, 37, 94, 31, 96, 78, 68, 30, 70, 84, 0, 46, 10, 14, 96, 58, 78, 24, 47, 76, 24, 69, 45, 84, 1, 9, 29, 39, 48, 30, 93, 53, 32, 78, 39, 23, 59, 29, 71, 42, 96, 74, 84, 57, 88, 11, 36, 59, 57, 55, 57, 92, 2, 22, 18, 57, 79, 86, 28, 17, 62, 59, 76, 98, 25, 48, 80, 21, 2, 73, 57, 71, 54, 37, 42, 52, 27, 98, 97, 50, 42, 28, 72, 5, 57, 24, 70, 88, 71, 48, 70, 14, 32, 10, 60, 39, 6, 35, 26, 27, 58, 2, 79, 37, 65, 76, 40, 12, 4, 62, 33, 65, 88, 49, 53, 0, 49, 30, 45, 64, 76, 8, 49, 68, 52, 23, 14, 29, 24, 19, 38, 62, 37, 46, 56, 28, 46, 53, 39, 60, 26, 79, 10, 4, 66, 36, 99, 83, 37, 58, 2, 25, 27, 12, 28, 32, 89, 12, 52, 93, 63, 57, 61, 53, 24, 64, 42, 65, 34, 98, 30, 49, 73, 97, 63, 31, 22, 37, 16, 38, 32, 95, 49, 55, 77, 35, 2, 70, 57, 59, 83, 75, 24, 59, 46, 76, 53, 12, 95, 68, 92, 50, 88, 31, 53, 72, 97, 62, 40, 28, 92, 39, 9, 20, 4, 59, 88, 76, 85, 27, 69, 24, 91, 33, 59, 43, 28, 31, 70, 30, 59, 92, 42, 30, 90, 40, 9, 36, 40, 48, 26, 11, 72, 14, 23, 10, 21, 47, 69, 46, 99, 90, 84, 16, 5, 81, 93, 19, 83, 55, 12, 76, 45, 40, 2, 98, 10, 24, 57, 37, 53, 84, 80, 18, 97, 70, 3, 39, 45, 64, 35, 13, 29, 68, 28, 96, 83, 58, 75, 48, 96, 98, 87, 94, 2, 33, 76, 47, 75, 81, 67, 76, 93, 57, 97, 41, 0, 82, 8, 33, 20, 55, 14, 30, 95, 11, 12, 76, 65, 6, 7, 18, 58, 73, 43, 25, 49, 26, 55, 0, 33, 96, 74, 61, 66, 24, 90, 32, 31, 39, 46, 39, 3, 93, 87, 82, 4, 32, 60, 34, 58, 59, 60, 24, 94, 13, 1, 9, 35, 49, 31, 83, 78, 98, 43, 50, 55, 52, 76, 24, 77, 86, 45, 50, 86, 17, 4, 45, 34, 69, 10, 19, 50, 19, 4, 54, 57, 57, 16, 43, 62, 2, 14, 66, 9, 38, 88, 26, 57, 35, 40, 80, 41, 29, 2, 26, 37, 18, 28, 45, 2, 80, 49, 18, 74, 20, 5, 42, 39, 28, 21, 57, 10, 49, 71, 35, 57, 6, 23, 79, 53, 0, 57, 70, 78, 70, 99, 15, 39, 8, 51, 53, 4, 52, 75, 54, 58, 72, 38, 46, 66, 22, 39, 84, 11, 98, 97, 84, 37, 2, 36, 76, 84, 92, 77, 81, 42, 53, 40, 40, 68, 72, 51, 18, 10, 9, 37, 59, 17, 39, 37, 62, 17, 23, 46, 58, 57, 12, 69, 0, 20, 21, 19, 37, 44, 54, 57, 78, 92, 24, 51, 60, 3, 24, 70, 18, 39, 97, 10, 54, 54, 46, 8, 4, 28, 74, 28, 62, 77, 65, 32, 61, 78, 83, 54, 12, 57, 22, 2, 79, 34, 73, 82, 48, 33, 92, 19, 29, 63, 62, 87, 10, 78, 47, 91, 2, 79, 80, 4, 81, 29, 24, 11, 96, 75, 29, 93, 37, 45, 30, 52, 6, 85, 90, 7, 57, 97, 8, 97, 16, 32, 52, 97, 1, 44, 23, 96, 42, 73, 37, 87, 79, 57, 58, 0, 21, 59, 18, 8, 67, 45, 19, 77, 48, 24, 62, 30, 37, 35, 64, 22, 11, 21, 93, 57, 24, 94, 80, 16, 98, 38, 84, 72, 32, 37, 44, 35, 7, 23, 17, 50, 53, 2, 92, 26, 61, 10, 49, 54, 11, 6, 12, 11, 99, 37, 50, 72, 3, 74, 24, 32, 58, 92, 98, 66, 37, 1, 7, 43, 70, 55, 62, 81, 19, 40, 16, 24, 69, 23, 79, 24, 9, 25, 71, 35, 39, 39, 62, 14, 57, 9, 85, 32, 50, 45, 21, 37, 45, 78, 80, 42, 72, 94, 32, 95, 43, 64, 45, 57, 5, 88, 69, 46, 66, 9, 26, 27, 8, 34, 88, 11, 97, 48, 72, 40, 58, 63, 10, 27, 74, 65, 89, 44, 57, 5, 81, 57, 37, 71, 58, 28, 14, 15, 51, 92, 67, 66, 83, 24, 34, 28, 89, 35, 84, 4, 7, 76, 18, 62, 64, 81, 62, 21, 86, 10, 90, 57, 16, 77, 76, 70, 13, 57, 76, 62, 21, 0, 35, 20, 2, 33, 8, 30, 24, 83, 64, 21, 14, 92, 1, 24, 30, 48, 96, 88, 39, 13, 30, 41, 44, 84, 14, 5, 73, 42, 75, 72, 64, 41, 41, 22, 77, 44, 21, 90, 74, 38, 5, 19, 57, 3, 66, 1, 5, 7, 63, 24, 10, 58, 10, 57, 38, 97, 81, 92, 8, 84, 63, 21, 0, 82, 86, 4, 20, 37, 11, 75, 10, 49, 47, 2, 61, 68, 29, 2, 83, 77, 61, 4, 67, 37, 87, 70, 7, 57, 73, 62, 97, 66, 4, 36, 65, 8, 30, 9, 7, 75, 91, 99, 33, 49, 47, 26, 43, 98, 73, 45, 88, 62, 8, 76, 78, 88, 23, 85, 65, 28, 89, 77, 35, 63, 94, 17, 83, 36, 23, 20, 15, 33, 67, 52, 2, 40, 38, 28, 71, 5, 36, 80, 23, 88, 10, 31, 23, 50, 78, 74, 0, 8, 86, 90, 24, 39, 56, 75, 17, 70, 71, 89, 10, 44, 73, 13, 46, 59, 98, 81, 85, 8, 35, 37, 58, 90, 80, 19, 95, 38, 91, 41, 52, 64, 15, 57, 85, 44, 0, 99, 48, 6, 23, 39, 4, 29, 33, 15, 3, 86, 54, 36, 75, 10, 96, 0, 18, 87, 48, 32, 1, 1, 53, 42, 91, 72, 2, 80, 99, 8, 16, 45, 7, 44, 21, 33, 23, 44, 85, 24, 32, 8, 10, 73, 23, 24, 57, 55, 42, 73, 55, 49, 64, 33, 97, 39, 40, 81, 69, 48, 42, 58, 72, 89, 98, 61, 68, 6, 53, 74, 80, 78, 86, 69, 8, 41, 78, 28, 67, 13, 10, 42, 39, 39, 84, 54, 27, 44, 48, 28, 55, 16, 54, 38, 30, 24, 5, 24, 68, 83, 22, 46, 13, 92, 2, 42, 68, 57, 12, 37, 51, 26, 21, 45, 10, 8, 79, 68, 71, 51, 14, 23, 70, 54, 59, 19, 89, 23, 56, 60, 51, 82, 44, 28, 59, 27, 54, 63, 55, 17, 40, 93, 14, 95, 9, 29, 32, 24, 73, 73, 37, 43, 97, 81, 7, 53, 55, 85, 24, 99, 93, 87, 2, 55, 30, 68, 32, 21, 7, 64, 82, 50, 33, 36, 93, 16, 10, 75, 26, 2, 81, 66, 37, 46, 87, 44, 37, 53, 60, 1, 56, 77, 59, 28, 36, 28, 30, 69, 71, 58, 39, 33, 59, 34, 11, 15, 7, 37, 38, 69, 78, 56, 15, 35, 55, 36, 73, 68, 53, 89, 18, 31, 14, 25, 98, 52, 34, 33, 78, 44, 80, 16, 13, 63, 94, 28, 62, 87, 75, 57, 19, 31, 34, 84, 81, 44, 69, 52, 73, 47, 99, 54, 88, 87, 4, 19, 60, 57, 77, 57, 70, 49, 69, 46, 63, 65, 73, 55, 94, 96, 25, 0, 37, 53, 57, 60, 59, 58, 6, 85, 10, 5, 26, 45, 34, 66, 30, 33, 98, 74, 74, 97, 72, 42, 2, 7, 10, 85, 35, 94, 18, 98, 34, 95, 88, 23, 7, 82, 17, 77, 18, 54, 31, 85, 10, 63, 21, 27, 92, 53, 87, 30, 33, 74, 81, 69, 10, 56, 53, 34, 4, 20, 16, 48, 26, 40, 24, 92, 20, 24, 62, 49, 2, 2, 67, 26, 70, 59, 98, 58, 2, 63, 87, 51, 3, 35, 7, 71, 59, 47, 51, 68, 22, 45, 76, 36, 76, 23, 66, 53, 93, 58, 32, 10, 42, 92, 89, 84, 21, 22, 1, 1, 8, 90, 94, 45, 54, 36, 75, 53, 66, 87, 14, 57, 12, 41, 74, 95, 46, 45, 45, 39, 69, 59, 10, 62, 40, 60, 37, 37, 6, 15, 72, 49, 12, 74, 70, 97, 85, 67, 69, 86, 12, 94, 78, 44, 57, 27, 99, 15, 88, 28, 51, 41, 82, 84, 16, 76, 90, 18, 87, 77, 32, 48, 37, 73, 43, 68, 33, 52, 64, 80, 42, 63, 79, 62, 80, 37, 4, 71, 87, 45, 79, 52, 43, 45, 57, 70, 45, 89, 90, 24, 10, 71, 85, 51, 54, 69, 21, 58, 31, 50, 89, 53, 2, 63, 79, 36, 42, 4, 13, 21, 10, 14, 46, 57, 36, 8, 17, 27, 24, 37, 57, 71, 9, 68, 46, 19, 40, 67, 39, 48, 4, 23, 6, 45, 50, 10, 21, 37, 45, 59, 10, 28, 49, 17, 47, 66, 6, 86, 72, 45, 2, 49, 23, 99, 9, 5, 15, 71, 57, 78, 85, 24, 28, 30, 16, 4, 95, 30, 51, 24, 2, 75, 83, 75, 77, 76, 24, 77, 98, 21, 5, 77, 84, 37, 39, 78, 58, 86, 23, 11, 67, 49, 9, 37, 97, 9, 38, 24, 10, 86, 72, 84, 60, 6, 64, 56, 97, 37, 90, 10, 60, 89, 85, 83, 15, 45, 35, 78, 10, 68, 53, 18, 18, 3, 3, 24, 7, 70, 58, 2, 4, 29, 92, 4, 75, 64, 58, 46, 19, 10, 36, 13, 22, 45, 63, 98, 26, 81, 62, 74, 65, 23, 11, 23, 24, 40, 53, 60, 72, 51, 0, 16, 37, 36, 1, 88, 71, 2, 0, 69, 11, 53, 62, 60, 84, 83, 57, 64, 85, 13, 91, 24, 10, 10, 37, 11, 28, 70, 50, 35, 86, 36, 97, 11, 29, 48, 25, 40, 47, 36, 76, 33, 45, 38, 65, 91, 17, 35, 92, 66, 64, 98, 10, 10, 16, 32, 38, 11, 5, 45, 98, 65, 22, 81, 74, 57, 69, 3, 57, 85, 92, 0, 28, 39, 77, 55, 18, 97, 37, 83, 41, 90, 23, 81, 64, 6, 49, 44, 28, 2, 42, 87, 73, 28, 78, 10, 65, 47, 49, 21, 24, 54, 11, 65, 74, 93, 9, 10, 96, 88, 3, 23, 7, 57, 24, 0, 2, 98, 33, 22, 17, 44, 84, 71, 57, 84, 24, 60, 75, 42, 64, 67, 88, 37, 38, 81, 24, 70, 20, 16, 31, 10, 12, 44, 38, 71, 20, 77, 18, 83, 82, 5, 77, 23, 70, 33, 62, 87, 47, 33, 18, 61, 57, 22, 84, 93, 38, 42, 93, 78, 69, 39, 81, 81, 60, 82, 84, 73, 74, 57, 81, 27, 44, 24, 8, 25, 4, 5, 39, 14, 12, 10, 37, 7, 57, 90, 84, 50, 19, 15, 31, 44, 52, 54, 86, 80, 12, 37, 26, 9, 57, 6, 66, 97, 10, 16, 33, 48, 40, 22, 75, 29, 48, 39, 69, 30, 18, 29, 74, 75, 22, 85, 40, 4, 0, 61, 91, 67, 7, 32, 28, 48, 2, 55, 90, 71, 14, 23, 18, 50, 9, 86, 58, 19, 84, 88, 17, 23, 26, 91, 18, 6, 45, 35, 2, 0, 65, 37, 74, 53, 64, 52, 39, 8, 44, 29, 44, 77, 2, 99, 83, 53, 72, 69, 76, 91, 88, 60, 39, 55, 24, 10, 20, 49, 8, 67, 72, 15, 10, 32, 23, 24, 69, 48, 43, 37, 48, 2, 30, 70, 99, 6, 85, 89, 25, 37, 70, 35, 38, 2, 53, 70, 60, 34, 10, 69, 92, 56, 92, 63, 99, 33, 71, 60, 67, 85, 30, 81, 56, 16, 86, 7, 78, 13, 99, 72, 39, 73, 49, 14, 40, 35, 37, 56, 72, 47, 76, 93, 10, 45, 39, 7, 15, 97, 62, 57, 90, 28, 98, 33, 23, 94, 56, 36, 87, 64, 23, 46, 63, 65, 44, 59, 90, 81, 1, 79, 8, 7, 84, 67, 92, 66, 35, 32, 92, 49, 10, 57, 92, 69, 93, 86, 57, 43, 84, 97, 60, 10, 48, 36, 17, 73, 59, 44, 76, 97, 61, 37, 48, 34, 85, 73, 57, 27, 90, 97, 13, 26, 69, 10, 63, 4, 27, 40, 68, 8, 73, 42, 20, 11, 57, 16, 73, 72, 7, 3, 94, 37, 1, 7, 50, 57, 48, 27, 85, 86, 67, 5, 0, 19, 47, 21, 70, 16, 50, 45, 13, 92, 59, 72, 43, 8, 55, 2, 43, 76, 68, 2, 63, 68, 38, 1, 42, 5, 38, 86, 42, 39, 23, 84, 35, 87, 80, 83, 10, 38, 64, 0, 85, 82, 30, 88, 12, 63, 72, 36, 5, 86, 27, 70, 8, 21, 65, 81, 92, 24, 29, 10, 60, 16, 37, 68, 6, 89, 16, 73, 57, 8, 39, 10, 83, 30, 35, 54, 79, 40, 42, 63, 31, 28, 77, 53, 86, 76, 70, 69, 36, 72, 50, 92, 85, 66, 80, 32, 31, 25, 53, 61, 45, 37, 15, 64, 24, 50, 70, 85, 18, 57, 71, 43, 44, 0, 94, 35, 7, 2, 46, 14, 45, 82, 18, 93, 37, 54, 91, 34, 41, 86, 70, 9, 73, 27, 4, 5, 79, 39, 98, 15, 90, 48, 48, 16, 42, 17, 23, 1, 51, 31, 96, 29, 70, 99, 82, 36, 84, 40, 60, 79, 58, 87, 77, 58, 40, 80, 58, 42, 48, 68, 68, 17, 23, 74, 2, 2, 70, 87, 48, 98, 18, 32, 61, 24, 22, 72, 0, 37, 62, 17, 54, 24, 57, 10, 45, 10, 39, 92, 97, 47, 7, 35, 77, 79, 63, 85, 31, 28, 43, 16, 70, 81, 90, 27, 20, 70, 70, 2, 78, 78, 55, 95, 51, 61, 81, 6, 51, 43, 1, 9, 77, 97, 88, 93, 58, 2, 95, 72, 94, 66, 38, 24, 19, 45, 90, 42, 73, 15, 10, 54, 59, 79, 61, 39, 27, 81, 29, 73, 37, 35, 23, 33, 90, 66, 64, 19, 74, 64, 34, 14, 7, 8, 38, 24, 86, 24, 75, 84, 2, 42, 63, 53, 23, 15, 94, 23, 65, 40, 97, 98, 63, 16, 68, 19, 74, 23, 41, 47, 24, 80, 17, 87, 32, 57, 12, 70, 58, 65, 13, 36, 71, 57, 14, 37, 78, 8, 42, 94, 55, 45, 65, 50, 90, 12, 98, 26, 57, 22, 91, 4, 77, 85, 47, 32, 73, 7, 64, 0, 7, 69, 33, 34, 60, 54, 42, 41, 47, 36, 30, 65, 25, 79, 24, 38, 76, 78, 33, 99, 71, 0, 80, 32, 27, 88, 53, 7, 13, 70, 64, 15, 62, 43, 8, 48, 94, 51, 32, 14, 46, 67, 87, 46, 4, 6, 92, 77, 71, 12, 79, 67, 2, 6, 3, 42, 13, 72, 32, 53, 27, 5, 62, 68, 78, 5, 76, 77, 57, 81, 63, 99, 0, 77, 33, 37, 75, 21, 68, 45, 73, 0, 22, 35, 92, 58, 78, 11, 30, 86, 89, 46, 5, 24, 92, 75, 18, 40, 44, 29, 14, 98, 34, 79, 73, 12, 31, 11, 26, 45, 36, 39, 49, 39, 73, 37, 11, 1, 73, 69, 73, 21, 50, 73, 51, 13, 4, 62, 72, 97, 64, 58, 25, 49, 9, 94, 20, 2, 90, 32, 72, 24, 56, 22, 80, 25, 33, 29, 47, 35, 74, 68, 0, 54, 51, 10, 5, 85, 33, 42, 30, 33, 36, 50, 2, 18, 10, 29, 70, 53, 29, 10, 4, 92, 15, 74, 9, 28, 23, 71, 79, 81, 57, 38, 67, 94, 55, 44, 49, 69, 64, 21, 76, 47, 40, 83, 62, 8, 0, 31, 2, 85, 51, 81, 36, 77, 35, 87, 10, 53, 49, 94, 58, 53, 66, 42, 3, 46, 98, 57, 4, 78, 73, 22, 83, 45, 15, 95, 28, 52, 98, 26, 88, 51, 75, 49, 56, 16, 92, 10, 91, 68, 94, 54, 9, 10, 90, 39, 35, 11, 1, 2, 74, 30, 38, 69, 83, 44, 91, 56, 14, 35, 2, 87, 14, 68, 28, 78, 46, 16, 20, 71, 16, 8, 35, 3, 23, 74, 89, 65, 60, 16, 30, 72, 21, 66, 74, 35, 87, 3, 47, 30, 96, 66, 5, 78, 7, 75, 7, 18, 68, 24, 80, 53, 50, 79, 37, 5, 56, 18, 10, 70, 94, 87, 30, 33, 96, 37, 28, 36, 24, 65, 58, 45, 70, 70, 98, 77, 87, 79, 43, 9, 68, 35, 60, 76, 2, 48, 64, 92, 11, 88, 42, 92, 39, 46, 7, 63, 58, 86, 67, 66, 71, 86, 47, 9, 27, 85, 41, 58, 36, 62, 37, 3, 16, 27, 21, 74, 48, 8, 92, 4, 73, 34, 11, 84, 57, 52, 2, 47, 97, 14, 24, 37, 40, 49, 8, 33, 6, 31, 11, 27, 16, 75, 92, 87, 58, 57, 31, 5, 40, 33, 33, 60, 90, 77, 34, 6, 30, 46, 19, 90, 30, 7, 71, 61, 50, 80, 44, 97, 73, 29, 87, 27, 98, 10, 73, 51, 72, 40, 77, 14, 37, 18, 77, 7, 19, 16, 9, 79, 59, 52, 12, 98, 43, 66, 76, 81, 83, 10, 57, 66, 35, 54, 1, 57, 6, 74, 72, 86, 21, 69, 17, 80, 79, 40, 64, 51, 0, 30, 62, 41, 71, 36, 75, 10, 92, 4, 97, 75, 38, 2, 84, 67, 0, 48, 27, 87, 24, 14, 26, 7, 12, 49, 25, 24, 86, 10, 33, 68, 4, 42, 8, 78, 71, 7, 37, 55, 32, 23, 11, 94, 79, 12, 30, 2, 32, 7, 22, 63, 69, 34, 36, 75, 92, 37, 33, 48, 39, 14, 59, 29, 1, 2, 24, 94, 60, 90, 2, 73, 10, 0, 5, 9, 41, 66, 46, 48, 0, 86, 30, 30, 32, 82, 21, 57, 9, 66, 95, 45, 64, 20, 53, 19, 69, 30, 44, 43, 2, 99, 50, 32, 37, 57, 24, 26, 43, 14, 79, 77, 5, 66, 69, 98, 78, 15, 99, 42, 85, 67, 74, 29, 57, 82, 78, 54, 2, 7, 40, 0, 87, 2, 64, 7, 38, 37, 85, 53, 10, 24, 17, 59, 30, 34, 51, 45, 67, 42, 85, 23, 10, 24, 78, 32, 10, 38, 90, 84, 57, 88, 96, 6, 27, 23, 58, 25, 11, 87, 85, 9, 85, 45, 22, 4, 1, 66, 73, 78, 37, 2, 51, 98, 27, 38, 24, 25, 9, 45, 85, 62, 38, 28, 9, 19, 75, 23, 41, 40, 65, 0, 4, 74, 40, 57, 80, 84, 56, 1, 10, 51, 75, 43, 23, 57, 47, 95, 44, 62, 15, 83, 70, 74, 47, 38, 99, 1, 23, 16, 31, 69, 70, 33, 11, 37, 70, 68, 20, 35, 18, 11, 73, 10, 32, 20, 43, 24, 19, 65, 91, 40, 91, 6, 80, 26, 2, 9, 55, 61, 24, 37, 30, 24, 42, 14, 80, 3, 10, 57, 26, 56, 84, 38, 82, 55, 33, 2, 40, 2, 61, 58, 16, 75, 93, 54, 54, 24, 60, 44, 23, 59, 55, 68, 26, 14, 70, 21, 61, 10, 38, 76, 62, 19, 18, 62, 64, 24, 11, 30, 94, 18, 17, 87, 24, 16, 32, 26, 73, 3, 56, 2, 47, 66, 65, 87, 18, 53, 5, 86, 40, 52, 80, 38, 53, 99, 93, 23, 85, 97, 30, 22, 61, 17, 71, 48, 15, 28, 51, 60, 15, 3, 97, 20, 27, 65, 14, 43, 76, 23, 41, 36, 2, 57, 45, 53, 16, 96, 74, 5, 15, 54, 40, 87, 95, 34, 67, 72, 55, 67, 88, 6, 54, 67, 16, 11, 59, 70, 76, 2, 8, 58, 13, 71, 97, 79, 97, 57, 96, 86, 84, 29, 84, 15, 60, 18, 70, 48, 96, 1, 43, 79, 68, 44, 14, 5, 0, 23, 37, 16, 34, 56, 83, 63, 26, 33, 10, 44, 37, 17, 67, 91, 2, 50, 2, 10, 51, 68, 26, 56, 39, 70, 11, 79, 83, 97, 2, 46, 62, 33, 19, 49, 80, 8, 38, 39, 76, 70, 55, 28, 19, 61, 52, 11, 93, 38, 17, 89, 29, 84, 67, 78, 80, 28, 11, 57, 37, 97, 44, 30, 72, 21, 81, 5, 1, 47, 95, 92, 59, 64, 72, 84, 11, 3, 7, 45, 84, 86, 62, 38, 1, 40, 18, 58, 84, 33, 23, 89, 62, 60, 46, 69, 48, 79, 56, 37, 2, 11, 65, 12, 84, 46, 54, 59, 51, 33, 72, 85, 57, 60, 60, 7, 18, 47, 82, 32, 54, 37, 45, 45, 10, 23, 36, 87, 70, 22, 60, 83, 80, 54, 43, 83, 37, 65, 18, 87, 16, 2, 37, 57, 84, 23, 91, 5, 14, 29, 86, 19, 75, 32, 97, 10, 97, 84, 14, 33, 66, 70, 14, 35, 55, 38, 12, 97, 11, 62, 49, 4, 39, 96, 67, 1, 18, 44, 98, 40, 84, 14, 48, 15, 7, 43, 60, 60, 16, 63, 76, 11, 73, 96, 56, 34, 18, 2, 10, 27, 60, 54, 92, 13, 50, 88, 31, 22, 24, 96, 24, 75, 15, 26, 61, 15, 43, 37, 65, 90, 24, 1, 53, 1, 10, 62, 22, 10, 42, 10, 3, 55, 93, 57, 74, 65, 66, 57, 10, 24, 81, 3, 33, 42, 48, 0, 57, 54, 16, 42, 24, 64, 23, 31, 14, 44, 99, 48, 14, 38, 85, 51, 94, 45, 9, 2, 43, 34, 56, 65, 10, 65, 98, 36, 54, 85, 69, 49, 12, 57, 10, 29, 30, 96, 6, 7, 10, 47, 53, 36, 24, 89, 37, 46, 37, 24, 22, 9, 2, 45, 91, 95, 40, 2, 69, 93, 62, 51, 22, 1, 70, 0, 76, 57, 6, 44, 60, 50, 64, 63, 97, 7, 96, 27, 16, 40, 15, 87, 95, 22, 66, 70, 74, 71, 92, 11, 78, 81, 89, 22, 9, 75, 58, 24, 66, 50, 90, 58, 76, 33, 20, 95, 18, 44, 79, 17, 93, 54, 11, 94, 19, 53, 85, 57, 3, 46, 23, 82, 37, 45, 2, 67, 7, 49, 14, 28, 85, 10, 74, 32, 30, 58, 33, 27, 27, 37, 18, 16, 72, 87, 53, 73, 75, 2, 4, 98, 60, 3, 35, 39, 11, 11, 72, 32, 77, 30, 10, 78, 47, 65, 86, 8, 12, 63, 76, 65, 85, 69, 29, 27, 22, 92, 66, 71, 22, 98, 74, 68, 70, 72, 55, 8, 23, 24, 81, 64, 92, 56, 75, 57, 89, 9, 3, 54, 71, 23, 46, 64, 10, 26, 97, 0, 2, 60, 5, 26, 56, 34, 2, 51, 12, 87, 18, 75, 86, 63, 17, 31, 89, 69, 54, 62, 25, 40, 43, 80, 97, 77, 93, 58, 84, 36, 71, 1, 0, 39, 2, 78, 74, 54, 10, 98, 42, 55, 60, 66, 27, 92, 49, 61, 78, 82, 15, 87, 39, 39, 19, 37, 93, 23, 49, 45, 55, 29, 72, 75, 2, 49, 57, 27, 37, 32, 2, 26, 42, 13, 4, 97, 4, 74, 55, 38, 69, 58, 69, 76, 32, 57, 14, 81, 5, 12, 81, 88, 71, 5, 68, 2, 39, 72, 1, 8, 28, 49, 62, 48, 18, 57, 10, 62, 72, 33, 40, 70, 6, 56, 3, 48, 1, 75, 80, 58, 37, 59, 83, 31, 88, 23, 7, 79, 85, 37, 61, 87, 10, 55, 45, 72, 49, 4, 81, 76, 11, 3, 93, 86, 98, 54, 97, 33, 10, 80, 83, 0, 57, 35, 72, 4, 45, 30, 78, 24, 50, 31, 3, 16, 80, 27, 79, 86, 49, 65, 34, 7, 44, 0, 22, 21, 40, 31, 7, 60, 71, 21, 29, 93, 37, 1, 6, 2, 49, 5, 65, 94, 28, 8, 6, 43, 36, 57, 16, 76, 7, 91, 90, 35, 52, 22, 17, 65, 19, 88, 6, 14, 44, 10, 36, 45, 78, 75, 37, 85, 85, 18, 97, 22, 14, 19, 23, 10, 53, 55, 69, 2, 87, 49, 77, 43, 4, 95, 36, 44, 28, 57, 2, 11, 70, 53, 73, 64, 21, 44, 73, 57, 92, 35, 70, 72, 59, 97, 83, 5, 7, 86, 13, 76, 87, 6, 36, 28, 60, 62, 29, 47, 9, 78, 66, 38, 39, 5, 9, 27, 17, 8, 83, 26, 27, 76, 19, 58, 24, 95, 23, 15, 75, 52, 68, 90, 30, 73, 4, 39, 54, 70, 45, 49, 24, 53, 2, 47, 95, 17, 81, 23, 62, 43, 3, 47, 51, 4, 72, 29, 22, 53, 83, 11, 66, 4, 38, 66, 10, 54, 71, 42, 56, 28, 15, 52, 37, 73, 10, 39, 54, 50, 6, 93, 70, 4, 70, 38, 75, 30, 10, 48, 29, 59, 94, 58, 37, 5, 10, 55, 15, 78, 96, 68, 60, 50, 35, 63, 13, 97, 60, 59, 8, 22, 8, 60, 3, 70, 3, 57, 69, 81, 89, 6, 7, 70, 84, 46, 12, 83, 9, 57, 60, 53, 26, 57, 65, 3, 16, 17, 36, 75, 26, 13, 49, 86, 47, 11, 84, 37, 57, 76, 99, 23, 9, 64, 28, 23, 57, 85, 15, 70, 68, 71, 44, 99, 68, 15, 14, 84, 3, 60, 4, 10, 92, 2, 11, 60, 37, 37, 7, 8, 13, 41, 15, 85, 59, 4, 11, 23, 65, 52, 38, 9, 48, 19, 80, 40, 14, 33, 71, 34, 62, 2, 24, 56, 40, 58, 79, 87, 68, 71, 78, 82, 16, 49, 63, 37, 47, 84, 40, 37, 24, 0, 0, 42, 2, 66, 24, 22, 55, 75, 50, 96, 78, 20, 44, 81, 83, 2, 57, 28, 25, 74, 0, 34, 7, 23, 5, 44, 58, 23, 88, 63, 31, 30, 37, 11, 0, 93, 69, 68, 33, 92, 85, 33, 6, 95, 65, 58, 95, 69, 64, 59, 41, 74, 97, 70, 93, 2, 79, 69, 38, 19, 97, 47, 95, 51, 4, 51, 78, 50, 68, 28, 2, 73, 44, 40, 53, 17, 55, 10, 30, 58, 50, 79, 73, 9, 14, 96, 62, 14, 10, 5, 37, 32, 23, 57, 58, 7, 43, 34, 17, 6, 98, 50, 67, 2, 55, 36, 67, 34, 24, 20, 96, 79, 79, 95, 20, 85, 69, 6, 10, 92, 99, 11, 12, 49, 66, 63, 85, 24, 43, 58, 30, 11, 80, 46, 72, 58, 50, 37, 17, 28, 99, 60, 71, 33, 24, 71, 68, 46, 44, 13, 5, 1, 98, 18, 80, 52, 29, 78, 72, 65, 66, 92, 44, 83, 3, 54, 89, 19, 89, 1, 97, 72, 75, 73, 84, 26, 5, 64, 7, 42, 49, 27, 66, 12, 13, 96, 19, 97, 10, 97, 37, 63, 89, 48, 60, 13, 45, 53, 25, 11, 6, 8, 68, 5, 39, 49, 50, 67, 29, 16, 67, 66, 33, 38, 5, 18, 25, 10, 47, 2, 8, 10, 85, 81, 30, 63, 80, 27, 65, 70, 35, 5, 29, 39, 41, 12, 99, 57, 43, 5, 84, 9, 23, 47, 83, 30, 79, 3, 86, 23, 88, 65, 37, 91, 8, 42, 15, 3, 24, 82, 72, 37, 88, 87, 24, 78, 1, 66, 10, 93, 39, 54, 80, 88, 9, 85, 69, 87, 86, 88, 97, 53, 3, 48, 5, 18, 28, 35, 65, 19, 37, 64, 55, 49, 39, 54, 94, 40, 50, 24, 59, 64, 33, 87, 33, 34, 23, 49, 32, 5, 86, 10, 37, 30, 44, 36, 84, 16, 1, 56, 68, 83, 41, 30, 85, 16, 42, 2, 39, 70, 0, 60, 7, 29, 37, 26, 97, 55, 90, 6, 6, 38, 4, 16, 92, 80, 58, 82, 25, 5, 87, 78, 10, 22, 81, 81, 60, 8, 91, 37, 5, 57, 1, 78, 72, 50, 37, 80, 20, 37, 37, 1, 17, 1, 83, 97, 2, 78, 79, 84, 0, 29, 45, 57, 3, 1, 12, 7, 95, 47, 48, 13, 23, 32, 77, 49, 43, 45, 88, 13, 93, 21, 39, 56, 10, 14, 54, 38, 14, 20, 64, 76, 7, 11, 60, 93, 67, 76, 62, 88, 43, 91, 42, 12, 89, 24, 9, 4, 73, 78, 25, 88, 63, 68, 2, 45, 76, 4, 61, 92, 23, 94, 46, 60, 58, 24, 35, 95, 50, 87, 40, 69, 25, 99, 27, 5, 37, 2, 30, 57, 37, 76, 68, 70, 44, 72, 12, 21, 66, 65, 78, 20, 2, 64, 55, 59, 87, 79, 1, 18, 19, 52, 10, 90, 24, 10, 5, 3, 52, 10, 70, 45, 12, 66, 95, 18, 15, 53, 12, 87, 96, 65, 56, 6, 97, 43, 60, 30, 87, 28, 63, 73, 92, 63, 54, 67, 11, 94, 2, 5, 95, 78, 3, 62, 25, 25, 2, 22, 20, 4, 70, 53, 14, 11, 10, 21, 33, 71, 95, 23, 78, 34, 16, 85, 63, 19, 49, 79, 82, 63, 92, 58, 42, 80, 94, 97, 89, 74, 37, 62, 72, 56, 75, 71, 33, 68, 16, 75, 43, 20, 85, 15, 64, 94, 82, 26, 57, 71, 15, 33, 3, 11, 28, 57, 26, 10, 95, 48, 24, 65, 43, 80, 59, 48, 71, 14, 59, 23, 27, 85, 24, 94, 12, 38, 4, 50, 67, 2, 32, 68, 53, 77, 86, 5, 95, 7, 75, 4, 81, 95, 8, 21, 63, 93, 38, 30, 37, 89, 36, 62, 69, 83, 22, 85, 47, 97, 87, 24, 19, 74, 80, 72, 43, 43, 37, 37, 24, 70, 9, 10, 46, 37, 32, 50, 51, 69, 43, 86, 73, 63, 71, 54, 28, 50, 96, 80, 53, 46, 31, 69, 42, 85, 71, 7, 24, 77, 2, 73, 45, 8, 23, 29, 10, 38, 14, 55, 26, 38, 98, 32, 22, 83, 85, 23, 29, 7, 40, 49, 72, 58, 57, 74, 86, 84, 36, 1, 37, 54, 75, 79, 89, 53, 31, 34, 72, 47, 7, 28, 81, 88, 83, 70, 68, 64, 81, 80, 10, 47, 54, 23, 5, 67, 14, 82, 48, 98, 57, 5, 2, 36, 92, 16, 76, 49, 72, 14, 38, 81, 18, 18, 53, 50, 71, 85, 53, 36, 55, 53, 55, 22, 22, 6, 34, 6, 99, 49, 94, 2, 62, 15, 12, 66, 7, 46, 69, 79, 90, 91, 72, 45, 96, 60, 86, 5, 1, 45, 26, 66, 88, 73, 39, 51, 9, 49, 47, 40, 0, 37, 26, 37, 83, 37, 32, 80, 24, 41, 57, 53, 14, 75, 15, 37, 39, 57, 79, 71, 65, 89, 49, 49, 53, 30, 30, 18, 83, 23, 60, 3, 63, 26, 89, 67, 25, 81, 1, 17, 97, 99, 57, 13, 52, 10, 53, 51, 40, 7, 73, 41, 71, 10, 73, 49, 30, 49, 91, 63, 95, 38, 14, 64, 64, 60, 71, 98, 59, 40, 81, 54, 73, 32, 68, 52, 39, 4, 88, 51, 9, 9, 5, 96, 99, 24, 97, 37, 7, 68, 14, 4, 68, 30, 58, 11, 93, 15, 71, 85, 15, 10, 67, 2, 58, 7, 85, 63, 61, 68, 21, 88, 57, 50, 65, 24, 70, 52, 64, 27, 26, 57, 16, 69, 10, 28, 30, 24, 39, 14, 45, 73, 48, 79, 42, 55, 47, 33, 61, 69, 51, 24, 14, 83, 18, 23, 99, 87, 24, 83, 84, 2, 75, 11, 72, 10, 1, 24, 30, 35, 25, 99, 74, 54, 16, 38, 20, 85, 28, 52, 36, 38, 31, 36, 36, 54, 15, 37, 68, 84, 9, 65, 91, 6, 24, 59, 39, 35, 43, 17, 59, 68, 32, 28, 64, 42, 24, 42, 33, 26, 38, 29, 94, 2, 52, 81, 66, 78, 82, 81, 39, 99, 75, 2, 39, 37, 45, 85, 67, 47, 53, 36, 29, 34, 11, 67, 26, 24, 44, 57, 93, 5, 29, 63, 44, 48, 2, 54, 23, 36, 60, 86, 24, 78, 56, 8, 49, 11, 43, 50, 42, 91, 39, 10, 30, 71, 3, 42, 66, 24, 8, 15, 83, 23, 70, 28, 89, 7, 85, 24, 38, 41, 52, 78, 50, 82, 93, 16, 23, 54, 16, 19, 27, 75, 90, 74, 99, 53, 23, 58, 33, 55, 8, 40, 32, 97, 8, 65, 64, 42, 38, 22, 10, 24, 66, 59, 51, 16, 87, 38, 46, 72, 85, 51, 34, 62, 95, 73, 53, 85, 4, 81, 24, 39, 68, 30, 46, 73, 9, 37, 74, 57, 25, 2, 49, 56, 71, 18, 72, 58, 33, 40, 42, 4, 38, 27, 38, 78, 93, 2, 45, 48, 12, 37, 70, 24, 39, 42, 7, 53, 57, 3, 25, 74, 3, 3, 7, 2, 1, 75, 96, 50, 30, 24, 89, 83, 44, 89, 23, 30, 51, 57, 37, 92, 12, 52, 81, 27, 48, 53, 76, 25, 16, 5, 85, 37, 98, 42, 62, 55, 24, 14, 51, 45, 35, 3, 20, 33, 85, 6, 0, 50, 45, 8, 77, 49, 29, 1, 36, 22, 24, 35, 30, 10, 50, 0, 77, 7, 87, 66, 8, 9, 68, 11, 2, 81, 24, 10, 17, 57, 97, 37, 72, 57, 99, 58, 62, 50, 30, 25, 85, 3, 46, 24, 91, 71, 13, 55, 65, 95, 27, 34, 57, 36, 20, 95, 17, 27, 10, 87, 60, 37, 22, 81, 5, 37, 62, 71, 83, 60, 48, 18, 84, 45, 2, 48, 87, 74, 87, 36, 89, 22, 0, 97, 33, 42, 67, 63, 3, 18, 5, 55, 9, 44, 88, 44, 62, 16, 3, 24, 79, 76, 78, 23, 15, 36, 67, 69, 43, 35, 78, 19, 34, 43, 75, 1, 3, 1, 81, 69, 67, 0, 76, 48, 53, 37, 40, 36, 81, 9, 63, 29, 86, 38, 1, 26, 8, 53, 58, 92, 6, 9, 52, 76, 22, 37, 35, 59, 1, 73, 7, 76, 33, 16, 75, 32, 56, 54, 70, 40, 58, 17, 87, 58, 87, 73, 40, 78, 1, 88, 62, 28, 50, 47, 39, 86, 18, 24, 41, 89, 87, 31, 21, 79, 64, 23, 14, 24, 7, 57, 35, 94, 37, 36, 30, 10, 0, 57, 40, 58, 99, 43, 34, 1, 36, 3, 81, 69, 80, 33, 18, 75, 62, 14, 5, 55, 32, 97, 2, 26, 27, 57, 10, 15, 57, 46, 86, 47, 13, 39, 88, 69, 63, 3, 58, 50, 46, 24, 83, 56, 73, 5, 76, 65, 67, 70, 43, 66, 26, 85, 57, 22, 29, 10, 37, 67, 29, 69, 43, 4, 10, 83, 33, 0, 73, 9, 2, 14, 88, 83, 15, 39, 29, 29, 30, 10, 12, 73, 11, 87, 62, 5, 34, 22, 57, 53, 80, 77, 37, 72, 76, 1, 32, 23, 91, 15, 0, 71, 88, 71, 63, 5, 37, 74, 57, 79, 39, 21, 0, 78, 24, 70, 93, 71, 73, 23, 90, 4, 4, 34, 9, 88, 54, 70, 53, 57, 27, 45, 98, 15, 45, 79, 72, 26, 97, 71, 76, 37, 1, 72, 94, 47, 39, 29, 18, 36, 75, 75, 25, 15, 31, 4, 98, 39, 71, 37, 87, 55, 55, 94, 35, 2, 46, 63, 38, 11, 54, 16, 51, 42, 29, 98, 93, 7, 62, 88, 37, 58, 77, 84, 56, 2, 50, 98, 17, 22, 43, 69, 64, 79, 23, 28, 62, 42, 11, 13, 31, 0, 51, 41, 50, 60, 29, 36, 14, 29, 9, 41, 28, 68, 73, 37, 4, 82, 19, 75, 66, 99, 0, 44, 76, 62, 97, 72, 3, 47, 98, 64, 52, 43, 93, 36, 87, 10, 24, 12, 16, 31, 73, 53, 52, 69, 4, 85, 45, 92, 32, 89, 29, 50, 30, 67, 36, 1, 87, 66, 68, 34, 21, 86, 7, 9, 58, 5, 18, 63, 24, 35, 35, 9, 92, 47, 42, 79, 3, 3, 40, 70, 65, 29, 28, 2, 17, 97, 62, 71, 20, 56, 10, 72, 86, 32, 82, 0, 75, 42, 0, 94, 2, 18, 91, 34, 11, 76, 71, 77, 26, 76, 70, 69, 45, 43, 89, 4, 24, 79, 68, 13, 32, 9, 62, 36, 95, 11, 3, 83, 77, 21, 9, 75, 37, 77, 18, 29, 10, 59, 57, 31, 94, 32, 10, 47, 33, 69, 60, 19, 49, 29, 11, 90, 4, 60, 25, 46, 98, 29, 72, 30, 24, 73, 68, 6, 63, 95, 8, 54, 10, 53, 11, 53, 50, 72, 78, 83, 70, 47, 40, 60, 5, 12, 99, 80, 7, 11, 62, 67, 98, 33, 56, 2, 82, 91, 2, 21, 62, 80, 1, 21, 6, 83, 68, 99, 17, 1, 68, 54, 12, 73, 97, 65, 59, 19, 19, 71, 73, 92, 9, 83, 93, 86, 75, 98, 47, 30, 64, 57, 71, 69, 4, 44, 54, 38, 59, 69, 17, 57, 84, 9, 55, 65, 54, 78, 0, 9, 2, 22, 97, 83, 9, 47, 2, 49, 47, 49, 79, 72, 47, 68, 96, 67, 6, 63, 87, 2, 74, 0, 20, 58, 66, 75, 7, 65, 77, 54, 59, 63, 53, 85, 80, 21, 83, 58, 77, 62, 8, 81, 97, 92, 1, 6, 55, 84, 32, 2, 73, 18, 5, 66, 60, 28, 29, 5, 44, 39, 48, 36, 25, 99, 84, 96, 5, 21, 75, 69, 2, 43, 28, 63, 8, 68, 44, 92, 40, 57, 7, 14, 9, 48, 86, 28, 22, 74, 66, 38, 0, 48, 6, 93, 29, 46, 61, 37, 15, 67, 27, 61, 63, 93, 69, 79, 64, 7, 74, 83, 58, 95, 2, 18, 21, 57, 2, 7, 80, 56, 39, 11, 36, 49, 3, 4, 29, 93, 53, 39, 60, 76, 97, 37, 76, 99, 27, 14, 59, 92, 54, 74, 54, 73, 93, 53, 98, 75, 85, 30, 52, 23, 2, 5, 37, 68, 27, 97, 90, 35, 72, 39, 52, 61, 35, 31, 78, 81, 46, 71, 72, 65, 78, 23, 65, 33, 99, 3, 38, 55, 16, 74, 79, 91, 87, 64, 2, 86, 71, 52, 78, 12, 55, 6, 82, 12, 54, 22, 2, 91, 60, 6, 53, 0, 21, 92, 97, 37, 23, 46, 21, 62, 88, 62, 90, 36, 95, 7, 28, 64, 53, 10, 63, 48, 48, 75, 13, 80, 80, 17, 20, 20, 99, 92, 26, 32, 37, 89, 43, 30, 8, 23, 45, 87, 48, 91, 80, 85, 98, 85, 13, 98, 16, 27, 28, 78, 2, 83, 69, 52, 24, 26, 39, 42, 42, 10, 92, 34, 4, 2, 4, 57, 29, 89, 26, 99, 77, 69, 2, 28, 51, 68, 30, 60, 95, 66, 49, 72, 43, 33, 16, 26, 36, 57, 1, 46, 55, 47, 7, 39, 35, 39, 80, 39, 14, 11, 69, 35, 81, 77, 76, 49, 87, 2, 47, 73, 41, 53, 35, 55, 57, 40, 37, 90, 38, 97, 15, 43, 45, 57, 65, 79, 37, 95, 91, 58, 88, 86, 65, 83, 44, 72, 49, 16, 91, 54, 8, 23, 11, 6, 54, 78, 92, 63, 63, 35, 55, 48, 9, 69, 22, 40, 90, 73, 15, 11, 52, 58, 93, 47, 38, 8, 26, 40, 76, 49, 82, 78, 53, 55, 92, 9, 14, 57, 24, 10, 38, 24, 59, 19, 44, 91, 63, 81, 5, 64, 57, 91, 23, 47, 43, 9, 55, 33, 73, 88, 33, 35, 83, 29, 50, 63, 88, 21, 71, 95, 48, 66, 9, 36, 23, 2, 44, 57, 18, 49, 77, 42, 75, 95, 33, 89, 9, 10, 53, 3, 52, 52, 35, 72, 74, 1, 10, 15, 50, 46, 6, 48, 16, 32, 14, 80, 97, 72, 52, 2, 10, 15, 15, 33, 80, 84, 55, 51, 36, 78, 25, 13, 52, 16, 53, 58, 4, 7, 49, 74, 10, 64, 11, 5, 76, 48, 8, 39, 48, 83, 87, 59, 93, 37, 37, 56, 74, 87, 38, 47, 98, 72, 91, 96, 16, 18, 60, 7, 59, 16, 55, 32, 24, 42, 78, 52, 11, 68, 29, 46, 35, 74, 69, 24, 17, 84, 80, 82, 45, 86, 8, 44, 69, 46, 64, 30, 45, 37, 24, 58, 84, 89, 55, 93, 98, 2, 3, 3, 9, 83, 86, 28, 31, 48, 9, 47, 44, 70, 85, 36, 97, 72, 68, 29, 34, 97, 6, 45, 57, 28, 64, 34, 40, 39, 26, 93, 10, 74, 54, 81, 91, 11, 58, 28, 5, 42, 68, 80, 21, 48, 87, 37, 70, 29, 37, 37, 68, 21, 80, 81, 81, 54, 20, 3, 11, 28, 32, 55, 22, 12, 74, 73, 5, 2, 70, 71, 53, 69, 71, 36, 99, 37, 49, 1, 7, 50, 41, 18, 11, 37, 30, 66, 76, 49, 86, 13, 69, 68, 55, 78, 24, 4, 72, 40, 37, 10, 56, 18, 80, 69, 15, 61, 46, 67, 49, 92, 78, 39, 55, 74, 38, 58, 91, 12, 37, 14, 88, 23, 60, 94, 79, 5, 78, 3, 87, 23, 40, 12, 66, 59, 0, 36, 2, 31, 43, 37, 36, 1, 16, 69, 35, 28, 82, 72, 12, 42, 39, 12, 7, 29, 88, 16, 38, 43, 2, 63, 84, 3, 9, 19, 66, 57, 24, 79, 51, 15, 2, 73, 78, 38, 65, 56, 80, 7, 39, 47, 59, 81, 36, 0, 46, 66, 62, 57, 74, 96, 63, 77, 79, 59, 8, 86, 37, 62, 37, 40, 92, 54, 50, 44, 35, 47, 53, 24, 86, 12, 70, 66, 77, 26, 7, 78, 18, 23, 10, 2, 27, 7, 25, 97, 2, 34, 98, 93, 40, 52, 44, 37, 85, 93, 38, 86, 55, 15, 82, 43, 34, 70, 57, 42, 0, 3, 47, 23, 27, 63, 78, 7, 22, 91, 99, 85, 68, 26, 88, 9, 19, 77, 21, 79, 43, 99, 24, 39, 56, 36, 4, 74, 52, 7, 98, 63, 13, 66, 83, 57, 78, 97, 65, 26, 75, 2, 51, 55, 0, 15, 72, 10, 59, 65, 29, 47, 87, 27, 32, 87, 79, 85, 37, 1, 49, 93, 0, 76, 81, 73, 54, 24, 44, 70, 2, 78, 87, 2, 16, 24, 59, 86, 72, 37, 57, 11, 77, 6, 95, 32, 20, 37, 5, 4, 11, 45, 85, 6, 58, 34, 25, 50, 59, 81, 13, 62, 53, 38, 68, 40, 69, 66, 10, 62, 59, 90, 51, 58, 44, 0, 36, 60, 38, 28, 9, 28, 56, 49, 92, 44, 49, 67, 70, 45, 82, 32, 24, 60, 91, 24, 89, 91, 51, 81, 55, 98, 24, 98, 97, 47, 57, 97, 13, 22, 44, 2, 23, 57, 98, 71, 27, 78, 10, 35, 45, 48, 52, 93, 53, 95, 67, 35, 93, 92, 69, 57, 5, 10, 37, 57, 4, 22, 2, 10, 38, 71, 36, 5, 74, 11, 31, 1, 43, 26, 66, 24, 86, 51, 20, 9, 33, 11, 95, 0, 15, 80, 38, 41, 93, 48, 65, 19, 56, 53, 10, 10, 28, 52, 3, 69, 32, 92, 24, 11, 73, 77, 71, 57, 85, 83, 34, 2, 19, 67, 65, 83, 7, 36, 39, 13, 3, 76, 75, 37, 73, 81, 59, 73, 83, 89, 51, 16, 23, 69, 88, 99, 78, 63, 8, 23, 72, 84, 72, 43, 57, 57, 2, 80, 77, 36, 49, 30, 38, 22, 8, 53, 37, 76, 2, 83, 10, 79, 93, 10, 24, 48, 2, 6, 77, 14, 83, 23, 31, 58, 4, 84, 48, 15, 98, 18, 57, 13, 88, 20, 77, 0, 72, 37, 25, 29, 10, 69, 99, 82, 78, 34, 86, 83, 0, 75, 48, 91, 40, 37, 38, 2, 58, 69, 61, 21, 20, 53, 89, 42, 16, 80, 19, 30, 38, 55, 28, 46, 90, 23, 64, 53, 57, 37, 35, 2, 75, 79, 24, 6, 79, 18, 72, 29, 32, 75, 23, 54, 59, 53, 16, 26, 37, 39, 72, 35, 57, 68, 85, 86, 36, 9, 5, 47, 86, 47, 49, 71, 69, 84, 57, 13, 36, 94, 16, 33, 9, 46, 90, 68, 14, 29, 27, 62, 35, 7, 15, 49, 59, 23, 57, 28, 19, 54, 98, 92, 26, 83, 3, 57, 66, 58, 84, 85, 74, 24, 75, 62, 31, 42, 0, 72, 39, 9, 6, 60, 73, 64, 47, 39, 41, 96, 1, 7, 16, 78, 51, 2, 49, 51, 23, 46, 92, 61, 7, 63, 45, 34, 85, 80, 14, 76, 38, 61, 12, 38, 3, 49, 3, 28, 36, 6, 39, 37, 38, 3, 78, 82, 99, 38, 11, 59, 66, 85, 68, 51, 28, 36, 45, 42, 86, 39, 37, 79, 97, 10, 17, 5, 83, 79, 48, 74, 62, 14, 10, 94, 31, 70, 97, 63, 96, 48, 65, 51, 62, 46, 24, 30, 65, 25, 69, 90, 0, 37, 70, 65, 22, 28, 24, 37, 23, 18, 5, 49, 96, 78, 87, 8, 36, 59, 32, 88, 46, 24, 45, 62, 55, 44, 43, 63, 68, 27, 16, 87, 24, 5, 27, 36, 39, 36, 13, 24, 16, 89, 19, 10, 23, 24, 44, 6, 56, 11, 67, 44, 57, 59, 93, 64, 17, 30, 88, 65, 94, 64, 99, 71, 67, 7, 47, 57, 20, 64, 26, 1, 24, 62, 78, 95, 27, 0, 18, 49, 95, 78, 35, 37, 24, 74, 93, 4, 73, 76, 33, 82, 79, 23, 80, 93, 52, 10, 88, 78, 88, 54, 71, 37, 67, 2, 27, 34, 98, 10, 81, 23, 39, 73, 23, 34, 57, 22, 97, 18, 41, 12, 38, 23, 78, 20, 71, 59, 80, 33, 23, 23, 19, 78, 35, 59, 39, 38, 44, 86, 70, 24, 45, 42, 48, 93, 2, 3, 44, 3, 11, 73, 75, 79, 86, 63, 95, 78, 37, 65, 82, 62, 85, 63, 47, 46, 35, 79, 28, 5, 30, 86, 63, 91, 10, 62, 19, 47, 57, 60, 6, 13, 23, 69, 2, 70, 1, 23, 29, 59, 57, 16, 80, 0, 1, 36, 74, 88, 59, 73, 98, 51, 98, 35, 43, 75, 83, 73, 29, 92, 38, 26, 38, 45, 9, 58, 63, 55, 91, 22, 39, 69, 44, 34, 84, 86, 55, 14, 7, 39, 67, 2, 33, 24, 58, 90, 70, 72, 37, 99, 23, 46, 96, 67, 98, 86, 8, 36, 83, 98, 37, 60, 36, 57, 44, 0, 22, 66, 7, 50, 78, 22, 69, 36, 92, 13, 65, 24, 2, 31, 37, 27, 42, 7, 96, 45, 49, 33, 24, 10, 73, 65, 18, 45, 28, 91, 70, 22, 67, 18, 81, 59, 3, 66, 33, 82, 72, 46, 67, 2, 57, 52, 14, 81, 87, 2, 4, 26, 37, 55, 51, 13, 18, 72, 52, 26, 97, 30, 76, 63, 90, 92, 18, 30, 11, 47, 5, 29, 2, 86, 42, 84, 66, 10, 89, 27, 52, 67, 79, 98, 10, 52, 58, 70, 99, 14, 10, 79, 11, 37, 54, 7, 85, 25, 78, 85, 16, 28, 73, 83, 31, 2, 50, 11, 95, 88, 26, 82, 10, 4, 95, 96, 51, 35, 3, 73, 64, 2, 20, 58, 86, 55, 20, 42, 59, 94, 38, 18, 33, 58, 98, 69, 34, 26, 9, 39, 3, 57, 37, 73, 64, 78, 11, 29, 9, 91, 68, 10, 2, 92, 44, 37, 90, 50, 70, 52, 91, 52, 11, 90, 16, 55, 13, 79, 60, 86, 0, 37, 15, 2, 48, 73, 22, 92, 65, 22, 80, 5, 2, 80, 97, 44, 87, 46, 28, 53, 42, 72, 51, 32, 74, 44, 50, 6, 39, 66, 47, 47, 42, 75, 68, 3, 1, 53, 66, 98, 0, 37, 33, 41, 7, 65, 87, 88, 48, 46, 65, 43, 57, 69, 33, 11, 43, 3, 10, 81, 22, 11, 24, 2, 61, 93, 49, 7, 48, 25, 92, 35, 84, 10, 30, 7, 2, 81, 95, 56, 51, 78, 27, 87, 0, 89, 10, 50, 59, 87, 50, 11, 78, 97, 7, 68, 73, 80, 99, 58, 21, 82, 87, 48, 29, 14, 80, 70, 7, 28, 37, 0, 55, 11, 52, 8, 97, 0, 23, 64, 71, 81, 37, 45, 2, 85, 16, 12, 47, 97, 3, 24, 15, 41, 79, 42, 26, 59, 70, 69, 94, 98, 5, 58, 23, 88, 92, 74, 86, 69, 52, 63, 77, 88, 92, 35, 43, 10, 91, 24, 44, 65, 57, 77, 4, 60, 84, 5, 34, 37, 44, 72, 15, 71, 70, 42, 37, 44, 85, 15, 82, 43, 57, 45, 85, 4, 6, 61, 43, 23, 71, 30, 42, 88, 87, 7, 55, 44, 87, 66, 57, 91, 57, 66, 26, 9, 46, 77, 46, 77, 97, 5, 36, 98, 31, 14, 35, 78, 53, 50, 7, 52, 8, 37, 79, 45, 74, 42, 83, 47, 31, 38, 19, 65, 73, 77, 42, 19, 64, 44, 69, 70, 88, 23, 82, 34, 84, 29, 23, 57, 9, 72, 62, 57, 38, 83, 77, 64, 9, 37, 79, 36, 4, 85, 21, 58, 38, 53, 10, 68, 48, 85, 75, 36, 97, 97, 87, 45, 55, 21, 7, 17, 88, 76, 22, 58, 38, 26, 27, 35, 91, 67, 15, 18, 54, 29, 42, 17, 31, 53, 19, 10, 46, 71, 37, 33, 24, 5, 32, 92, 28, 46, 69, 16, 34, 77, 70, 87, 98, 94, 76, 85, 23, 49, 97, 24, 79, 23, 59, 61, 85, 83, 57, 10, 4, 55, 72, 33, 14, 68, 60, 37, 10, 95, 65, 45, 40, 57, 81, 73, 37, 5, 35, 80, 70, 53, 55, 98, 41, 93, 94, 94, 83, 51, 86, 78, 0, 10, 91, 44, 71, 50, 66, 57, 20, 80, 42, 21, 16, 35, 41, 67, 5, 51, 30, 48, 69, 34, 78, 62, 22, 53, 15, 67, 5, 84, 82, 74, 90, 96, 67, 38, 24, 65, 80, 31, 15, 69, 40, 94, 85, 92, 12, 78, 51, 24, 68, 23, 92, 11, 51, 1, 39, 30, 33, 46, 0, 19, 52, 37, 86, 18, 75, 61, 44, 9, 2, 67, 6, 0, 74, 91, 62, 41, 65, 6, 68, 26, 90, 70, 91, 58, 34, 40, 65, 36, 98, 94, 12, 2, 37, 15, 96, 57, 39, 69, 43, 90, 10, 74, 48, 31, 15, 16, 93, 2, 60, 77, 37, 10, 99, 66, 2, 3, 62, 26, 35, 71, 98, 30, 18, 45, 77, 66, 98, 97, 97, 64, 41, 57, 85, 2, 62, 94, 57, 44, 50, 12, 87, 50, 22, 19, 45, 87, 25, 79, 37, 22, 47, 76, 58, 16, 12, 62, 7, 39, 66, 29, 5, 70, 97, 84, 17, 97, 15, 33, 32, 7, 57, 53, 7, 4, 16, 16, 10, 80, 41, 73, 25, 10, 42, 63, 76, 30, 29, 57, 86, 22, 91, 10, 1, 43, 47, 24, 53, 71, 19, 77, 40, 19, 33, 32, 65, 59, 14, 34, 1, 5, 72, 87, 15, 52, 27, 52, 69, 7, 77, 83, 14, 3, 56, 2, 25, 51, 85, 6, 47, 64, 83, 93, 8, 6, 3, 67, 71, 56, 70, 2, 51, 77, 4, 24, 68, 33, 91, 98, 68, 6, 12, 43, 75, 79, 25, 21, 20, 0, 86, 80, 40, 15, 98, 38, 11, 70, 88, 10, 51, 23, 30, 74, 64, 29, 39, 19, 74, 78, 59, 11, 49, 89, 15, 67, 50, 96, 18, 25, 92, 27, 83, 10, 4, 45, 44, 78, 43, 16, 16, 47, 46, 52, 47, 18, 5, 85, 24, 54, 71, 88, 12, 91, 97, 72, 99, 67, 39, 8, 56, 7, 49, 24, 2, 49, 51, 51, 35, 62, 65, 17, 29, 71, 37, 12, 99, 29, 95, 78, 75, 69, 88, 10, 8, 84, 84, 95, 57, 88, 65, 24, 3, 36, 95, 88, 53, 70, 83, 33, 54, 35, 10, 58, 13, 11, 60, 8, 87, 23, 81, 8, 24, 88, 59, 12, 83, 31, 17, 35, 40, 2, 77, 66, 95, 53, 20, 10, 42, 90, 47, 64, 96, 67, 55, 42, 59, 0, 19, 18, 2, 15, 24, 70, 48, 18, 90, 6, 57, 33, 35, 57, 88, 12, 15, 37, 87, 78, 88, 43, 11, 28, 2, 9, 56, 69, 63, 4, 71, 32, 81, 51, 59, 49, 90, 54, 70, 79, 37, 37, 6, 84, 74, 50, 44, 68, 63, 62, 73, 53, 78, 19, 22, 88, 62, 18, 41, 83, 71, 2, 71, 64, 0, 57, 4, 77, 43, 97, 86, 10, 50, 23, 68, 13, 42, 16, 88, 25, 51, 35, 18, 85, 26, 58, 74, 37, 17, 24, 35, 81, 36, 71, 76, 28, 83, 10, 73, 87, 72, 96, 94, 57, 70, 57, 29, 85, 72, 50, 13, 91, 32, 96, 18, 82, 58, 59, 78, 9, 73, 12, 89, 56, 39, 2, 81, 81, 12, 16, 68, 4, 87, 21, 5, 49, 43, 24, 69, 1, 34, 5, 92, 18, 24, 36, 43, 72, 73, 20, 49, 30, 7, 1, 75, 77, 43, 71, 47, 12, 57, 73, 38, 22, 12, 32, 84, 10, 60, 74, 69, 30, 89, 89, 24, 90, 33, 28, 7, 30, 5, 68, 68, 80, 24, 79, 37, 9, 14, 37, 73, 11, 24, 1, 27, 55, 53, 6, 23, 18, 56, 48, 98, 35, 40, 58, 37, 52, 4, 25, 10, 73, 2, 42, 36, 58, 78, 69, 5, 37, 10, 15, 62, 24, 65, 68, 39, 92, 56, 36, 65, 19, 53, 84, 7, 23, 19, 44, 96, 27, 77, 53, 46, 85, 40, 95, 49, 4, 74, 71, 44, 24, 11, 86, 97, 37, 35, 12, 97, 14, 46, 78, 20, 96, 47, 53, 91, 10, 56, 55, 98, 48, 8, 23, 96, 10, 87, 25, 60, 46, 43, 68, 75, 64, 10, 37, 45, 73, 60, 99, 62, 10, 13, 96, 44, 7, 81, 46, 77, 83, 53, 96, 57, 74, 69, 76, 14, 9, 27, 10, 10, 41, 57, 34, 39, 36, 95, 57, 83, 32, 47, 58, 79, 71, 43, 31, 33, 79, 59, 33, 65, 40, 24, 11, 49, 93, 38, 13, 35, 44, 18, 63, 23, 57, 54, 29, 62, 26, 50, 26, 40, 2, 82, 79, 11, 9, 74, 15, 40, 87, 32, 15, 30, 14, 11, 50, 69, 63, 36, 36, 39, 10, 36, 44, 18, 37, 62, 75, 8, 20, 69, 1, 18, 16, 36, 88, 29, 85, 40, 5, 56, 38, 30, 22, 64, 91, 5, 83, 33, 27, 60, 55, 45, 7, 58, 53, 1, 19, 16, 67, 39, 42, 6, 69, 55, 15, 61, 46, 99, 19, 10, 69, 37, 7, 63, 40, 3, 8, 15, 2, 95, 34, 77, 83, 83, 24, 83, 40, 70, 54, 47, 9, 24, 68, 14, 78, 56, 39, 96, 25, 5, 38, 15, 55, 54, 24, 24, 63, 73, 24, 12, 56, 10, 88, 45, 45, 30, 22, 10, 10, 50, 52, 58, 2, 69, 57, 1, 23, 63, 26, 84, 70, 2, 57, 0, 5, 50, 20, 29, 67, 0, 18, 22, 10, 11, 38, 81, 39, 14, 22, 24, 66, 47, 31, 11, 64, 33, 28, 69, 74, 6, 35, 36, 65, 92, 78, 29, 57, 32, 53, 57, 51, 55, 10, 84, 37, 9, 88, 51, 16, 53, 68, 51, 59, 28, 54, 16, 82, 76, 11, 74, 27, 51, 45, 78, 29, 23, 43, 87, 7, 99, 39, 26, 17, 19, 58, 18, 57, 21, 15, 24, 1, 12, 83, 37, 88, 35, 45, 68, 85, 7, 86, 62, 9, 98, 78, 94, 85, 61, 74, 83, 95, 57, 10, 18, 7, 73, 22, 73, 99, 60, 89, 45, 49, 23, 11, 79, 15, 86, 8, 65, 35, 63, 12, 23, 48, 69, 78, 50, 34, 78, 78, 15, 78, 78, 71, 24, 15, 4, 68, 2, 57, 81, 47, 53, 43, 2, 1, 22, 54, 30, 21, 64, 27, 77, 80, 37, 83, 37, 47, 83, 11, 14, 61, 2, 86, 73, 97, 70, 48, 24, 73, 3, 44, 58, 55, 54, 30, 57, 30, 39, 78, 50, 99, 59, 98, 66, 25, 14, 8, 98, 57, 11, 81, 35, 10, 5, 44, 63, 74, 57, 4, 9, 24, 0, 87, 93, 71, 9, 53, 10, 96, 83, 53, 55, 54, 66, 58, 14, 97, 95, 18, 0, 55, 22, 67, 43, 3, 34, 39, 69, 78, 34, 83, 57, 22, 49, 57, 68, 37, 15, 9, 22, 63, 55, 45, 67, 68, 37, 11, 24, 85, 97, 65, 14, 56, 68, 23, 66, 29, 68, 37, 37, 4, 38, 36, 1, 21, 3, 75, 69, 26, 90, 29, 82, 57, 60, 30, 12, 92, 80, 85, 2, 31, 71, 37, 62, 63, 11, 9, 69, 62, 51, 26, 5, 37, 34, 87, 81, 9, 6, 75, 97, 12, 24, 36, 50, 89, 18, 39, 58, 11, 73, 71, 1, 22, 69, 33, 64, 38, 60, 51, 62, 15, 72, 64, 63, 63, 17, 40, 40, 35, 37, 75, 0, 92, 50, 74, 10, 39, 54, 50, 20, 59, 16, 81, 24, 25, 77, 3, 70, 70, 62, 8, 96, 9, 40, 38, 75, 28, 59, 73, 23, 97, 33, 39, 84, 0, 98, 5, 2, 67, 57, 24, 40, 23, 59, 29, 37, 17, 75, 24, 28, 60, 33, 8, 19, 60, 6, 99, 12, 59, 46, 34, 54, 67, 97, 68, 89, 62, 47, 53, 23, 97, 81, 34, 24, 99, 10, 26, 20, 20, 54, 72, 88, 76, 51, 7, 74, 18, 50, 39, 17, 24, 36, 89, 99, 79, 60, 36, 34, 0, 77, 95, 59, 11, 20, 10, 12, 81, 28, 75, 35, 2, 86, 9, 47, 94, 44, 36, 74, 75, 12, 43, 63, 2, 49, 21, 62, 54, 51, 3, 23, 2, 2, 12, 11, 54, 36, 0, 44, 95, 41, 63, 77, 74, 92, 25, 87, 92, 90, 81, 10, 33, 49, 9, 37, 3, 48, 94, 59, 97, 7, 58, 24, 3, 29, 29, 22, 79, 32, 15, 77, 94, 4, 15, 67, 4, 21, 76, 0, 64, 26, 80, 88, 27, 10, 14, 79, 1, 57, 84, 9, 73, 10, 54, 37, 74, 87, 33, 97, 56, 34, 62, 85, 38, 11, 99, 16, 27, 83, 86, 8, 49, 77, 69, 81, 41, 42, 24, 31, 30, 53, 37, 36, 60, 45, 79, 44, 80, 63, 66, 60, 79, 20, 23, 44, 72, 92, 49, 65, 50, 73, 3, 38, 11, 11, 68, 30, 51, 55, 23, 24, 57, 49, 28, 59, 43, 53, 97, 95, 65, 21, 2, 88, 53, 57, 87, 51, 93, 28, 45, 31, 87, 38, 89, 79, 48, 92, 70, 41, 27, 52, 16, 98, 21, 39, 38, 32, 18, 63, 84, 3, 11, 35, 44, 3, 83, 28, 29, 75, 40, 29, 36, 45, 39, 98, 59, 6, 64, 72, 74, 42, 47, 65, 0, 89, 23, 10, 7, 9, 11, 35, 8, 22, 49, 11, 13, 35, 12, 69, 94, 89, 8, 31, 1, 45, 5, 87, 24, 85, 86, 53, 33, 43, 37, 57, 13, 57, 53, 38, 20, 1, 48, 73, 80, 55, 23, 62, 20, 33, 4, 75, 60, 26, 97, 36, 10, 61, 64, 55, 34, 28, 88, 42, 33, 37, 39, 97, 27, 26, 14, 66, 37, 89, 64, 23, 79, 88, 58, 15, 74, 9, 88, 71, 1, 22, 3, 19, 94, 23, 45, 79, 64, 42, 24, 37, 10, 24, 51, 24, 67, 44, 25, 39, 69, 43, 10, 0, 87, 93, 5, 80, 1, 5, 56, 85, 34, 37, 81, 29, 73, 8, 88, 86, 11, 80, 51, 74, 40, 35, 79, 55, 57, 99, 8, 71, 57, 78, 55, 84, 94, 37, 86, 85, 61, 12, 95, 50, 21, 64, 33, 3, 69, 86, 84, 45, 19, 72, 36, 85, 94, 38, 62, 24, 86, 94, 48, 92, 10, 1, 57, 38, 37, 30, 26, 2, 73, 19, 46, 42, 13, 54, 75, 73, 71, 30, 78, 19, 47, 54, 57, 42, 21, 56, 34, 63, 40, 23, 86, 51, 18, 27, 14, 86, 75, 21, 95, 88, 16, 73, 37, 85, 76, 37, 97, 10, 67, 28, 79, 44, 1, 72, 26, 59, 31, 93, 24, 61, 79, 10, 80, 92, 72, 28, 57, 66, 65, 74, 11, 75, 2, 33, 1, 4, 50, 65, 31, 0, 58, 97, 83, 86, 51, 77, 27, 68, 37, 14, 8, 84, 92, 5, 85, 4, 53, 54, 47, 53, 12, 22, 57, 40, 50, 11, 22, 52, 58, 55, 71, 57, 27, 10, 6, 16, 71, 16, 17, 83, 7, 73, 59, 41, 50, 1, 47, 90, 64, 21, 28, 43, 64, 70, 47, 55, 36, 35, 26, 70, 48, 53, 46, 68, 86, 83, 39, 76, 73, 23, 33, 75, 81, 85, 58, 71, 94, 45, 18, 23, 19, 16, 22, 99, 31, 47, 56, 69, 23, 70, 37, 6, 2, 89, 93, 7, 27, 45, 74, 74, 14, 50, 27, 18, 94, 69, 26, 27, 71, 75, 46, 86, 84, 55, 76, 1, 81, 68, 2, 19, 78, 32, 6, 3, 92, 37, 63, 38, 80, 0, 45, 26, 48, 9, 66, 30, 6, 62, 10, 63, 91, 36, 59, 88, 57, 83, 83, 18, 97, 36, 12, 7, 85, 54, 77, 9, 51, 39, 52, 83, 58, 52, 10, 38, 89, 82, 99, 36, 2, 84, 28, 66, 22, 8, 57, 72, 70, 63, 73, 81, 4, 5, 1, 69, 9, 81, 51, 10, 50, 44, 24, 62, 5, 37, 31, 17, 74, 38, 12, 80, 43, 79, 60, 40, 2, 94, 30, 97, 62, 32, 97, 60, 36, 80, 72, 70, 87, 57, 68, 64, 34, 78, 22, 10, 81, 75, 36, 37, 73, 61, 13, 66, 20, 71, 49, 63, 63, 6, 58, 43, 13, 0, 16, 65, 37, 60, 83, 24, 33, 73, 37, 32, 67, 0, 40, 66, 96, 73, 65, 69, 60, 33, 52, 55, 11, 95, 54, 1, 3, 2, 94, 97, 94, 40, 48, 83, 51, 6, 36, 36, 43, 55, 95, 92, 34, 51, 72, 69, 40, 64, 14, 53, 49, 40, 1, 22, 12, 46, 58, 60, 52, 31, 80, 36, 57, 74, 28, 50, 12, 32, 54, 39, 29, 73, 55, 71, 43, 68, 45, 54, 94, 63, 9, 86, 93, 42, 4, 55, 46, 38, 11, 94, 88, 39, 5, 49, 44, 97, 62, 62, 85, 48, 61, 55, 45, 8, 82, 2, 69, 8, 64, 70, 10, 51, 77, 70, 92, 92, 63, 7, 5, 30, 6, 51, 53, 93, 97, 0, 57, 65, 18, 75, 44, 78, 28, 38, 12, 28, 60, 69, 24, 67, 62, 18, 77, 75, 53, 12, 55, 52, 11, 93, 18, 77, 74, 47, 24, 30, 49, 10, 85, 23, 88, 9, 68, 78, 46, 46, 37, 82, 41, 39, 71, 83, 4, 58, 57, 36, 98, 5, 61, 57, 15, 44, 85, 10, 30, 81, 73, 2, 43, 72, 83, 54, 56, 10, 90, 59, 48, 87, 34, 96, 95, 76, 13, 31, 44, 74, 79, 57, 7, 81, 16, 36, 22, 38, 26, 87, 50, 81, 85, 93, 72, 18, 62, 58, 43, 66, 4, 5, 11, 17, 9, 84, 37, 85, 38, 72, 57, 80, 87, 35, 24, 37, 58, 69, 10, 66, 50, 15, 66, 66, 44, 45, 57, 87, 50, 15, 78, 92, 73, 71, 46, 83, 3, 43, 40, 41, 97, 43, 53, 18, 63, 65, 28, 47, 12, 1, 55, 0, 62, 43, 65, 78, 25, 35, 17, 41, 2, 45, 12, 45, 58, 20, 73, 69, 60, 18, 49, 17, 2, 54, 71, 5, 9, 92, 15, 22, 50, 65, 73, 55, 64, 37, 29, 76, 51, 98, 50, 0, 75, 57, 68, 2, 84, 44, 17, 3, 82, 68, 81, 59, 31, 79, 66, 29, 77, 53, 54, 83, 47, 82, 31, 23, 64, 23, 61, 69, 23, 88, 39, 47, 53, 7, 24, 16, 38, 52, 20, 50, 94, 12, 43, 0, 76, 48, 58, 74, 81, 62, 88, 37, 73, 59, 16, 49, 10, 89, 92, 33, 1, 51, 57, 84, 30, 86, 46, 2, 22, 59, 7, 14, 27, 81, 11, 80, 27, 56, 97, 77, 24, 53, 85, 39, 74, 2, 36, 41, 69, 88, 51, 98, 34, 86, 84, 87, 53, 60, 34, 66, 45, 23, 0, 49, 7, 68, 47, 37, 97, 23, 23, 42, 70, 53, 84, 73, 68, 38, 21, 87, 70, 39, 14, 83, 55, 63, 7, 10, 81, 3, 99, 63, 60, 97, 51, 1, 62, 96, 37, 75, 44, 92, 97, 88, 95, 57, 24, 18, 26, 33, 53, 92, 43, 97, 36, 87, 87, 2, 77, 2, 26, 93, 63, 80, 11, 12, 10, 9, 81, 40, 77, 16, 91, 74, 48, 23, 96, 97, 27, 87, 41, 37, 69, 66, 80, 45, 28, 59, 30, 24, 53, 82, 75, 20, 3, 93, 77, 19, 17, 77, 11, 32, 65, 46, 49, 86, 8, 15, 25, 68, 4, 3, 49, 30, 97, 66, 0, 19, 82, 78, 4, 24, 56, 29, 2, 45, 10, 61, 44, 16, 53, 16, 88, 16, 57, 2, 10, 6, 10, 69, 5, 64, 37, 80, 89, 63, 37, 43, 22, 23, 87, 8, 39, 60, 14, 78, 29, 0, 6, 96, 83, 92, 15, 99, 42, 85, 22, 97, 87, 75, 23, 23, 37, 56, 75, 72, 87, 10, 16, 3, 37, 84, 15, 27, 50, 25, 93, 7, 18, 92, 66, 66, 79, 70, 67, 40, 41, 2, 92, 16, 57, 22, 38, 64, 35, 30, 10, 85, 37, 62, 90, 37, 53, 21, 0, 41, 15, 61, 10, 35, 14, 92, 56, 75, 96, 23, 17, 19, 13, 11, 94, 28, 93, 93, 37, 12, 60, 83, 29, 57, 49, 17, 46, 91, 24, 75, 43, 43, 66, 81, 44, 94, 48, 66, 36, 39, 6, 80, 63, 59, 21, 81, 71, 58, 75, 39, 17, 68, 7, 77, 10, 54, 8, 62, 61, 20, 83, 56, 44, 64, 2, 12, 99, 70, 33, 42, 27, 64, 22, 37, 66, 89, 8, 0, 63, 55, 79, 32, 56, 6, 23, 46, 12, 68, 94, 28, 41, 25, 88, 47, 69, 86, 37, 82, 27, 26, 28, 54, 43, 32, 2, 83, 24, 49, 40, 36, 17, 73, 3, 7, 79, 57, 66, 37, 59, 73, 59, 29, 8, 18, 63, 10, 10, 50, 10, 1, 77, 16, 68, 74, 12, 51, 0, 53, 98, 4, 81, 99, 88, 77, 58, 89, 2, 50, 40, 23, 2, 73, 37, 53, 0, 47, 68, 42, 34, 32, 44, 14, 88, 46, 23, 47, 11, 54, 29, 44, 18, 40, 8, 11, 98, 22, 84, 9, 69, 99, 39, 43, 15, 38, 9, 53, 43, 81, 81, 58, 57, 24, 68, 87, 49, 17, 38, 36, 39, 81, 48, 23, 55, 42, 47, 57, 10, 63, 18, 84, 24, 24, 69, 74, 81, 99, 32, 37, 34, 78, 5, 30, 98, 73, 36, 59, 28, 6, 63, 31, 36, 49, 77, 88, 10, 7, 44, 64, 10, 83, 88, 72, 2, 11, 34, 10, 39, 79, 37, 9, 95, 69, 39, 29, 24, 98, 40, 92, 72, 88, 49, 67, 64, 9, 41, 54, 14, 87, 29, 78, 6, 3, 4, 18, 19, 18, 5, 46, 90, 42, 83, 16, 10, 58, 49, 30, 24, 69, 69, 32, 62, 44, 28, 34, 84, 79, 25, 23, 3, 11, 47, 10, 10, 18, 67, 72, 5, 23, 39, 97, 67, 7, 44, 99, 76, 82, 24, 78, 92, 66, 4, 32, 54, 55, 60, 49, 19, 57, 14, 48, 89, 73, 26, 12, 96, 58, 79, 14, 9, 73, 87, 7, 60, 57, 92, 10, 69, 84, 72, 93, 44, 69, 55, 26, 72, 50, 47, 79, 65, 64, 55, 94, 74, 39, 10, 19, 34, 43, 77, 30, 35, 10, 72, 24, 9, 53, 37, 83, 74, 49, 97, 57, 46, 70, 57, 51, 23, 72, 95, 43, 75, 25, 77, 26, 11, 68, 94, 78, 26, 42, 92, 37, 88, 97, 37, 41, 7, 11, 88, 89, 78, 67, 87, 72, 68, 65, 54, 66, 17, 7, 15, 28, 77, 97, 40, 92, 78, 36, 57, 57, 43, 2, 1, 53, 82, 20, 54, 2, 4, 66, 10, 85, 57, 53, 62, 81, 13, 51, 80, 22, 78, 55, 38, 10, 96, 88, 15, 75, 65, 37, 7, 63, 85, 95, 85, 56, 25, 39, 48, 69, 67, 37, 77, 19, 58, 57, 27, 77, 7, 54, 76, 58, 68, 12, 62, 86, 51, 92, 24, 1, 98, 6, 40, 30, 23, 9, 10, 78, 35, 11, 19, 3, 41, 37, 86, 70, 31, 40, 60, 81, 56, 20, 29, 85, 78, 79, 34, 48, 62, 37, 10, 67, 99, 76, 17, 61, 65, 34, 22, 69, 30, 30, 70, 68, 7, 32, 82, 12, 14, 24, 54, 13, 45, 24, 55, 59, 9, 37, 86, 52, 81, 57, 88, 86, 94, 30, 45, 57, 44, 63, 71, 95, 6, 7, 17, 60, 92, 80, 9, 37, 36, 97, 36, 64, 91, 14, 57, 33, 38, 32, 23, 21, 37, 97, 84, 59, 2, 38, 49, 34, 28, 10, 58, 7, 63, 63, 37, 57, 44, 37, 50, 9, 44, 39, 83, 57, 76, 44, 31, 7, 29, 79, 96, 97, 10, 43, 85, 62, 73, 53, 1, 33, 63, 10, 83, 67, 16, 74, 38, 76, 9, 68, 87, 74, 40, 91, 95, 5, 2, 7, 76, 24, 75, 29, 38, 70, 96, 44, 88, 45, 30, 80, 49, 72, 17, 39, 15, 23, 15, 87, 0, 26, 91, 22, 65, 14, 64, 60, 35, 24, 2, 85, 38, 83, 2, 71, 49, 85, 6, 97, 88, 27, 57, 33, 81, 49, 96, 92, 37, 74, 62, 36, 28, 25, 6, 48, 19, 24, 23, 32, 18, 17, 72, 31, 7, 43, 73, 35, 12, 38, 64, 77, 22, 83, 68, 83, 81, 38, 75, 10, 13, 10, 74, 46, 98, 67, 65, 59, 42, 66, 24, 69, 50, 57, 4, 98, 83, 77, 94, 70, 9, 50, 60, 16, 62, 81, 98, 24, 92, 25, 75, 20, 55, 6, 2, 61, 10, 85, 44, 49, 48, 57, 38, 42, 7, 31, 68, 55, 28, 88, 44, 37, 35, 84, 10, 35, 91, 79, 58, 77, 35, 68, 16, 1, 77, 72, 16, 22, 90, 52, 35, 26, 91, 72, 80, 24, 75, 65, 32, 94, 18, 55, 23, 58, 60, 21, 29, 86, 25, 49, 70, 90, 1, 0, 32, 23, 22, 79, 29, 37, 29, 17, 51, 89, 75, 37, 70, 7, 39, 26, 37, 18, 9, 49, 81, 25, 46, 74, 57, 10, 48, 40, 12, 60, 75, 2, 72, 58, 10, 54, 91, 46, 53, 80, 64, 33, 11, 88, 5, 85, 78, 99, 63, 39, 40, 42, 15, 66, 47, 61, 9, 87, 44, 69, 12, 58, 66, 7, 39, 62, 2, 34, 65, 44, 15, 94, 68, 37, 67, 71, 14, 75, 86, 94, 64, 24, 1, 3, 57, 73, 1, 6, 60, 33, 30, 75, 88, 87, 8, 10, 5, 64, 9, 85, 37, 73, 96, 27, 87, 97, 28, 55, 32, 93, 11, 72, 9, 62, 41, 78, 65, 32, 23, 24, 92, 12, 0, 34, 48, 57, 59, 71, 12, 80, 34, 30, 11, 36, 77, 85, 24, 49, 16, 82, 6, 78, 88, 98, 5, 16, 39, 81, 42, 51, 0, 34, 69, 81, 79, 0, 11, 80, 10, 93, 57, 90, 66, 80, 97, 36, 28, 10, 92, 38, 55, 17, 1, 11, 10, 57, 24, 21, 65, 77, 42, 36, 18, 2, 40, 27, 68, 75, 72, 50, 66, 84, 68, 67, 23, 97, 48, 58, 40, 29, 89, 34, 54, 59, 12, 6, 57, 29, 87, 6, 63, 78, 38, 65, 75, 59, 50, 26, 58, 94, 45, 79, 60, 2, 39, 97, 74, 37, 72, 40, 8, 26, 75, 3, 23, 57, 42, 96, 79, 40, 19, 68, 75, 6, 53, 59, 64, 89, 99, 26, 15, 20, 69, 16, 29, 28, 45, 97, 14, 15, 78, 80, 76, 75, 92, 23, 17, 38, 49, 30, 14, 24, 3, 58, 70, 53, 75, 71, 7, 52, 1, 37, 85, 39, 10, 38, 4, 35, 57, 90, 0, 5, 48, 11, 92, 52, 89, 11, 94, 57, 53, 32, 33, 2, 95, 23, 93, 50, 65, 36, 55, 5, 2, 65, 42, 92, 53, 21, 23, 67, 86, 57, 88, 22, 8, 77, 55, 87, 10, 44, 14, 39, 29, 43, 80, 1, 2, 21, 58, 88, 83, 72, 83, 83, 19, 37, 5, 69, 18, 59, 89, 10, 82, 88, 15, 4, 52, 97, 81, 71, 76, 39, 0, 93, 24, 24, 49, 79, 59, 36, 43, 40, 73, 47, 12, 15, 90, 92, 76, 89, 51, 18, 18, 44, 86, 48, 81, 94, 27, 74, 69, 10, 0, 79, 23, 43, 27, 72, 18, 26, 2, 89, 97, 79, 19, 2, 54, 88, 74, 73, 40, 68, 37, 64, 19, 49, 44, 85, 75, 11, 85, 24, 78, 0, 59, 16, 12, 98, 32, 33, 34, 32, 49, 58, 74, 64, 45, 62, 11, 61, 6, 45, 0, 31, 90, 78, 53, 50, 36, 37, 2, 93, 79, 12, 85, 92, 2, 73, 21, 43, 55, 60, 27, 93, 83, 37, 75, 85, 83, 71, 29, 2, 92, 48, 83, 86, 91, 53, 30, 96, 42, 77, 49, 81, 41, 15, 99, 53, 60, 32, 15, 33, 69, 92, 10, 93, 86, 23, 12, 10, 67, 21, 58, 80, 28, 31, 37, 51, 23, 74, 14, 2, 38, 20, 52, 57, 68, 93, 42, 83, 90, 71, 10, 97, 89, 56, 29, 8, 35, 99, 42, 53, 10, 97, 63, 37, 21, 10, 32, 13, 10, 78, 90, 35, 38, 45, 27, 2, 96, 71, 71, 30, 82, 98, 87, 69, 72, 63, 19, 21, 35, 88, 97, 79, 58, 32, 66, 72, 73, 15, 22, 79, 37, 28, 13, 74, 7, 4, 38, 16, 97, 51, 86, 80, 38, 15, 3, 10, 7, 2, 51, 5, 33, 8, 57, 44, 87, 43, 62, 81, 43, 6, 21, 23, 53, 91, 75, 17, 50, 55, 78, 57, 32, 79, 63, 72, 71, 71, 92, 79, 75, 77, 68, 29, 85, 0, 62, 14, 4, 31, 37, 78, 20, 5, 72, 23, 47, 55, 25, 28, 89, 8, 73, 65, 51, 63, 57, 12, 79, 49, 45, 0, 72, 60, 67, 41, 58, 7, 40, 26, 51, 68, 25, 83, 97, 51, 94, 37, 29, 65, 24, 82, 93, 39, 73, 5, 3, 27, 78, 33, 24, 91, 4, 34, 45, 6, 1, 37, 9, 28, 67, 70, 1, 39, 86, 58, 83, 39, 82, 14, 86, 32, 60, 4, 94, 91, 45, 11, 65, 86, 73, 55, 36, 26, 75, 8, 2, 77, 41, 43, 28, 17, 16, 97, 0, 17, 17, 51, 83, 0, 82, 35, 62, 84, 64, 95, 55, 10, 2, 7, 37, 15, 71, 36, 72, 88, 55, 17, 24, 57, 38, 23, 67, 69, 48, 45, 49, 7, 1, 42, 23, 85, 68, 9, 75, 33, 27, 70, 37, 3, 23, 23, 69, 5, 37, 12, 39, 71, 11, 81, 73, 96, 54, 51, 78, 70, 57, 75, 87, 49, 78, 23, 4, 55, 32, 52, 23, 10, 5, 61, 68, 65, 9, 13, 26, 69, 24, 83, 8, 49, 37, 76, 38, 36, 68, 65, 38, 20, 32, 43, 14, 65, 47, 75, 50, 99, 72, 74, 54, 89, 86, 79, 71, 92, 29, 55, 79, 12, 87, 79, 91, 11, 49, 86, 59, 2, 99, 79, 37, 66, 40, 75, 88, 35, 39, 57, 30, 99, 50, 32, 85, 10, 37, 36, 90, 2, 72, 70, 84, 51, 73, 74, 46, 81, 58, 2, 33, 23, 32, 11, 75, 85, 26, 34, 15, 94, 26, 31, 2, 39, 14, 2, 39, 83, 83, 65, 62, 4, 78, 95, 5, 81, 19, 23, 60, 27, 57, 13, 18, 60, 93, 64, 9, 6, 78, 77, 45, 1, 75, 55, 10, 71, 22, 45, 7, 10, 69, 60, 33, 37, 93, 5, 22, 36, 8, 39, 32, 98, 95, 26, 50, 9, 71, 52, 10, 28, 64, 33, 33, 62, 10, 74, 65, 71, 25, 23, 77, 37, 2, 94, 61, 7, 90, 8, 15, 90, 92, 11, 88, 0, 43, 68, 57, 50, 8, 93, 66, 49, 78, 72, 33, 92, 44, 59, 6, 74, 66, 2, 41, 26, 12, 23, 24, 65, 87, 12, 67, 48, 77, 62, 70, 3, 3, 20, 23, 92, 2, 53, 40, 35, 54, 72, 1, 38, 78, 22, 88, 85, 80, 23, 36, 1, 7, 39, 85, 71, 37, 84, 57, 24, 57, 79, 40, 75, 13, 25, 4, 84, 70, 22, 2, 66, 53, 87, 13, 33, 51, 66, 91, 33, 23, 14, 50, 84, 71, 97, 50, 26, 45, 77, 88, 58, 58, 84, 46, 14, 6, 74, 39, 65, 10, 23, 27, 35, 24, 58, 24, 38, 10, 93, 90, 62, 20, 7, 69, 61, 74, 9, 56, 54, 8, 70, 10, 82, 47, 7, 69, 24, 24, 57, 16, 93, 26, 74, 75, 57, 77, 37, 8, 18, 5, 54, 42, 67, 33, 32, 47, 38, 50, 30, 23, 2, 85, 12, 70, 28, 14, 40, 10, 30, 91, 65, 82, 39, 64, 2, 72, 81, 3, 65, 26, 78, 67, 92, 36, 51, 63, 10, 93, 71, 5, 66, 35, 72, 83, 38, 52, 24, 2, 69, 39, 59, 65, 58, 73, 17, 50, 46, 39, 53, 38, 13, 94, 4, 71, 73, 78, 0, 93, 91, 0, 95, 9, 9, 15, 48, 2, 83, 0, 10, 2, 38, 41, 26, 80, 1, 8, 48, 63, 18, 58, 82, 26, 80, 75, 74, 99, 5, 0, 54, 10, 50, 33, 66, 96, 9, 37, 2, 85, 39, 22, 94, 71, 94, 87, 64, 45, 17, 10, 17, 1, 5, 88, 92, 10, 57, 66, 93, 81, 83, 73, 74, 73, 50, 56, 88, 99, 34, 57, 16, 90, 57, 17, 11, 70, 2, 32, 69, 2, 36, 47, 33, 58, 29, 45, 74, 71, 59, 5, 0, 78, 12, 29, 34, 11, 59, 36, 16, 42, 57, 2, 50, 4, 65, 73, 22, 47, 80, 34, 12, 73, 39, 5, 46, 39, 78, 86, 43, 12, 46, 65, 27, 15, 18, 64, 18, 85, 90, 37, 70, 43, 10, 80, 1, 7, 66, 41, 35, 37, 10, 74, 76, 10, 0, 32, 97, 90, 25, 98, 23, 29, 43, 16, 58, 46, 62, 50, 77, 62, 23, 72, 66, 74, 9, 99, 7, 52, 54, 48, 40, 24, 37, 54, 2, 63, 76, 11, 53, 30, 2, 62, 70, 83, 98, 63, 39, 15, 13, 68, 67, 39, 92, 16, 11, 65, 40, 1, 52, 44, 76, 12, 37, 85, 36, 13, 55, 8, 85, 37, 93, 1, 98, 40, 25, 10, 24, 2, 2, 26, 79, 69, 56, 72, 35, 68, 14, 7, 40, 57, 72, 71, 77, 2, 35, 81, 0, 4, 28, 86, 46, 10, 68, 20, 85, 30, 88, 38, 97, 2, 87, 13, 10, 69, 18, 45, 60, 97, 4, 95, 39, 9, 28, 33, 27, 97, 3, 38, 38, 18, 36, 91, 11, 82, 68, 31, 12, 7, 61, 84, 45, 93, 57, 71, 93, 84, 27, 32, 65, 11, 28, 72, 45, 87, 3, 74, 99, 71, 60, 24, 37, 70, 10, 20, 5, 60, 66, 73, 30, 97, 9, 27, 15, 18, 59, 48, 49, 33, 53, 68, 16, 78, 68, 19, 3, 38, 69, 18, 11, 13, 81, 97, 88, 47, 53, 50, 7, 28, 73, 14, 89, 58, 68, 29, 85, 92, 33, 11, 72, 87, 75, 23, 32, 37, 4, 29, 60, 46, 45, 71, 37, 44, 69, 47, 73, 2, 18, 1, 72, 43, 58, 72, 67, 8, 47, 18, 2, 33, 70, 36, 55, 85, 22, 37, 7, 67, 44, 48, 84, 29, 30, 11, 50, 61, 47, 25, 73, 23, 82, 91, 10, 10, 67, 80, 75, 14, 55, 47, 48, 79, 23, 49, 57, 56, 47, 95, 50, 60, 62, 83, 53, 81, 69, 38, 95, 24, 20, 31, 95, 65, 23, 86, 74, 23, 77, 76, 61, 45, 44, 95, 7, 80, 27, 25, 57, 26, 32, 35, 18, 58, 10, 14, 91, 12, 70, 88, 97, 93, 6, 90, 36, 84, 98, 54, 53, 24, 23, 23, 57, 94, 57, 41, 66, 1, 16, 33, 2, 85, 60, 42, 97, 65, 37, 81, 26, 30, 53, 37, 10, 37, 64, 98, 8, 80, 56, 14, 10, 69, 2, 4, 79, 57, 95, 71, 68, 12, 63, 10, 37, 28, 9, 25, 95, 1, 40, 87, 35, 78, 59, 20, 62, 49, 59, 53, 37, 32, 51, 43, 64, 48, 24, 40, 5, 45, 19, 8, 83, 33, 2, 92, 31, 74, 98, 59, 72, 29, 40, 76, 62, 46, 12, 72, 15, 61, 30, 59, 87, 19, 2, 88, 43, 40, 46, 53, 34, 83, 77, 68, 58, 86, 84, 47, 44, 22, 91, 37, 91, 86, 46, 93, 55, 81, 18, 70, 53, 37, 74, 71, 58, 51, 1, 11, 49, 10, 79, 51, 86, 33, 0, 7, 66, 22, 18, 69, 40, 6, 97, 36, 69, 62, 33, 68, 40, 10, 12, 57, 91, 57, 24, 82, 29, 89, 67, 85, 37, 74, 47, 70, 53, 56, 71, 43, 15, 43, 82, 33, 36, 39, 57, 45, 97, 46, 24, 99, 23, 57, 88, 10, 43, 96, 26, 7, 64, 54, 57, 2, 36, 92, 37, 0, 88, 2, 57, 66, 5, 83, 75, 33, 84, 91, 24, 25, 22, 50, 55, 20, 90, 92, 38, 34, 62, 29, 92, 71, 32, 37, 81, 57, 75, 18, 2, 68, 65, 58, 87, 63, 43, 78, 20, 32, 23, 52, 35, 18, 6, 70, 0, 76, 48, 6, 27, 77, 79, 97, 76, 28, 88, 47, 31, 20, 6, 36, 64, 2, 37, 48, 10, 89, 15, 77, 98, 93, 42, 36, 21, 86, 74, 59, 55, 93, 92, 2, 16, 98, 23, 86, 37, 2, 43, 47, 72, 29, 1, 75, 30, 51, 61, 69, 33, 97, 49, 95, 68, 45, 4, 28, 39, 62, 37, 27, 14, 71, 10, 18, 64, 12, 8, 9, 34, 36, 37, 45, 7, 19, 14, 3, 23, 27, 73, 53, 88, 15, 83, 33, 4, 53, 57, 77, 74, 63, 78, 77, 80, 1, 40, 11, 53, 14, 0, 72, 47, 49, 27, 33, 36, 93, 72, 19, 84, 74, 98, 23, 66, 35, 70, 47, 37, 30, 35, 55, 98, 24, 57, 23, 51, 39, 70, 32, 10, 37, 18, 15, 24, 51, 0, 85, 37, 71, 13, 84, 49, 58, 92, 36, 78, 80, 88, 7, 62, 62, 57, 74, 65, 84, 64, 47, 57, 74, 93, 40, 68, 19, 41, 50, 85, 60, 88, 46, 47, 45, 70, 74, 51, 37, 14, 72, 1, 98, 13, 57, 37, 38, 24, 33, 50, 80, 9, 2, 93, 94, 68, 60, 62, 39, 8, 23, 14, 14, 59, 76, 85, 54, 38, 76, 47, 55, 28, 33, 94, 97, 14, 5, 10, 57, 2, 16, 60, 30, 87, 23, 58, 57, 71, 62, 80, 19, 50, 17, 61, 57, 7, 22, 57, 62, 74, 78, 29, 54, 3, 58, 11, 90, 91, 0, 58, 36, 68, 53, 78, 48, 40, 46, 76, 99, 79, 2, 57, 77, 52, 90, 45, 14, 60, 9, 40, 24, 89, 59, 97, 31, 85, 58, 2, 87, 86, 24, 11, 96, 43, 83, 57, 56, 18, 81, 79, 30, 73, 48, 44, 88, 23, 12, 5, 71, 99, 39, 1, 88, 58, 75, 30, 72, 97, 23, 39, 8, 38, 72, 15, 40, 45, 76, 81, 43, 42, 29, 35, 9, 10, 92, 11, 68, 76, 72, 61, 42, 57, 21, 46, 6, 57, 72, 1, 39, 9, 70, 37, 51, 40, 59, 19, 66, 65, 12, 87, 43, 24, 67, 78, 88, 42, 55, 83, 45, 65, 1, 55, 95, 88, 85, 54, 24, 84, 3, 6, 66, 14, 87, 10, 29, 33, 45, 19, 18, 24, 8, 83, 69, 8, 64, 78, 29, 51, 11, 49, 36, 24, 8, 14, 12, 81, 70, 16, 45, 12, 64, 5, 28, 90, 32, 72, 46, 72, 55, 59, 95, 78, 24, 80, 67, 58, 55, 77, 20, 6, 2, 97, 3, 98, 92, 15, 37, 60, 19, 50, 24, 22, 50, 63, 36, 65, 51, 59, 21, 56, 72, 10, 69, 83, 17, 59, 4, 94, 30, 38, 2, 23, 13, 6, 22, 98, 70, 26, 42, 16, 30, 10, 37, 9, 71, 37, 82, 62, 26, 42, 47, 4, 78, 36, 55, 79, 55, 76, 15, 86, 90, 25, 10, 23, 88, 68, 10, 35, 95, 94, 19, 30, 5, 23, 43, 35, 3, 62, 22, 24, 40, 63, 21, 98, 85, 61, 68, 18, 18, 0, 99, 6, 59, 24, 93, 68, 21, 10, 22, 10, 6, 0, 79, 60, 46, 75, 40, 74, 18, 79, 5, 35, 5, 39, 94, 62, 49, 43, 24, 78, 68, 9, 24, 77, 67, 88, 8, 52, 71, 69, 83, 90, 47, 44, 5, 11, 86, 18, 72, 38, 61, 6, 61, 98, 86, 92, 59, 83, 97, 10, 81, 81, 44, 91, 33, 13, 56, 70, 42, 59, 77, 35, 24, 16, 83, 49, 90, 7, 11, 7, 79, 4, 11, 6, 96, 33, 46, 78, 62, 77, 80, 42, 90, 18, 77, 57, 28, 70, 88, 17, 48, 45, 85, 69, 64, 21, 62, 15, 46, 31, 4, 42, 42, 92, 24, 70, 57, 2, 44, 1, 65, 78, 57, 30, 63, 48, 17, 54, 45, 87, 19, 45, 52, 2, 29, 65, 84, 12, 45, 87, 63, 33, 99, 2, 79, 10, 43, 74, 57, 55, 69, 28, 18, 16, 39, 36, 7, 94, 8, 8, 5, 44, 50, 48, 58, 33, 38, 19, 17, 1, 8, 87, 22, 2, 49, 38, 59, 88, 49, 81, 2, 3, 78, 78, 12, 5, 83, 79, 70, 22, 3, 73, 6, 12, 44, 54, 81, 96, 72, 35, 86, 73, 68, 83, 72, 2, 47, 46, 36, 76, 44, 10, 11, 55, 10, 29, 17, 6, 71, 36, 73, 28, 96, 41, 42, 11, 33, 52, 70, 54, 89, 7, 84, 43, 2, 37, 87, 33, 5, 60, 8, 95, 56, 54, 17, 6, 85, 24, 59, 15, 84, 20, 10, 42, 34, 89, 11, 25, 71, 75, 2, 74, 94, 77, 15, 86, 9, 28, 78, 63, 86, 56, 2, 0, 92, 39, 7, 10, 18, 41, 90, 92, 97, 11, 99, 34, 40, 70, 59, 19, 0, 25, 26, 63, 23, 37, 68, 31, 64, 1, 9, 71, 65, 26, 59, 30, 55, 45, 42, 57, 76, 97, 66, 37, 61, 56, 89, 60, 95, 92, 64, 24, 36, 9, 60, 67, 9, 41, 65, 50, 71, 96, 24, 65, 36, 81, 66, 2, 27, 20, 17, 64, 70, 84, 87, 92, 98, 58, 43, 4, 79, 42, 82, 28, 36, 29, 72, 15, 8, 77, 39, 55, 51, 31, 10, 40, 45, 34, 84, 28, 0, 15, 69, 3, 22, 53, 90, 23, 54, 12, 75, 10, 58, 77, 83, 59, 64, 35, 79, 65, 55, 11, 65, 41, 19, 37, 72, 30, 54, 85, 38, 97, 5, 10, 75, 57, 97, 51, 72, 13, 69, 3, 97, 94, 51, 33, 23, 82, 11, 81, 47, 68, 84, 91, 88, 52, 22, 84, 40, 79, 55, 2, 34, 81, 64, 70, 95, 16, 7, 71, 12, 59, 76, 81, 51, 36, 54, 91, 72, 36, 73, 88, 0, 33, 69, 42, 43, 88, 26, 18, 51, 82, 56, 35, 11, 79, 70, 4, 47, 88, 24, 57, 31, 11, 46, 46, 31, 8, 73, 45, 12, 78, 53, 9, 71, 0, 36, 53, 33, 86, 65, 6, 42, 10, 48, 62, 35, 38, 39, 84, 53, 48, 78, 43, 24, 15, 72, 64, 53, 74, 84, 85, 4, 92, 83, 12, 58, 67, 68, 1, 86, 15, 35, 30, 70, 15, 94, 9, 69, 10, 8, 52, 50, 5, 62, 26, 12, 46, 53, 7, 37, 99, 66, 18, 2, 10, 96, 91, 43, 7, 86, 68, 28, 73, 56, 18, 69, 24, 52, 79, 45, 10, 76, 84, 33, 10, 18, 81, 32, 12, 70, 68, 86, 1, 60, 55, 18, 10, 77, 37, 85, 86, 50, 98, 14, 14, 5, 72, 60, 71, 6, 70, 7, 54, 79, 36, 47, 74, 82, 28, 8, 28, 8, 59, 31, 32, 2, 57, 79, 10, 66, 2, 27, 79, 3, 0, 20, 69, 73, 57, 62, 20, 2, 1, 10, 24, 63, 79, 10, 7, 70, 10, 56, 37, 2, 0, 95, 9, 18, 79, 48, 67, 38, 2, 11, 63, 68, 32, 0, 92, 37, 60, 29, 70, 57, 85, 84, 28, 11, 51, 19, 79, 5, 68, 12, 77, 91, 43, 29, 53, 83, 86, 80, 22, 50, 92, 29, 56, 3, 34, 98, 78, 28, 37, 88, 59, 74, 30, 21, 39, 43, 24, 99, 24, 90, 26, 86, 2, 32, 94, 35, 62, 37, 38, 47, 3, 0, 84, 93, 23, 99, 18, 12, 48, 79, 14, 43, 50, 23, 11, 32, 3, 65, 62, 77, 31, 67, 93, 2, 78, 67, 10, 48, 10, 82, 0, 79, 77, 44, 25, 3, 89, 10, 35, 5, 52, 84, 9, 89, 24, 39, 71, 83, 42, 55, 70, 69, 51, 90, 60, 53, 90, 11, 89, 27, 88, 50, 24, 55, 65, 74, 88, 87, 69, 23, 51, 72, 68, 5, 36, 17, 6, 67, 15, 8, 54, 62, 30, 27, 16, 31, 24, 40, 56, 75, 66, 48, 39, 43, 83, 7, 0, 88, 86, 15, 2, 72, 59, 50, 83, 30, 90, 45, 57, 41, 40, 81, 48, 44, 60, 26, 57, 32, 92, 49, 9, 32, 82, 38, 24, 7, 77, 45, 69, 36, 25, 47, 40, 11, 10, 2, 94, 52, 53, 47, 92, 38, 2, 5, 18, 30, 57, 80, 81, 52, 7, 76, 45, 24, 10, 84, 86, 61, 57, 44, 48, 83, 34, 5, 24, 80, 44, 32, 22, 77, 40, 80, 81, 92, 2, 22, 71, 8, 55, 45, 79, 92, 38, 87, 55, 47, 9, 3, 89, 7, 6, 83, 38, 4, 30, 76, 32, 52, 78, 72, 16, 37, 28, 11, 37, 12, 77, 4, 78, 57, 99, 3, 90, 37, 69, 4, 24, 54, 88, 1, 45, 82, 22, 43, 22, 91, 41, 57, 97, 36, 6, 87, 10, 14, 52, 32, 94, 67, 29, 65, 37, 23, 60, 59, 29, 33, 84, 66, 8, 89, 68, 5, 97, 0, 49, 58, 28, 14, 36, 64, 74, 24, 2, 21, 83, 76, 40, 80, 57, 17, 22, 59, 16, 70, 7, 75, 37, 2, 20, 48, 9, 38, 19, 95, 6, 37, 94, 87, 71, 48, 56, 46, 38, 53, 23, 2, 49, 5, 72, 57, 93, 78, 78, 84, 58, 22, 40, 55, 81, 58, 85, 57, 54, 65, 15, 0, 22, 88, 92, 14, 29, 92, 21, 83, 53, 51, 78, 35, 38, 1, 98, 59, 0, 62, 75, 23, 61, 14, 2, 55, 77, 89, 0, 20, 18, 84, 60, 80, 85, 93, 56, 43, 3, 39, 31, 94, 52, 97, 96, 45, 24, 24, 35, 78, 3, 44, 67, 74, 0, 48, 18, 17, 77, 50, 29, 46, 47, 57, 3, 98, 28, 43, 15, 58, 48, 12, 61, 73, 87, 57, 77, 44, 81, 63, 10, 2, 39, 35, 45, 32, 58, 69, 73, 96, 80, 77, 94, 65, 78, 22, 74, 11, 10, 29, 36, 39, 97, 57, 26, 38, 83, 33, 11, 60, 54, 5, 99, 24, 71, 57, 37, 68, 44, 69, 65, 5, 17, 71, 79, 4, 69, 34, 24, 83, 78, 28, 26, 10, 23, 48, 27, 32, 49, 17, 91, 87, 64, 82, 27, 37, 49, 58, 29, 80, 97, 85, 49, 42, 98, 28, 18, 29, 33, 62, 62, 87, 26, 10, 84, 63, 76, 18, 75, 10, 57, 37, 72, 2, 45, 68, 63, 24, 84, 22, 9, 49, 92, 77, 3, 7, 66, 14, 50, 84, 47, 70, 14, 28, 51, 9, 56, 28, 94, 54, 34, 40, 46, 42, 95, 23, 33, 46, 71, 26, 93, 56, 23, 24, 6, 40, 45, 32, 38, 26, 21, 66, 85, 75, 59, 81, 2, 81, 51, 68, 77, 31, 85, 42, 7, 9, 93, 3, 86, 26, 37, 23, 44, 71, 81, 51, 58, 26, 72, 19, 78, 38, 24, 35, 90, 94, 34, 54, 18, 94, 60, 58, 79, 11, 52, 48, 28, 87, 82, 63, 56, 15, 0, 44, 60, 6, 70, 20, 55, 24, 17, 79, 36, 64, 7, 87, 2, 48, 85, 23, 32, 63, 44, 58, 88, 7, 91, 82, 35, 34, 57, 53, 32, 80, 69, 53, 56, 36, 98, 38, 33, 63, 95, 63, 47, 82, 42, 79, 42, 45, 21, 19, 16, 19, 62, 72, 63, 34, 75, 3, 45, 26, 58, 15, 9, 84, 4, 66, 1, 13, 14, 41, 99, 25, 24, 57, 57, 72, 56, 31, 39, 15, 22, 51, 24, 85, 78, 11, 99, 34, 22, 48, 84, 88, 48, 79, 24, 27, 0, 59, 34, 82, 90, 65, 24, 4, 24, 30, 40, 13, 46, 7, 43, 71, 69, 15, 14, 28, 17, 50, 40, 7, 88, 63, 79, 82, 10, 46, 53, 42, 57, 70, 40, 51, 32, 10, 78, 57, 19, 88, 49, 10, 51, 72, 28, 79, 85, 83, 10, 69, 24, 55, 57, 93, 57, 83, 46, 29, 20, 7, 53, 24, 99, 70, 91, 27, 34, 37, 64, 74, 18, 56, 63, 71, 94, 49, 61, 88, 58, 53, 98, 10, 26, 6, 23, 75, 5, 31, 36, 8, 0, 90, 93, 0, 63, 32, 62, 52, 24, 55, 37, 68, 82, 24, 41, 76, 86, 19, 44, 8, 30, 53, 88, 90, 99, 5, 49, 61, 32, 37, 10, 90, 45, 92, 57, 98, 62, 52, 11, 16, 36, 53, 4, 37, 36, 63, 38, 30, 4, 44, 57, 9, 11, 59, 29, 80, 24, 73, 83, 83, 13, 96, 98, 27, 34, 96, 36, 26, 58, 86, 10, 26, 51, 51, 42, 39, 37, 22, 48, 86, 97, 49, 70, 13, 88, 17, 6, 33, 73, 10, 81, 46, 54, 65, 37, 70, 24, 8, 2, 39, 29, 47, 45, 66, 53, 53, 73, 25, 69, 32, 29, 3, 23, 81, 24, 17, 49, 80, 9, 3, 63, 83, 36, 17, 74, 48, 89, 55, 43, 9, 31, 7, 94, 56, 9, 24, 57, 64, 11, 57, 71, 98, 25, 88, 43, 31, 89, 44, 82, 1, 79, 56, 37, 84, 63, 6, 23, 60, 66, 60, 2, 51, 88, 88, 91, 72, 66, 63, 39, 5, 74, 23, 37, 92, 64, 6, 23, 80, 51, 83, 46, 66, 83, 32, 7, 22, 35, 72, 32, 7, 51, 11, 46, 63, 10, 34, 83, 66, 85, 79, 94, 67, 83, 65, 11, 31, 14, 75, 86, 33, 71, 58, 12, 24, 66, 62, 49, 12, 93, 50, 5, 69, 2, 57, 96, 24, 75, 30, 37, 6, 49, 10, 32, 84, 48, 24, 74, 96, 63, 40, 7, 74, 13, 26, 58, 29, 10, 12, 2, 85, 75, 67, 24, 96, 12, 79, 63, 2, 79, 87, 22, 61, 5, 68, 24, 77, 57, 36, 45, 32, 55, 68, 73, 14, 14, 9, 3, 63, 4, 24, 43, 71, 72, 30, 29, 77, 0, 50, 36, 54, 66, 51, 94, 65, 81, 40, 50, 57, 73, 20, 4, 76, 39, 82, 57, 62, 75, 59, 11, 73, 28, 56, 2, 39, 0, 85, 60, 72, 93, 38, 42, 7, 34, 13, 36, 28, 57, 24, 1, 57, 86, 2, 28, 94, 50, 3, 19, 53, 77, 11, 21, 35, 63, 71, 96, 88, 94, 64, 78, 71, 6, 65, 54, 60, 27, 70, 57, 80, 82, 39, 81, 61, 22, 15, 57, 3, 6, 73, 33, 52, 34, 45, 35, 97, 74, 92, 26, 74, 30, 51, 30, 98, 89, 37, 60, 18, 78, 92, 27, 76, 83, 23, 19, 57, 29, 39, 58, 25, 98, 22, 39, 63, 51, 83, 79, 75, 28, 8, 70, 12, 53, 32, 99, 90, 76, 37, 10, 59, 21, 38, 46, 33, 55, 69, 36, 64, 89, 2, 22, 21, 39, 98, 82, 47, 8, 57, 17, 10, 49, 86, 51, 48, 56, 79, 65, 92, 66, 73, 74, 46, 99, 58, 60, 26, 23, 24, 14, 92, 7, 32, 30, 26, 68, 5, 35, 54, 24, 71, 2, 33, 20, 9, 53, 57, 84, 84, 90, 97, 86, 60, 54, 87, 81, 85, 8, 32, 3, 61, 42, 6, 8, 86, 53, 3, 41, 62, 9, 42, 22, 11, 78, 87, 84, 82, 88, 23, 45, 92, 43, 9, 68, 51, 10, 27, 48, 14, 37, 31, 10, 11, 68, 59, 83, 76, 12, 46, 38, 24, 22, 35, 30, 2, 13, 25, 62, 0, 5, 83, 98, 29, 60, 62, 3, 32, 71, 14, 8, 57, 37, 58, 53, 46, 66, 40, 76, 11, 29, 98, 76, 68, 65, 2, 55, 61, 12, 88, 13, 45, 6, 16, 19, 23, 54, 37, 27, 23, 71, 10, 93, 33, 38, 13, 73, 81, 40, 79, 51, 37, 38, 18, 62, 60, 72, 83, 39, 27, 73, 10, 56, 68, 32, 32, 43, 84, 78, 63, 82, 24, 55, 54, 91, 9, 0, 46, 5, 42, 62, 7, 98, 23, 86, 2, 56, 47, 30, 48, 16, 53, 95, 41, 37, 81, 60, 69, 65, 10, 58, 88, 10, 7, 69, 3, 76, 42, 26, 65, 69, 71, 49, 67, 25, 37, 93, 24, 35, 77, 55, 54, 44, 55, 53, 37, 34, 76, 16, 28, 27, 8, 88, 38, 47, 20, 99, 3, 5, 39, 85, 60, 33, 22, 73, 88, 9, 68, 99, 14, 33, 15, 29, 23, 99, 81, 80, 43, 33, 39, 53, 58, 63, 64, 87, 97, 10, 37, 97, 58, 28, 71, 79, 79, 97, 46, 10, 60, 16, 54, 30, 45, 78, 45, 91, 35, 93, 42, 6, 24, 66, 97, 1, 88, 35, 11, 57, 56, 95, 76, 85, 39, 64, 13, 57, 15, 69, 10, 92, 83, 67, 97, 60, 57, 73, 86, 57, 26, 78, 38, 32, 46, 19, 9, 10, 68, 54, 2, 62, 21, 35, 90, 38, 47, 25, 38, 4, 42, 82, 10, 88, 27, 72, 8, 24, 67, 22, 93, 5, 83, 93, 11, 39, 8, 65, 44, 37, 74, 18, 71, 89, 46, 27, 30, 53, 41, 34, 44, 1, 63, 69, 48, 47, 74, 45, 46, 97, 76, 64, 29, 12, 59, 57, 10, 51, 59, 37, 24, 55, 9, 50, 53, 0, 7, 60, 70, 72, 54, 1, 35, 62, 10, 68, 63, 31, 79, 83, 26, 11, 30, 81, 4, 70, 77, 37, 77, 4, 91, 56, 68, 66, 73, 10, 36, 95, 0, 6, 76, 19, 39, 39, 24, 32, 39, 88, 18, 85, 92, 5, 60, 91, 14, 85, 7, 69, 36, 62, 75, 16, 1, 92, 7, 79, 10, 70, 61, 76, 3, 6, 24, 94, 34, 58, 60, 17, 69, 37, 15, 61, 93, 6, 17, 81, 36, 44, 71, 6, 24, 55, 15, 7, 11, 94, 58, 43, 43, 77, 2, 97, 69, 59, 37, 15, 93, 23, 9, 33, 33, 27, 25, 83, 45, 4, 17, 49, 55, 0, 54, 92, 40, 67, 16, 53, 24, 56, 44, 24, 57, 94, 70, 62, 93, 80, 10, 57, 13, 23, 69, 42, 43, 66, 38, 69, 43, 80, 44, 37, 54, 73, 5, 4, 55, 49, 33, 58, 39, 36, 92, 98, 97, 66, 82, 29, 24, 46, 2, 90, 80, 50, 18, 58, 83, 15, 46, 1, 50, 57, 62, 21, 49, 73, 92, 41, 2, 47, 44, 83, 40, 21, 95, 75, 24, 70, 66, 37, 45, 3, 82, 12, 24, 71, 80, 24, 71, 7, 16, 15, 89, 91, 22, 49, 40, 74, 44, 6, 47, 66, 54, 11, 74, 63, 94, 31, 80, 85, 40, 55, 83, 24, 35, 39, 26, 87, 54, 27, 10, 6, 4, 38, 57, 72, 58, 19, 33, 5, 1, 71, 93, 29, 67, 70, 15, 47, 58, 41, 86, 73, 59, 24, 35, 12, 39, 83, 17, 62, 9, 56, 94, 61, 68, 97, 37, 37, 23, 7, 39, 0, 50, 5, 47, 33, 28, 40, 1, 38, 5, 76, 88, 88, 75, 77, 81, 43, 65, 24, 8, 87, 28, 33, 81, 50, 2, 92, 43, 85, 31, 54, 57, 16, 1, 87, 54, 11, 32, 99, 2, 42, 29, 86, 35, 84, 24, 75, 64, 62, 10, 38, 15, 12, 37, 52, 11, 77, 16, 45, 69, 28, 96, 67, 54, 34, 18, 94, 74, 78, 58, 97, 57, 29, 83, 50, 61, 52, 56, 71, 50, 24, 39, 34, 14, 2, 31, 99, 30, 61, 82, 45, 1, 1, 81, 23, 44, 9, 67, 43, 67, 88, 36, 85, 33, 92, 67, 81, 12, 42, 23, 84, 24, 42, 70, 87, 78, 60, 93, 96, 92, 94, 65, 45, 39, 37, 55, 37, 38, 96, 84, 88, 2, 59, 85, 88, 60, 88, 2, 68, 65, 11, 71, 74, 67, 33, 42, 26, 11, 9, 24, 43, 75, 62, 4, 24, 85, 63, 49, 64, 49, 19, 68, 23, 46, 44, 79, 9, 67, 39, 74, 68, 71, 49, 2, 23, 51, 53, 23, 36, 7, 75, 7, 20, 48, 9, 73, 95, 35, 97, 97, 8, 85, 26, 43, 21, 57, 93, 68, 17, 64, 43, 48, 22, 58, 3, 0, 86, 76, 77, 53, 37, 82, 81, 54, 85, 43, 37, 97, 5, 6, 55, 70, 12, 87, 41, 57, 3, 37, 29, 14, 77, 40, 46, 12, 11, 10, 85, 73, 32, 27, 59, 53, 24, 72, 57, 31, 0, 35, 78, 9, 78, 91, 18, 7, 83, 60, 16, 26, 64, 76, 77, 50, 50, 37, 3, 10, 84, 57, 60, 2, 30, 94, 25, 83, 40, 0, 47, 82, 50, 12, 74, 81, 49, 60, 84, 78, 12, 69, 20, 2, 63, 22, 44, 18, 58, 86, 8, 51, 10, 88, 24, 37, 98, 49, 39, 58, 23, 27, 7, 50, 43, 11, 94, 88, 69, 88, 16, 6, 50, 99, 88, 66, 23, 12, 94, 53, 9, 83, 49, 2, 39, 90, 69, 44, 77, 29, 58, 21, 87, 65, 44, 0, 47, 38, 73, 97, 36, 86, 6, 63, 42, 57, 43, 37, 32, 92, 58, 43, 56, 62, 90, 65, 85, 87, 79, 67, 97, 4, 80, 66, 78, 60, 52, 66, 38, 90, 36, 78, 17, 10, 11, 42, 32, 75, 28, 13, 69, 39, 40, 2, 82, 9, 33, 33, 4, 78, 57, 70, 53, 6, 65, 89, 94, 57, 54, 77, 64, 15, 13, 68, 42, 83, 2, 28, 72, 62, 80, 77, 44, 33, 55, 40, 48, 94, 62, 81, 37, 38, 5, 4, 31, 93, 64, 24, 23, 98, 82, 70, 59, 35, 45, 53, 38, 40, 57, 80, 1, 22, 58, 60, 50, 88, 98, 14, 1, 14, 59, 43, 4, 52, 9, 64, 16, 86, 34, 43, 69, 81, 30, 59, 18, 60, 59, 1, 2, 2, 33, 33, 35, 5, 8, 28, 97, 74, 39, 11, 62, 77, 4, 37, 16, 22, 92, 21, 78, 64, 24, 64, 59, 28, 15, 48, 83, 47, 38, 81, 34, 73, 45, 20, 13, 70, 24, 46, 84, 73, 28, 66, 39, 24, 10, 6, 53, 57, 29, 19, 29, 7, 1, 79, 3, 72, 24, 82, 53, 59, 84, 38, 73, 36, 99, 21, 50, 38, 98, 32, 24, 14, 45, 11, 75, 43, 51, 70, 85, 7, 22, 45, 24, 67, 48, 15, 60, 94, 88, 42, 1, 49, 58, 6, 80, 55, 17, 37, 97, 69, 10, 38, 57, 26, 30, 10, 24, 63, 10, 78, 6, 18, 74, 3, 77, 95, 35, 97, 54, 61, 45, 11, 4, 1, 78, 81, 54, 29, 23, 39, 12, 49, 62, 27, 10, 8, 6, 86, 57, 90, 99, 18, 57, 4, 75, 78, 33, 25, 57, 70, 77, 62, 32, 67, 39, 93, 39, 7, 39, 64, 57, 58, 85, 59, 42, 53, 42, 63, 37, 81, 0, 67, 13, 56, 36, 45, 51, 5, 32, 60, 53, 51, 24, 5, 66, 15, 74, 82, 75, 62, 18, 43, 61, 47, 2, 24, 56, 95, 34, 85, 47, 83, 0, 22, 33, 84, 7, 36, 51, 16, 81, 29, 79, 10, 30, 75, 98, 56, 66, 41, 17, 47, 10, 73, 31, 80, 93, 84, 10, 2, 48, 55, 65, 89, 79, 83, 60, 87, 22, 54, 0, 98, 74, 97, 53, 73, 52, 5, 43, 57, 39, 9, 55, 25, 40, 66, 44, 10, 10, 70, 2, 88, 31, 34, 73, 62, 52, 8, 96, 34, 29, 88, 65, 72, 55, 84, 27, 70, 43, 22, 6, 69, 33, 5, 57, 36, 85, 58, 57, 36, 33, 71, 43, 85, 65, 9, 35, 28, 57, 56, 60, 54, 70, 62, 61, 51, 90, 84, 54, 26, 60, 11, 78, 95, 10, 53, 0, 62, 91, 35, 59, 37, 48, 9, 59, 47, 10, 2, 9, 22, 11, 47, 57, 74, 15, 19, 29, 23, 10, 82, 98, 34, 68, 57, 63, 95, 93, 97, 1, 13, 66, 43, 95, 2, 66, 22, 7, 3, 51, 97, 3, 68, 69, 38, 59, 43, 37, 98, 36, 64, 0, 20, 66, 27, 88, 75, 31, 35, 54, 7, 30, 96, 37, 38, 19, 33, 3, 60, 37, 85, 90, 42, 12, 72, 20, 76, 78, 11, 12, 71, 53, 34, 99, 69, 44, 16, 57, 82, 58, 59, 19, 14, 34, 75, 71, 77, 54, 96, 11, 10, 32, 8, 83, 13, 24, 53, 60, 34, 88, 51, 21, 47, 74, 37, 11, 23, 10, 57, 87, 7, 59, 8, 37, 15, 57, 65, 66, 24, 23, 21, 23, 70, 87, 83, 53, 96, 10, 10, 33, 79, 73, 43, 88, 18, 51, 25, 2, 26, 81, 72, 78, 85, 28, 54, 2, 18, 91, 30, 17, 70, 81, 33, 68, 10, 24, 81, 34, 71, 8, 8, 64, 23, 2, 98, 61, 21, 89, 74, 54, 43, 45, 24, 19, 81, 48, 37, 75, 41, 74, 38, 24, 57, 20, 40, 38, 15, 83, 7, 88, 98, 2, 18, 10, 83, 43, 81, 60, 46, 86, 39, 89, 68, 34, 22, 52, 42, 65, 12, 55, 87, 10, 37, 0, 11, 30, 99, 23, 60, 66, 13, 35, 2, 31, 61, 62, 12, 79, 8, 99, 48, 76, 54, 42, 24, 34, 23, 43, 31, 0, 79, 35, 65, 96, 46, 49, 16, 42, 27, 44, 15, 62, 56, 10, 80, 1, 80, 70, 62, 0, 24, 33, 40, 63, 35, 66, 76, 70, 11, 16, 63, 76, 55, 58, 6, 35, 85, 74, 4, 4, 81, 92, 63, 12, 92, 34, 65, 24, 85, 0, 47, 22, 68, 48, 24, 6, 59, 2, 91, 3, 22, 11, 95, 76, 81, 80, 90, 73, 49, 13, 16, 94, 58, 26, 52, 52, 71, 40, 10, 2, 81, 24, 19, 73, 2, 1, 48, 28, 40, 43, 67, 70, 23, 62, 98, 17, 28, 80, 60, 10, 30, 15, 99, 2, 57, 71, 62, 76, 53, 93, 64, 80, 3, 95, 34, 78, 94, 77, 94, 68, 41, 38, 1, 55, 85, 27, 70, 10, 92, 36, 67, 77, 6, 7, 36, 90, 77, 3, 77, 82, 55, 49, 67, 48, 75, 24, 83, 90, 79, 87, 18, 99, 0, 22, 5, 40, 2, 11, 44, 51, 55, 5, 14, 5, 73, 17, 11, 37, 16, 30, 13, 17, 14, 92, 88, 72, 69, 40, 91, 44, 10, 66, 62, 67, 71, 15, 26, 10, 2, 10, 20, 2, 10, 84, 38, 10, 57, 35, 61, 59, 73, 38, 28, 8, 37, 25, 27, 32, 57, 45, 19, 65, 37, 6, 6, 55, 59, 99, 7, 76, 70, 67, 3, 43, 24, 31, 10, 77, 32, 60, 92, 57, 38, 71, 2, 35, 5, 53, 86, 58, 37, 5, 78, 68, 28, 11, 39, 17, 64, 30, 26, 89, 64, 30, 90, 88, 47, 10, 37, 36, 88, 6, 57, 53, 69, 8, 72, 41, 14, 50, 2, 86, 94, 52, 79, 91, 73, 2, 41, 45, 60, 80, 63, 77, 93, 1, 68, 39, 2, 93, 15, 5, 53, 35, 72, 23, 25, 33, 45, 57, 7, 75, 42, 12, 69, 6, 18, 33, 36, 1, 78, 90, 24, 70, 73, 43, 26, 57, 13, 7, 57, 23, 59, 85, 12, 85, 87, 70, 37, 52, 79, 24, 21, 22, 93, 40, 84, 31, 2, 72, 2, 10, 75, 11, 33, 79, 2, 93, 70, 72, 14, 2, 85, 1, 44, 64, 81, 10, 37, 25, 38, 24, 7, 67, 7, 1, 9, 88, 36, 72, 40, 4, 29, 37, 27, 97, 87, 5, 65, 68, 35, 19, 75, 78, 68, 93, 75, 2, 97, 70, 40, 97, 10, 64, 43, 37, 62, 92, 14, 49, 10, 40, 69, 49, 45, 78, 81, 72, 16, 97, 37, 99, 22, 73, 63, 23, 66, 3, 33, 22, 0, 45, 85, 35, 46, 82, 22, 39, 84, 26, 88, 37, 79, 97, 11, 7, 51, 55, 24, 69, 40, 12, 17, 1, 33, 64, 75, 95, 52, 58, 81, 17, 32, 10, 15, 24, 74, 55, 62, 12, 5, 17, 4, 81, 29, 74, 1, 9, 3, 69, 45, 24, 94, 2, 97, 30, 40, 35, 33, 18, 31, 10, 64, 22, 44, 63, 48, 81, 45, 77, 45, 10, 97, 48, 30, 64, 14, 10, 50, 97, 74, 46, 93, 5, 27, 25, 22, 83, 23, 73, 98, 30, 28, 65, 49, 67, 23, 12, 98, 80, 64, 41, 41, 74, 62, 74, 85, 57, 11, 63, 8, 83, 45, 36, 56, 23, 28, 67, 79, 0, 54, 96, 4, 7, 32, 84, 40, 78, 75, 75, 48, 95, 23, 39, 98, 68, 2, 35, 93, 0, 13, 10, 63, 74, 0, 37, 9, 71, 55, 56, 58, 23, 8, 91, 53, 15, 79, 51, 6, 39, 34, 41, 74, 71, 95, 75, 87, 60, 54, 44, 20, 87, 12, 23, 24, 87, 10, 59, 22, 28, 9, 14, 99, 6, 8, 29, 32, 62, 83, 37, 57, 30, 38, 43, 92, 24, 37, 26, 60, 80, 62, 45, 29, 72, 68, 70, 66, 92, 63, 44, 37, 10, 93, 10, 61, 37, 2, 32, 87, 60, 27, 29, 8, 15, 43, 10, 88, 75, 86, 34, 7, 78, 23, 82, 77, 48, 81, 85, 85, 37, 43, 10, 66, 57, 65, 84, 14, 69, 65, 51, 88, 90, 9, 29, 52, 37, 1, 17, 36, 72, 53, 6, 4, 32, 63, 79, 10, 62, 7, 5, 81, 42, 8, 11, 74, 43, 69, 54, 85, 37, 10, 36, 20, 90, 72, 89, 5, 21, 60, 0, 44, 85, 11, 83, 36, 83, 75, 47, 80, 95, 71, 28, 64, 10, 68, 39, 77, 25, 99, 62, 43, 0, 37, 53, 37, 24, 81, 26, 26, 95, 37, 10, 36, 82, 13, 72, 2, 33, 23, 88, 77, 97, 14, 22, 79, 28, 40, 28, 70, 10, 33, 32, 97, 44, 6, 32, 84, 85, 37, 51, 60, 37, 36, 7, 77, 71, 90, 41, 54, 50, 55, 3, 43, 43, 47, 9, 4, 88, 26, 6, 11, 67, 35, 74, 42, 97, 23, 10, 1, 33, 80, 92, 80, 38, 70, 97, 94, 30, 44, 97, 28, 7, 23, 27, 13, 10, 68, 37, 34, 40, 32, 86, 39, 72, 74, 9, 12, 39, 22, 28, 50, 88, 2, 88, 69, 41, 49, 38, 13, 93, 71, 74, 23, 75, 81, 5, 29, 34, 25, 67, 97, 24, 71, 86, 13, 58, 28, 71, 27, 85, 77, 46, 24, 22, 39, 9, 2, 2, 51, 96, 30, 34, 25, 37, 57, 75, 97, 12, 92, 26, 90, 79, 40, 35, 94, 59, 12]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "tmp = []\n",
    "\n",
    "for i in range(len(result)):\n",
    "    tmp.append(int(re.sub(r'[^0-9]', '', result[i][1])))\n",
    "    \n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce652e3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T13:44:38.518073Z",
     "iopub.status.busy": "2022-06-13T13:44:38.517741Z",
     "iopub.status.idle": "2022-06-13T13:44:38.563159Z",
     "shell.execute_reply": "2022-06-13T13:44:38.562447Z"
    },
    "papermill": {
     "duration": 0.192302,
     "end_time": "2022-06-13T13:44:38.565116",
     "exception": false,
     "start_time": "2022-06-13T13:44:38.372814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('../input/sorghum-id-fgvc-9/train_cultivar_mapping.csv')\n",
    "df_all.dropna(inplace=True)\n",
    "\n",
    "unique_cultivars = list(df_all[\"cultivar\"].unique())\n",
    "\n",
    "predictions = [unique_cultivars[pred] for pred in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "460ddd33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T13:44:38.899325Z",
     "iopub.status.busy": "2022-06-13T13:44:38.898959Z",
     "iopub.status.idle": "2022-06-13T13:44:38.978402Z",
     "shell.execute_reply": "2022-06-13T13:44:38.977520Z"
    },
    "papermill": {
     "duration": 0.227668,
     "end_time": "2022-06-13T13:44:38.980397",
     "exception": false,
     "start_time": "2022-06-13T13:44:38.752729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>cultivar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000005362.png</td>\n",
       "      <td>PI_218112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000099707.png</td>\n",
       "      <td>PI_329333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000135300.png</td>\n",
       "      <td>PI_92270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000136796.png</td>\n",
       "      <td>PI_329256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000292439.png</td>\n",
       "      <td>PI_156393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename   cultivar\n",
       "0  1000005362.png  PI_218112\n",
       "1  1000099707.png  PI_329333\n",
       "2  1000135300.png   PI_92270\n",
       "3  1000136796.png  PI_329256\n",
       "4  1000292439.png  PI_156393"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.read_csv('../input/sorghum-id-fgvc-9/sample_submission.csv')\n",
    "sub['cultivar'] = predictions\n",
    "sub.to_csv('submission5.csv', index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7196bd4",
   "metadata": {
    "papermill": {
     "duration": 0.143774,
     "end_time": "2022-06-13T13:44:39.270941",
     "exception": false,
     "start_time": "2022-06-13T13:44:39.127167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5408.629721,
   "end_time": "2022-06-13T13:44:42.417622",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-13T12:14:33.787901",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
